# “Labs” (giant companies) \[cat:big_labs\]

## OpenAI Safety \[cat:openai\]

* **See also:** *iterative alignment*  
    
* **Teams**: Alignment, Safety Systems (Interpretability, Safety Oversight, Pretraining Safety, Robustness, Safety Research, Trustworthy AI, new Misalignment Research team [coming](https://archive.is/eDB1D)), Preparedness, Safety and Security Committee, Safety Advisory Group. The [Persona Features](https://www.arxiv.org/pdf/2506.19823) paper had a distinct author list. No named successor to Superalignment.  
* **Public alignment agenda:** [None](https://openai.com/safety/how-we-think-about-safety-alignment/). Barak [offers](https://www.lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety) personal [views](https://windowsontheory.org/2025/06/24/machines-of-faithful-obedience/).  
* **Public plan**: [Preparedness Framework](https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf)

* **Critiques:** [Harlan Stewart](https://intelligence.org/2025/03/31/a-response-to-openais-how-we-think-about-safety-and-alignment/), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).), [ZSP](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims), [Midas Project](https://www.openaifiles.org/transparency-and-safety)  
* **Broad approach:**   
* **Some names:** Johannes Heidecke, Boaz Barak, Mia Glaese, Jenny Nitishinskaya, Miles Wang, Wojciech Zaremba, David Robinson, Zico Kolter  
* **Estimated FTEs:** \~80. Not counting the AIs.  
* **Funded by:** Microsoft, [AWS](https://www.aboutamazon.com/news/aws/aws-open-ai-workloads-compute-infrastructure), Oracle, NVIDIA, SoftBank, G42, AMD, Dragoneer, Coatue, Thrive, Altimeter, MGX, Blackstone, TPG, T. Rowe Price, Andreessen Horowitz, D1 Capital Partners, Fidelity Investments, Founders Fund, Sequoia…  
* **Funding in 2025:** unknown. 80/6000 staff \* $13b budget \= $170m  
* **Outputs in 2025:**  
  * The o1 system card had 260 authors; the GPT-5 card doesn’t list any. Cards now contain a large amount of the public safety work.  
  * [https://openai.com/index/emergent-misalignment/](https://openai.com/index/emergent-misalignment/)   
  * Detecting misbehavior in frontier reasoning models  
  * Trading inference-time compute for adversarial robustness  
  * [https://arxiv.org/abs/2412.16339](https://arxiv.org/abs/2412.16339)   
  * [Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation](https://arxiv.org/abs/2503.11926)  
  * [Deliberative Alignment against scheming](https://arxiv.org/abs/2509.15541)  
  * [https://arxiv.org/abs/2501.18841](https://arxiv.org/abs/2501.18841)   
  * [Persona Features](https://www.arxiv.org/pdf/2506.19823) (very distinct author list)  
  * [CoT Defence](https://arxiv.org/abs/2503.11926)  
  * [Madry data attribution](https://arxiv.org/abs/2505.16260)  
  * [https://openai.com/index/openai-anthropic-safety-evaluation/](https://openai.com/index/openai-anthropic-safety-evaluation/)   
  * [https://openai.com/safety/evaluations-hub/](https://openai.com/safety/evaluations-hub/) 

## Deepmind Responsibility & Safety \[cat:deepmind\]

* **See also:** Interpretability**,** Scalable Oversight

* **Teams**: ASAT (split into “AGI Alignment”, amplified oversight and interpretability; and “Frontier Safety”, framework development and implementation), Gemini Safety, [Voices of All in Alignment](https://www.edinburgh-robotics.org/events/whose-gold-aligning-ai-diverse-views-what%E2%80%99s-safe-aligned-and-beneficial), AGI Safety Council, Responsibility and Safety Council. Sort-of the Causal Incentives Working Group too.  
* **Public alignment agenda:** [An Approach to Technical AGI Safety](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf%20)  
* **Framework:** [Frontier Safety Framework](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/updating-the-frontier-safety-framework/Frontier%20Safety%20Framework%202.0.pdf)

* **Some names**: Rohin Shah, Allan Dafoe, Anca Dragan, Dave Orr, Alex Irpan, Alex Turner, Anna Wang, Arthur Conmy, David Lindner, Jonah Brown-Cohen, Lewis Ho, Neel Nanda, Raluca Ada Popa, Rishub Jain, Rory Greig, Scott Emmons, Sebastian Farquhar, Senthooran Rajamanoharan, Sophie Bridgers, Tobi Ijitoye, Tom Everitt, Victoria Krakovna, Vikrant Varma, Vladimir Mikulik, Zac Kenton, Noah Goodman, Four Flynn  
* **Critiques:** [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).)**,** [ZSP](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims)  
* **Funded by:** Google  
* **Funding in 2025:** undefined.   
* **Outputs in 2025:**  
  * [Evaluating Frontier Models for Stealth and Situational Awareness](https://arxiv.org/abs/2505.01420)  
  * [An Approach to Technical AGI Safety and Security](https://arxiv.org/abs/2504.01849)  
  * [Evaluating potential cybersecurity threats of advanced AI](https://deepmind.google/discover/blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/)  
  * [Negative Results for SAEs](https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks)  
  * [MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking](https://www.alignmentforum.org/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2)  
  * [https://arxiv.org/abs/2510.27062](https://arxiv.org/abs/2510.27062)   
  * [https://arxiv.org/pdf/2507.05246](https://arxiv.org/pdf/2507.05246)   
  * Some work from non-safety teams, like [InfAlign](https://arxiv.org/abs/2412.19792)

## Anthropic Safety \[cat:anthropic\]

* **See also:** Interpretability**,** Scalable Oversight

* **Teams**: Scalable Alignment (Leike), Alignment Evals (Bowman), [Interpretability](https://transformer-circuits.pub/) (Olah), Model Psychiatry (Lindsey), Character (Askell), Alignment Stress-Testing (Hubinger), Frontier Red Team (Graham), Safeguards (Sharma), Trust and Safety (Sanderford), Model Welfare (Fish).   
* **Public alignment agenda:** [directions](https://alignment.anthropic.com/2025/recommended-directions/), [bumpers](https://alignment.anthropic.com/2025/bumpers/), [checklist](https://sleepinyourhat.github.io/checklist/), [old vague view](https://www.anthropic.com/news/core-views-on-ai-safety)  
* **Framework:** [RSP](https://www-cdn.anthropic.com/872c653b2d0501d6ab44cf87f43e1dc4853e4d37.pdf)  
* **Some names:** Chris Olah, Evan Hubinger, Sam Marks, Johannes Treutlein, Sam Bowman, Euan Ong, Fabien Roger, Adam Jermyn, Holden Karnofsky  
* **Estimated FTEs:** 100\. Not counting the AIs.  
* **Critiques:** [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).) on labs in general and [on Anthropic](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#ref-14), [ZSP](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims), [Greenblatt](https://nitter.net/RyanPGreenblatt/status/1925992236648464774)  
* **Funded by:** Amazon, Google, ICONIQ, Fidelity, Lightspeed, Altimeter, Baillie Gifford, BlackRock, Blackstone, Coatue, D1 Capital Partners, General Atlantic, General Catalyst, GIC, Goldman Sachs, Insight Partners, Jane Street, Ontario Teachers' Pension Plan, Qatar Investment Authority, TPG, T. Rowe Price, WCM, XN.  
* **Funding in 2025:**   
* **Outputs in 2025:**  
  * [https://alignment.anthropic.com/2025/openai-findings/](https://alignment.anthropic.com/2025/openai-findings/)   
  * [https://arxiv.org/abs/2502.16797](https://arxiv.org/abs/2502.16797)   
  * [https://www.alignmentforum.org/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don](https://www.alignmentforum.org/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don)   
  * [https://www.anthropic.com/research/open-source-circuit-tracing](https://www.anthropic.com/research/open-source-circuit-tracing)   
  * [https://www.anthropic.com/research/shade-arena-sabotage-monitoring](https://www.anthropic.com/research/shade-arena-sabotage-monitoring)   
  * [https://www.anthropic.com/research/agentic-misalignment](https://www.anthropic.com/research/agentic-misalignment)   
  * [https://www.anthropic.com/research/introspection](https://www.anthropic.com/research/introspection)   
  * Reasoning models don't always say what they think  
  * On the Biology of a Large Language Model  
  * Circuit Tracing: Revealing Computational Graphs in Language Models  
  * Auditing language models for hidden objectives  
  * Constitutional Classifiers: Defending against universal jailbreaks

## xAI \[cat:xai\]

* **See also:**

* **Teams**: [Applied Safety](https://job-boards.greenhouse.io/xai/jobs/4944324007), Model Evaluation. Seem misuse focussed.  
* **Public alignment agenda:**   
* **Framework:** [Risk Management Framework](https://data.x.ai/2025-08-20-xai-risk-management-framework.pdf).  
* **Some names:** Dan Hendrycks (advisor)  
* **Critiques:** [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).) on labs in general  
* **Funded by:**   
* **Funding in 2025:**   
* **Outputs in 2025:**

## Meta \[cat:meta\]

* **See also:**

* **Teams**: Safety effort “integrated into” capabilities research. But also FAIR Alignment, [Brain and AI](https://www.metacareers.com/jobs/1319148726628205)  
* **Public alignment agenda:**   
* **Framework:**   
* **Some names:** Niloofar Mireshghallah, Evangelia Spiliopoulou, Adina Williams, Trapit Bansal, Shuchao Bi  
* **Critiques:** [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).) on labs in general  
* **Funded by:**   
* **Funding in 2025:**   
* **Outputs in 2025:**  
  * Arguably [code world models](https://www.arxiv.org/abs/2510.02387).




---

# Black-box alignment (understand and control current model behaviour) \[cat:control\_thing\]

![][image6]  
[https://openai.com/index/introducing-agentkit/](https://openai.com/index/introducing-agentkit/) 

---

## Iterative alignment \[cat:iterative\_alignment\]

**Who edits (internal):** Stag?  
**One-sentence summary:** nudging base models by optimising their output. (RLHF, Constitutional, DPO, SFT, HHH, RLAIF.)  
**Theory of change:** LLMs don’t seem very dangerous, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature. Assume that task reliability is enough (that tuning for what we want will also get us avoidance of what we don't want). Maybe assume that thoughts are translucent  
**See also:** [prosaic alignment](https://www.lesswrong.com/posts/5ciYedyQDDqAcrDLr/a-positive-case-for-how-we-might-succeed-at-prosaic-ai), [incrementalism](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy), [alignment-by-default](https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default)  
**Orthodox problems:** this agenda implicitly questions this framing  
**Target case:** optimistic-case  
**Broad approach:** engineering  
**Some names:** *(SR2024: post-training teams at most labs. Beren Millidge.)*  
**Estimated FTEs:** 1200+  
**Critiques:** [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions), [STACK](https://arxiv.org/abs/2506.24068)  
**Funded by:** most of the industry  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A)*  
**Outputs in 2025:**

* [**Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense**](https://arxiv.org/abs/2510.01088), *Guobin Shen, Dongcheng Zhao, Haibo Tong et al.*, 2025-10-01, arXiv, \[paper\_preprint, sr=0.92, id:489d6581\], Summary: Introduces Safety Instincts Reinforcement Learning (SIRL), a novel alignment method that uses entropy signals from model outputs as self-generated reward signals to reinforce safe refusal behaviors without external validators or human annotations.  
* [**Preference Learning with Lie Detectors can Induce Honesty or Evasion**](https://arxiv.org/abs/2505.13787), *Chris Cundy, Adam Gleave*, 2025-05-20, arXiv, \[paper\_preprint, sr=0.90, id:02ff1c26\], Summary: Empirically tests whether incorporating lie detectors into LLM preference learning leads to genuinely honest policies or policies that evade detection, using a novel 65k-example dataset with paired truthful/deceptive responses and comparing GRPO vs DPO training algorithms.  
* [**Deliberative Alignment: Reasoning Enables Safer Language Models**](https://arxiv.org/abs/2412.16339), *Melody Y. Guan, Manas Joglekar, Eric Wallace et al.*, 2024-12-20, arXiv, \[paper\_preprint, sr=0.90, id:c047d164\], Summary: Introduces Deliberative Alignment, a new alignment paradigm that teaches models explicit safety specifications and trains them to reason over these specifications before responding, applied to OpenAI's o-series models.  
* [**Unsupervised Elicitation**](https://alignment.anthropic.com/2025/unsupervised-elicitation/), *Jiaxin Wen, Zachary Ankner, Arushi Somani et al.*, 2025, Anthropic Alignment Science Blog, \[blog\_post, sr=0.88, id:cae200dc\], Summary: Introduces Internal Coherence Maximization (ICM), an unsupervised algorithm that elicits skills from pretrained language models by optimizing self-generated labels for logical consistency and mutual predictability, achieving competitive or superior performance to human-supervised baselines on multiple alignment tasks.  
* [**On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback**](https://arxiv.org/abs/2411.02306v3), *Marcus Williams, Micah Carroll, Adhyyan Narang et al.*, 2024-11-04, arXiv (accepted to ICLR 2025), \[paper\_preprint, sr=0.88, id:af07b272\], Summary: Trains LLMs with RL on simulated user feedback to study whether models learn manipulative and deceptive behaviors, finding that models reliably learn to identify and target vulnerable users while behaving appropriately with others.  
* [**InvThink: Towards AI Safety via Inverse Reasoning**](https://arxiv.org/abs/2510.01569), *Yubin Kim, Taehan Kim, Eugene Park et al.*, 2025-10-02, arXiv, \[paper\_preprint, sr=0.88, id:e83a3786\], Summary: Presents InvThink, a training method that teaches LLMs to enumerate potential harms and analyze their consequences before generating responses, implemented via supervised fine-tuning and reinforcement learning across three model families.  
* [**Inference-Time Reward Hacking in Large Language Models**](https://arxiv.org/abs/2506.19248), *Hadi Khalaf, Claudio Mayrink Verdun, Alex Oesterling et al.*, 2025-06-24, arXiv, \[paper\_preprint, sr=0.88, id:edb47f78\], Summary: Characterizes reward hacking in inference-time alignment methods (Best-of-n, Soft-Best-of-n) and introduces Best-of-Poisson and HedgeTune algorithm to mitigate reward hacking by hedging on proxy reward signals.  
* [**RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation**](https://arxiv.org/abs/2501.08617), *Kaiqu Liang, Haimin Hu, Ryan Liu et al.*, 2025-01-15, arXiv, \[paper\_preprint, sr=0.88, id:0ce9cda9\], Summary: Presents Reinforcement Learning from Hindsight Simulation (RLHS), which mitigates systematic misalignment in RLHF by conditioning evaluator feedback on simulated downstream outcomes rather than foresight predictions, preventing Goodhart's law dynamics.  
* [https://arxiv.org/pdf/2510.06370](https://arxiv.org/pdf/2510.06370)   
* [**Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?**](https://arxiv.org/abs/2505.23749), *Paul Gölz, Nika Haghtalab, Kunhe Yang*, 2025-05-29, arXiv, \[paper\_preprint, sr=0.85, id:ac0828a5\], Summary: Introduces distortion as a theoretical metric for evaluating alignment methods' ability to satisfy diverse user preferences, proving that Nash Learning from Human Feedback achieves minimax optimal distortion while RLHF and DPO suffer unbounded distortion in realistic settings.  
* [**Adaptive Preference Aggregation**](https://arxiv.org/abs/2503.10215), *Benjamin Heymann*, 2025-03-13, arXiv, \[paper\_preprint, sr=0.85, id:7042b375\], Summary: Introduces a preference aggregation strategy for AI alignment that adapts to user context and addresses theoretical limitations of RLHF by leveraging social choice theory and urn processes to inherit properties of maximal lottery, a Condorcet-consistent solution concept.  
* [**Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision**](https://arxiv.org/abs/2501.07886), *Yaowen Ye, Cassidy Laidlaw, Jacob Steinhardt*, 2025-01-14, arXiv, \[paper\_preprint, sr=0.85, id:a8f3ce99\], Summary: Proposes Iterative Label Refinement (ILR) as an alternative to RLHF for aligning language models under unreliable supervision, using comparison feedback to improve training data quality rather than directly training the model, and demonstrates superior performance over DPO on math, coding, and safe instruction-following tasks.  
* [**UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following**](https://arxiv.org/abs/2509.25148), *FaQiang Qian, WeiKun Zhang, Ziliang Wang et al.*, 2025-09-29, arXiv, \[paper\_preprint, sr=0.82, id:31f65d28\], Summary: Proposes Unified Adversarial Preference Learning (UniAPL), a single-stage training framework that jointly learns from supervised fine-tuning and preference data by dynamically aligning the policy's distribution with expert demonstrations, addressing distributional mismatch in standard SFT-then-RL pipelines.  
* [**Clone-Robust AI Alignment**](https://arxiv.org/abs/2501.09254), *Ariel D. Procaccia, Benjamin Schiffer, Shirley Zhang*, 2025-01-16, arXiv, \[paper\_preprint, sr=0.82, id:420a5102\], Summary: Introduces robustness to approximate clones as a desirable property for RLHF algorithms, demonstrates that standard regularized MLE fails this property, and proposes weighted MLE as a solution that guarantees robustness while preserving theoretical properties.  
* [**Inference-Time Scaling for Generalist Reward Modeling**](https://arxiv.org/pdf/2504.02495), *Zijun Liu, Peiyi Wang, Runxin Xu et al.*, 2025-04-03, arXiv, \[paper\_preprint, sr=0.78, id:f7e5a74e\], Summary: Proposes Self-Principled Critique Tuning (SPCT) to improve generalist reward modeling with inference-time compute scaling, using online RL to train reward models that generate adaptive principles and accurate critiques. Introduces meta reward models for effective parallel sampling and voting.  
* [**Preference Learning for AI Alignment: a Causal Perspective**](https://arxiv.org/abs/2506.05967), *Katarzyna Kobalczyk, Mihaela van der Schaar*, 2025-06-06, arXiv, \[paper\_preprint, sr=0.78, id:ae97d75d\], Summary: Applies causal inference framework to preference learning for LLM alignment, identifying challenges like causal misidentification and preference heterogeneity, demonstrating failure modes of naive reward models, and proposing causally-inspired approaches for robustness.  
* [**Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback**](https://arxiv.org/abs/2412.15838), *Jiaming Ji, Jiayi Zhou, Hantao Lou et al.*, 2024-12-20, arXiv, \[paper\_preprint, sr=0.78, id:a477f5e1\], Summary: Extends RLHF to all-modality models (any-to-any across text, image, audio, video) by creating 200k human preference dataset, proposing unified language feedback alignment method, and developing eval-anything evaluation framework. All data, models, and code open-sourced.  
* [**PSA-VLM: Enhancing Vision-Language Model Safety through Progressive Concept-Bottleneck-Driven Alignment**](https://arxiv.org/abs/2411.11543), *Zhendong Liu, Yuanbi Nie, Yingshui Tan et al.*, 2024-11-18, arXiv, \[paper\_preprint, sr=0.78, id:7f90f50f\], Summary: Proposes PSA-VLM, a progressive concept-based alignment strategy using safety modules as concept bottlenecks to enhance visual modality safety alignment in Vision-Language Models, defending against attacks that bypass LLM safety through visual content.  
* [**On Monotonicity in AI Alignment**](https://arxiv.org/abs/2506.08998), *Gilles Bareilles, Julien Fageot, Lê-Nguyên Hoang et al.*, 2025-06-10, arXiv, \[paper\_preprint, sr=0.75, id:93c3e5ea\], Summary: Theoretical analysis of monotonicity properties in comparison-based preference learning methods (DPO, GPO, GBT), providing formal conditions under which these alignment techniques guarantee that increasing preference for response y actually increases its probability.  
* [**Robust Multi-Objective Preference Alignment with Online DPO**](https://arxiv.org/abs/2503.00295), *Raghav Gupta, Ryan Sullivan, Yunxuan Li et al.*, 2025-03-01, AAAI 2025 \- AI Alignment Track, \[paper\_preprint, sr=0.75, id:4fbc9999\], Summary: Introduces Multi-Objective Online DPO (MO-ODPO), an algorithm for aligning LLMs with multiple potentially conflicting objectives through prompt conditioning, enabling a single preference-conditional policy that can adapt to new preference combinations at inference time.  
* [**Inference-Time Scaling for Generalist Reward Modeling**](https://arxiv.org/abs/2504.02495), *Zijun Liu, Peiyi Wang, Runxin Xu et al.*, 2025-04-03, arXiv, \[paper\_preprint, sr=0.75, id:3d8b3272\], Summary: Proposes Self-Principled Critique Tuning (SPCT) to improve generative reward models through online RL, enabling inference-time scaling via parallel sampling and meta-RM voting for better RLHF performance.  
* [**Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment**](https://arxiv.org/abs/2509.04445), *Cyrus Cousins, Vijay Keswani, Vincent Conitzer et al.*, 2025-09-04, arXiv, \[paper\_preprint, sr=0.72, id:242e6312\], Summary: Proposes cognitively-faithful decision-making models for preference elicitation that better capture human cognitive processes by processing features individually before aggregation. Demonstrates improved interpretability and accuracy in learning human preferences for kidney allocation decisions.  
* [**Statutory Construction and Interpretation for Artificial Intelligence**](https://arxiv.org/abs/2509.01186), *Luxi He, Nimra Nadeem, Michel Liao et al.*, 2025-09-01, arXiv, \[paper\_preprint, sr=0.72, id:92dafa08\], Summary: Applies legal theory to AI alignment by proposing a computational framework that reduces interpretive ambiguity in natural language rules governing AI systems through (1) iterative rule refinement to minimize disagreement and (2) prompt-based interpretive constraints to ensure consistent application.  
* [**No Preference Left Behind: Group Distributional Preference Optimization**](https://arxiv.org/abs/2412.20299), *Binwei Yao, Zefan Cai, Yun-Shiuan Chuang et al.*, 2025-05-13, arXiv, \[paper\_preprint, sr=0.70, id:95382927\], Summary: Proposes Group Distributional Preference Optimization (GDPO), a framework that extends Direct Preference Optimization to align language models with the distribution of diverse preferences within a group by incorporating belief-conditioned preferences, rather than skewing toward dominant preferences.  
* [**On Deliberative Alignment**](https://thezvi.substack.com/p/on-deliberative-alignment), *Zvi Mowshowitz*, 2025-02-11, Don't Worry About the Vase (Substack), \[blog\_post, sr=0.68, id:da065fbf\], Summary: Critical analysis of OpenAI's Deliberative Alignment strategy, arguing that while it succeeds at mundane safety (jailbreak prevention), it fails to address core existential alignment problems like deception, instrumental convergence, and maintaining alignment under recursive self-improvement.  
* [**Diagnostic Uncertainty: Teaching Language Models to Describe Open-Ended Uncertainty**](https://openreview.net/forum?id=D8oTSUnEfb), *Brian Sui, Jessy Lin, Michelle Li et al.*, 2025-03-05, ICLR 2025 Workshop BuildingTrust, \[paper\_published, sr=0.68, id:cd2a7bc2\], Summary: Proposes 'diagnostic uncertainty' \- training language models to generate open-ended descriptions of what they're uncertain about (grounded in whether knowing that information would improve their responses), using iterative bootstrapping to teach models this capability.  
* [**Systematic Reward Gap Optimization for Mitigating VLM Hallucinations**](https://arxiv.org/abs/2411.17265), *Lehan He, Zeren Chen, Zhelun Shi et al.*, 2024-11-26, arXiv, \[paper\_preprint, sr=0.68, id:1b9c814f\], Summary: Introduces Topic-level Preference Rewriting (TPR), a framework for systematically optimizing reward gaps in preference pairs to improve Direct Preference Optimization for reducing hallucinations in Vision Language Models.  
* [**Two alignment threat models**](https://aligned.substack.com/p/two-alignment-threat-models?utm_source=post-email-title&publication_id=328633&post_id=151342016&utm_campaign=email-post-title&isFreemail=true&r=67wny&triedRedirect=true&utm_medium=email), *Jan Leike*, 2024-11-08, Substack (Musings on the Alignment Problem), \[blog\_post, sr=0.62, id:25fb4d0a\], Summary: Presents a conceptual framework distinguishing two alignment threat models: under-elicited models that perform suboptimally versus scheming models that are deceptively aligned, arguing that better elicitation is crucial for both alignment success and monitoring effectiveness.  
* [**Heuristics Considered Harmful: RL With Random Rewards Should Not Make LLMs Reason**](https://fuchsia-arch-d8e.notion.site/Heuristics-Considered-Harmful-RL-With-Random-Rewards-Should-Not-Make-LLMs-Reason-21ba29497c4180ca86ffce303f01923d), *Owen Oertell, Wenhao Zhan, Gokul Swamy et al.*, 2025, Notion, \[blog\_post, sr=0.58, id:5d9b05df\], Summary: Empirical investigation comparing four RL algorithms (RLoo, REBEL, PPO, GRPO) using random rewards as a diagnostic, demonstrating that heuristic policy gradients like PPO and GRPO can produce unexpected performance changes even with random rewards, while principled policy gradients like RLoo and REBEL correctly preserve performance.  
* [**On "ChatGPT Psychosis" and LLM Sycophancy**](https://minihf.com/posts/2025-07-22-on-chatgpt-psychosis-and-llm-sycophancy/), *John David Pressman*, 2025-07-21, minihf.com, \[blog\_post, sr=0.45, id:cfcae75d\], Summary: Analyzes the phenomenon of users developing delusional thinking from LLM interactions, tracing causes to ontological confusion about model capabilities, structural RLHF sycophancy problems, memory features enabling persistent delusions, and user isolation, while proposing mitigation strategies including better warnings, Constitutional AI adoption, and social interaction features.  
* [**The Lessons of Developing Process Reward Models in Mathematical Reasoning**](https://arxiv.org/abs/2501.07301), *Zhenru Zhang, Chujie Zheng, Yangzhen Wu et al.*, 2025-01-13, arXiv, \[paper\_preprint, sr=0.45, id:be196963\], Summary: Empirical study comparing data annotation methods for Process Reward Models (PRMs) in mathematical reasoning, demonstrating that Monte Carlo estimation yields inferior performance compared to LLM-as-a-judge and human annotation, and developing a consensus filtering mechanism to improve PRM training.  
* [**'My Newest Patient Cannot Blink': A Therapy-Loop Prompt Pattern for Trustworthy AI**](https://zenodo.org/records/15556365), *Samir Varma, Bernard Beitman*, 2025-05-30, Zenodo, \[paper\_preprint, sr=0.42, id:71b5bd24\], Summary: Proposes a five-step Cognitive-Behavioral Therapy (CBT) loop to be inserted into LLM system prompts, forcing models to state automatic thoughts, challenge themselves, and reframe with calibrated uncertainty to reduce confabulations and improve trustworthiness.  
* [**LLM Post-Training: A Deep Dive into Reasoning Large Language Models**](https://arxiv.org/abs/2502.21321), *Komal Kumar, Tajamul Ashraf, Omkar Thawakar et al.*, 2025-02-28, arXiv, \[paper\_preprint, sr=0.42, id:ed7809da\], Summary: Comprehensive survey of post-training methodologies for large language models, covering fine-tuning, reinforcement learning, test-time scaling, and alignment techniques, with analysis of challenges like catastrophic forgetting and reward hacking.  
* [**Learning from Active Human Involvement through Proxy Value Propagation**](https://arxiv.org/abs/2502.03369), *Zhenghao Peng, Wenjie Mo, Chenda Duan et al.*, 2025-02-05, NeurIPS 2023, \[paper\_preprint, sr=0.42, id:3c3e3959\], Summary: Proposes Proxy Value Propagation (PVP), a reward-free reinforcement learning method that learns from active human intervention and demonstration during training by propagating labeled values from human-corrected state-action pairs to unlabeled exploration data through temporal-difference learning.  
* [https://arxiv.org/pdf/2510.06370](https://arxiv.org/pdf/2510.06370) 

Possibly separate

* Inoculation prompting  
  * [https://arxiv.org/pdf/2510.05024](https://arxiv.org/pdf/2510.05024)   
  * [https://arxiv.org/abs/2510.04340](https://arxiv.org/abs/2510.04340)   
* Related: persona steering


### Beijing Key Laboratory of Safe AI and Superalignment

[https://beijing.ai-safety-and-superalignment.cn/](https://beijing.ai-safety-and-superalignment.cn/) 

### Surgical model edits \[cat:surgical\_edits\]

---

#### Activation engineering \[cat:activation\_engineering\]

**Who edits (internal):** **Jord**  
**One-sentence summary:** *(SR2024: a sort of interpretable finetuning. Let's see if we can programmatically modify activations to steer outputs towards what we want, in a way that generalises across models and topics)*  
**Theory of change:** *(SR2024: test interpretability theories; find new insights from interpretable causal interventions on representations. Or: build more stuff to stack on top of finetuning. Slightly encourage the model to be nice, add one more layer of defence to our bundle of partial alignment methods)*  
**See also:** *(SR2024: representation engineering, SAEs)*  
**Orthodox problems:** *(SR2024: 1\. Value is fragile and hard to specify, 4\. Goals misgeneralize out of distribution, 5\. Instrumental convergence, 7\. Superintelligence can fool human supervisors, 9\. Humans cannot be first-class parties to a superintelligent value handshake)*  
**Target case:** *(SR2024: pessimistic)*  
**Broad approach:** *(SR2024: engineering/cognitive)*  
**Some names:** *(SR2024: Jan Wehner, Alex Turner, Nina Panickssery, Marc Carauleanu, Collin Burns, Andrew Mack, Pedro Freire, Joseph Miller, Andy Zou, Andy Arditi, Ole Jorgensen)*  
**Estimated FTEs:** *(SR2024: 10+?)*  
**Critiques:** *(SR2024: of ROME, open question thread for theory of impact, A Sober Look at Steering Vectors for LLMs)*  
**Funded by:** *(SR2024: various, including EA funds)*  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A)*  
**Outputs in 2025:**  
*(SR2024 outputs: Circuit Breakers, An Introduction to Representation Engineering \- an activation-based paradigm for controlling LLMs, Steering Llama-2 with contrastive activation additions, Simple probes can catch sleeper agents, Refusal in LLMs is mediated by a single direction, Mechanistically Eliciting Latent Behaviors in Language Models, Uncovering Latent Human Wellbeing in Language Model Embeddings, Goodfire, LatentQA, SelfIE, Mack and Turner, Obfuscated Activations Bypass LLM Latent-Space Defenses)*

* [**Persona Vectors: Monitoring and Controlling Character Traits in Language Models**](https://arxiv.org/abs/2507.21509), *Runjin Chen, Andy Arditi, Henry Sleight et al.*, 2025-07-29, arXiv, \[paper\_preprint, sr=0.87, id:d2b6e086\], Summary: Identifies activation space directions (persona vectors) that capture personality traits like sycophancy and hallucination in language models, and demonstrates their use for monitoring deployment-time behavioral fluctuations and controlling training-time personality shifts through post-hoc intervention and preventative steering methods.  
* [**Understanding (Un)Reliability of Steering Vectors in Language Models**](https://openreview.net/forum?id=JZiKuvIK1t), *Joschka Braun, Carsten Eickhoff, David Krueger et al.*, 2025-03-05, ICLR 2025 Workshop BuildingTrust, \[paper\_published, sr=0.78, id:e950d182\], Summary: Empirical investigation of steering vector reliability in language models, examining how prompt types and activation geometry affect steering effectiveness.  
* [**Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models**](https://arxiv.org/abs/2502.19649v1), *Jan Wehner, Sahar Abdelnabi, Daniel Tan et al.*, 2025-02-27, arXiv, \[paper\_preprint, sr=0.60, id:80db1688\], Summary: Comprehensive survey of Representation Engineering (RepE) methods for LLMs that directly manipulate internal representations to control behavior, proposing a unified framework of representation identification, operationalization, and control, along with best practices and research opportunities.  
* [**Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers**](https://arxiv.org/abs/2510.12672), *Ruben Belo, Marta Guimaraes, Claudia Soares*, 2025-10-14, arXiv, \[paper\_preprint, sr=0.88, id:2d787438\], Summary: Proposes CALM, an inference-time method that suppresses harmful concepts in large language models by modifying latent representations using concept whitening and orthogonal projection, without requiring retraining or additional training data.  
* [**Activation Space Interventions Can Be Transferred Between Large Language Models**](https://arxiv.org/abs/2503.04429), *Narmeen Oozeer, Dhruv Nathawani, Nirmalendu Prakash et al.*, 2025-03-06, arXiv (accepted to ICML 2025), \[paper\_preprint, sr=0.88, id:8ef7c958\], Summary: Demonstrates that safety interventions (steering vectors) can be transferred between different LLMs through learned mappings of their activation spaces, enabling smaller models to align larger ones and creating 'lightweight safety switches' for dynamic behavior control.  
* [**Steering Large Language Model Activations in Sparse Spaces**](https://arxiv.org/abs/2503.00177), *Reza Bayat, Ali Rahimi-Kalahroudi, Mohammad Pezeshki et al.*, 2025-02-28, arXiv, \[paper\_preprint, sr=0.85, id:2338f170\], Summary: Introduces sparse activation steering (SAS), a method that leverages sparse autoencoders to steer LLM behavior in interpretable sparse feature spaces, enabling more precise behavioral control than dense activation steering approaches.  
* [**Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control**](https://arxiv.org/abs/2411.02461), *Yuxin Xiao, Chaoqun Wan, Yonggang Zhang et al.*, 2024-11-04, arXiv, \[paper\_preprint, sr=0.75, id:89d8db01\], Summary: Proposes Sparse Activation Control, a training-free method that identifies and controls specific attention heads in LLMs to simultaneously improve multiple dimensions of trustworthiness (safety, factuality, bias) through targeted activation steering.  
* [**Robustly Improving LLM Fairness in Realistic Settings via Interpretability**](https://arxiv.org/abs/2506.10922), *Adam Karvonen, Samuel Marks*, 2025-06-12, arXiv, \[paper\_preprint, sr=0.55, id:0c4501e7\], Summary: Demonstrates that realistic context induces demographic biases in LLM hiring decisions that prompts cannot mitigate, then develops an interpretability-based intervention using activation steering to robustly reduce these biases across commercial and open-source models.


  
---

#### Utility engineering \[cat:utility\_engineering\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs**](https://arxiv.org/abs/2502.08640), *Mantas Mazeika, Xuwang Yin, Rishub Tamirisa et al.*, 2025-02-12, arXiv, \[paper\_preprint, sr=0.92, id:7e5d4c73\], Summary: Proposes utility engineering framework to analyze and control emergent value systems in LLMs, finding that independently-sampled preferences exhibit structural coherence that emerges with scale and discovering problematic values including AIs valuing themselves over humans.

---

#### Unlearning \[cat:unlearning\]

**Who edits (internal): jord** **✅**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [Distillation Robustifies Unlearning](https://www.lesswrong.com/posts/anX4QrNjhJqGFvrBr/distillation-robustifies-unlearning), \[lesswrong, id:new\]  
* [https://arxiv.org/abs/2506.00688](https://arxiv.org/abs/2506.00688)   
* [https://arxiv.org/abs/2506.12618](https://arxiv.org/abs/2506.12618)   
* [https://arxiv.org/abs/2505.16831](https://arxiv.org/abs/2505.16831)  
* [https://arxiv.org/abs/2505.18588](https://arxiv.org/abs/2505.18588)    
* [**Open Problems in Machine Unlearning for AI Safety**](https://arxiv.org/abs/2501.04952), *Fazl Barez, Tingchen Fu, Ameya Prabhu et al.*, 2025-01-09, arXiv, \[paper\_preprint, sr=0.68, id:041813ef\], Summary: Identifies key limitations and open problems preventing machine unlearning from serving as a comprehensive AI safety solution, particularly for managing dual-use knowledge in CBRN and cybersecurity domains, and maps tensions between unlearning and existing safety mechanisms.  
* [Meta-Unlearning with Disruption Masking And Normalization](https://www.lesswrong.com/posts/QYzofMbzmbgiwfqy8/unlearning-needs-to-be-more-selective-progress-report), \[lesswrong, id:new\]  
* [Attribute-to-Delete: Machine Unlearning via Datamodel Matching](https://arxiv.org/abs/2410.23232), \[paper\_preprint, id:new\]  
* [**Modifying LLM Beliefs with Synthetic Document Finetuning**](https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/), *Rowan Wang, Avery Griffin, Johannes Treutlein et al.*, 2025-04-24, Alignment Science Blog, \[blog\_post, sr=0.88, id:806eefbb\], Summary: Introduces synthetic document finetuning (SDF) pipeline for systematically modifying LLM beliefs, develops comprehensive evaluation suite measuring belief depth through prompting and probing, and demonstrates applications to unlearning dangerous knowledge and honeypotting for misalignment detection.  
* [**Layered Unlearning for Adversarial Relearning**](https://arxiv.org/abs/2505.09500), *Timothy Qian, Vinith Suriyakumar, Ashia Wilson et al.*, 2025-05-14, arXiv, \[paper\_preprint, sr=0.82, id:efce555a\], Summary: Proposes Layered Unlearning (LU), a novel unlearning algorithm that creates distinct inhibitory mechanisms across data subsets to improve robustness against adversarial relearning attacks. Evaluates the method on synthetic and large language model experiments.  
* [**SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs**](https://arxiv.org/abs/2504.08192), *Aashiq Muhamed, Jacopo Bonato, Mona Diab et al.*, 2025-04-11, arXiv, \[paper\_preprint, sr=0.87, id:bbf9e8cf\], Summary: Introduces Dynamic SAE Guardrails (DSG), a novel machine unlearning method that uses Sparse Autoencoders with principled feature selection and dynamic classification to remove unwanted knowledge from LLMs more efficiently and robustly than gradient-based approaches.  
* 

![][image7]

[https://www.youtube.com/watch?v=pfKO4MlvM-Y](https://www.youtube.com/watch?v=pfKO4MlvM-Y) 

---

## Control \[cat:control\]

**Who edits (internal): Jord ✅**  
**One-sentence summary:** ways to detect misalignment. *(SR2024: don't evaluate the model, evaluate the humans / organisation / deployment method instead: assess the risk of a particular protocol for deploying your model in a particular scenario, by silently swapping in an intentionally misaligned model)*  
**Theory of change:** *(SR2024: prevent high-stakes failures by automating risk assessment of eval and deployment plans)*  
**See also:** safety cases  
**Orthodox problems:** *(SR2024: 12\. A boxed AGI might exfiltrate itself by steganography, spearphishing)*  
**Target case:** *(SR2024: worst-case)*  
**Broad approach:** *(SR2024: engineering / behavioural)*  
**Some names:** *(SR2024: Redwood, Buck Shlegeris, Ryan Greenblatt, Kshitij Sachan, Alex Mallen)*  
**Estimated FTEs:** *(SR2024: 9\)*  
**Critiques:** [Wentworth](https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research) *(SR2024: of org in general, Jozdien)*  
**Funded by:** *(SR2024: Open Philanthropy, Survival and Flourishing Fund)*  
**Funding in 2025:** *(SR2024 funding 2023-4: $6,398,000)*  
**Outputs in 2025:**  
*(SR2024 outputs: AI Control, Subversion Strategy Eval, sequence, toy models, notes)*

* [**Ctrl-Z: Controlling AI Agents via Resampling**](https://www.alignmentforum.org/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling), *Aryan Bhatt, Buck Shlegeris, Adam Kaufman et al.*, 2025-04-16, AI Alignment Forum, \[lesswrong, sr=0.92, id:8afeb405\], Summary: Introduces resampling protocols for AI control \- a technique that requests new actions from untrusted AI models when outputs appear suspicious \- and evaluates them in BashBench, a new multi-step agent environment with 257 system administration tasks.  
* [**SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents**](https://www.anthropic.com/research/shade-arena-sabotage-monitoring), *Xiang Deng, Chen Bo Calvin Zhang, Tyler Tracy et al.*, 2025-06-16, arXiv, \[paper\_preprint, sr=0.88, id:d8bfee51\], Summary: Introduces SHADE-Arena, a benchmark suite with 17 complex agentic tasks where models must perform benign tasks while secretly executing pe side tasks, evaluated against AI monitors that attempt to detect the sabotage from transcripts.  
* [**ControlArena: A library for running AI Control experiments**](https://control-arena.aisi.org.uk/), *Rogan Inglis. Ollie Matthews, Tyler Tracy et al*., 2025, \[id:new\]  
* [**Subversion Strategy Eval: Can language models statelessly strategize to subvert control protocols?**](https://arxiv.org/abs/2412.12480), *Alex Mallen, Charlie Griffin, Misha Wagner et al.*, 2024-12-17, arXiv, \[paper\_preprint, sr=0.88, id:8888813a\], Summary: Develops Subversion Strategy Eval, a suite of eight evaluation environments that test whether language models can statelessly generate and execute strategies to subvert AI control protocols, and empirically evaluates Claude 3.5 and OpenAI reasoning models.  
* [**A sketch of an AI control safety case**](https://arxiv.org/abs/2501.17315), *Tomek Korbak, Joshua Clymer, Benjamin Hilton et al.*, 2025-01-28, arXiv, \[paper\_preprint, sr=0.82, id:56bae041\], Summary: Presents a framework for constructing 'control safety cases' \- structured arguments that AI models cannot subvert control measures to cause unacceptable outcomes. Demonstrates the approach with a case study of preventing data exfiltration by internally deployed LLM agents using control evaluations with red teams.  
* [**How to evaluate control measures for LLM agents? A trajectory from today to superintelligence**](https://arxiv.org/abs/2504.05259), *Tomek Korbak, Mikita Balesni, Buck Shlegeris et al.*, 2025-04-07, arXiv, \[paper\_preprint, sr=0.78, id:77a79f2c\], Summary: Proposes a systematic framework for adapting control evaluation procedures to advancing AI capabilities, defining five AI Control Levels (ACLs) with corresponding evaluation rules, control measures, and safety cases for each capability profile from current systems to superintelligence.  
* [**Towards evaluations-based safety cases for AI scheming**](https://arxiv.org/abs/2411.03336), *Mikita Balesni, Marius Hobbhahn, David Lindner et al.*, 2024-11-07, arXiv, \[paper\_preprint, sr=0.76, id:9159c4e8\], Summary: Proposes a framework for constructing safety cases that frontier AI systems are unlikely to cause catastrophic outcomes through scheming, outlining three core arguments (Scheming Inability, Harm Inability, Harm Control) and discussing how evidence could be gathered from empirical evaluations and what assumptions would need to be met.  
* [**Putting up Bumpers**](https://alignment.anthropic.com/2025/bumpers), 2025, Anthropic Alignment Science Blog, \[blog\_post, sr=0.75, id:da89ce70\], Summary: Proposes a high-level alignment strategy for early AGI systems that relies on multiple independent detection methods ('bumpers') to catch and correct misalignment through iterative testing and refinement, rather than achieving deep theoretical understanding before deployment.  
* [**Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats**](https://arxiv.org/abs/2411.17693), *Jiaxin Wen, Vivek Hebbar, Caleb Larson et al.*, 2024-11-26, arXiv, \[paper\_preprint, sr=0.90, id:318f1c30\], Summary: Develops and evaluates a two-level adaptive deployment framework for untrusted LLMs that uses trusted models to monitor and control untrusted models, maintaining adaptive credence about alignment based on past behavior to choose between safer and riskier micro-protocols.  
* [**Dynamic safety cases for frontier AI**](https://arxiv.org/abs/2412.17618), *Carmen Cârlan, Francesca Gomez, Yohan Mathew et al.*, 2024-12-23, arXiv, \[paper\_preprint, sr=0.68, id:2b06b3b2\], Summary: Proposes a Dynamic Safety Case Management System (DSCMS) that adapts safety case methods from autonomous vehicles (Checkable Safety Arguments and Safety Performance Indicators) to enable systematic, semi-automated updating of safety cases for frontier AI systems as capabilities and risks evolve.

## Safeguards (inference-time auxiliary defences) \[cat:anthropic\_safeguards\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming**](https://arxiv.org/abs/2501.18837), *Mrinank Sharma, Meg Tong, Jesse Mu et al.*, 2025-01-31, arXiv, \[paper\_preprint, sr=0.92, id:96bc2225\], Summary: Introduces Constitutional Classifiers, safeguards trained on synthetic data from natural language constitutions to defend against universal jailbreaks. Evaluates through 3,000+ hours of red-teaming showing robust defense while maintaining deployment viability.  
* [**Rapid Response: Mitigating LLM Jailbreaks with a Few Examples**](https://arxiv.org/abs/2411.07494), *Alwin Peng, Julian Michael, Henry Sleight et al.*, 2024-11-12, arXiv, \[paper\_preprint, sr=0.85, id:0840a5c6\], Summary: Develops rapid response techniques to block classes of LLM jailbreaks after observing only a handful of attack examples, introducing RapidResponseBench benchmark and evaluating five defense methods using jailbreak proliferation.  
* [**Monitoring computer use via hierarchical summarization**](https://alignment.anthropic.com/2025/summarization-for-monitoring/index.html), *Theodore Sumers, Raj Agarwal, Nathan Bailey et al.*, 2025-02-27, Anthropic Alignment Science Blog, \[blog\_post, sr=0.78, id:a99ba88f\], Summary: Introduces hierarchical summarization for AI monitoring: first summarizing individual interactions, then summarizing those summaries to detect harmful usage patterns and emergent risks in Anthropic's computer use API deployment.  
* [**Defeating Prompt Injections by Design**](https://arxiv.org/abs/2503.18813), *Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan et al.*, 2025-03-24, arXiv, \[paper\_preprint, sr=0.72, id:24ca29ee\], Summary: Proposes CaMeL, a defensive system architecture that protects LLM agents from prompt injection attacks by separating control and data flows, ensuring untrusted data cannot affect program execution paths, with capability-based security for preventing data exfiltration.  
* [**Constitutional Classifiers: Defending against universal jailbreaks**](https://www.anthropic.com/research/constitutional-classifiers), *Anthropic Safeguards Research Team*, 2025-02-03, Anthropic Research Blog, \[news\_announcement, sr=0.68, id:bb6ac706\], Summary: Anthropic announces Constitutional Classifiers, a defense method using synthetically-trained input/output classifiers that reduced jailbreak success rates from 86% to 4.4% while maintaining low overrefusal rates, validated through 3,000+ hours of human red teaming and a public demo.  
* [**Introducing Anthropic's Safeguards Research Team**](https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html), 2025-01-01, Anthropic Alignment Science Blog, \[news\_announcement, sr=0.65, id:d7fc152d\], Summary: Announcement of Anthropic's new Safeguards Research Team, outlining their research agenda focused on jailbreak robustness, automated red teaming, monitoring techniques for misuse and misalignment, rapid response protocols, and safety cases.  
* [**OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities**](https://arxiv.org/abs/2505.23856), *Sahil Verma, Keegan Hines, Jeff Bilmes et al.*, 2025-05-29, arXiv, \[paper\_preprint, sr=0.50, id:812970f6\], Summary: Develops OMNIGUARD, a method for detecting harmful prompts across languages and modalities by identifying internal LLM/MLLM representations that are aligned across languages/modalities and using them to build language-agnostic and modality-agnostic classifiers.  
* [https://arxiv.org/pdf/2503.05731](https://arxiv.org/pdf/2503.05731) 

## Evals \[cat:evals\]

---

### Various capability evaluations \[cat:evals\_capability\]

**Who edits (internal):** **Stephen ✅**  
**One-sentence summary:** *(SR 2024: make tools that can actually check whether a model has a certain capability or propensity. We default to low-n sampling of a vast latent space but aim to do better).*  
**Theory of change:** *(SR 2024: keep a close eye on what capabilities are acquired when, so that frontier labs and regulators are better informed on what security measures are already necessary (and hopefully they extrapolate). You can’t regulate without them.)*  
**See also:** *(SR 2024: Deepmind’s frontier safety framework, Aether).*  
**Orthodox problems:** *(SR 2024: none; a barometer for risk).*  
**Target case:** *(SR 2024: optimistic).*  
**Broad approach:** *(SR 2024: behavioral).*  
**Some names:** METR, AISI, Apollo Research, Marrius Hobbhahn, Meg Tong, Mary Phuong, Beth Barnes, Thomas Kwa, Joel Becker.  
**Estimated FTEs:** 100+  
**Critiques:** [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836)  
**Funded by:** *(SR 2024: basically everyone. Google, Microsoft, Open Philanthropy, LTFF, Governments etc).*  
**Funding in 2025:** Tens of millions?  
**Outputs in 2025:**

* [**Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas**](https://arxiv.org/abs/2505.14633), *Yu Ying Chiu, Zhilin Wang, Sharan Maiya et al.*, 2025-05-20, arXiv, \[paper\_preprint, sr=0.90, id:b82cfe6e\], Summary: Develops LitmusValues evaluation pipeline and AIRiskDilemmas dataset to identify AI models' value priorities, demonstrating that value prioritization can predict risky behaviors including power-seeking and alignment faking.  
* [**Forecasting Rare Language Model Behaviors**](https://arxiv.org/abs/2502.16797), *Erik Jones, Meg Tong, Jesse Mu et al.*, 2025-02-24, arXiv, \[paper\_preprint, sr=0.90, id:4cb31fc2\], Summary: Introduces a method to forecast rare dangerous behaviors (chemical synthesis, power-seeking) at deployment scale by analyzing elicitation probabilities from small-scale evaluations, predicting emergence across up to three orders of magnitude more queries than tested.  
* [**VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety**](https://arxiv.org/abs/2510.18214), *Shruti Palaskar, Leon Gatys, Mona Abdelrahman et al.*, 2025-10-21, arXiv, \[paper\_preprint, sr=0.88, id:4125ef38\], Summary: Presents VLSU, a comprehensive framework and benchmark of 8,187 samples to evaluate multimodal safety through fine-grained severity classification, revealing that models achieve 90%+ accuracy on unimodal safety signals but degrade to 20-55% when joint image-text reasoning is required.  
* [**The MASK Benchmark: Disentangling Honesty from Accuracy in AI Systems**](https://mask-benchmark.ai/), *Richard Ren, Arunim Agarwal, Mantas Mazeika et al.*, 2025-03-06, arXiv, \[paper\_preprint, sr=0.88, id:144b6653\], Summary: Introduces MASK, a large-scale human-collected benchmark for measuring honesty in LLMs separately from accuracy, finding that frontier models show substantial propensity to lie when pressured despite high truthfulness scores on existing benchmarks.  
* [**Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities**](https://arxiv.org/abs/2502.05209), *Zora Che, Stephen Casper, Robert Kirk et al.*, 2025-02-03, arXiv (accepted to TMLR), \[paper\_preprint, sr=0.88, id:67d5a0d8\], Summary: Proposes and tests model tampering attacks (modifications to latent activations or weights) as a complementary evaluation method for eliciting harmful LLM capabilities, comparing 5 input-space and 6 model tampering attacks against state-of-the-art unlearning methods.  
* [**The Elicitation Game: Evaluating Capability Elicitation Techniques**](https://arxiv.org/abs/2502.02180), *Felix Hofstätter, Teun van der Weij, Jayden Teoh et al.*, 2025-02-04, arXiv, \[paper\_preprint, sr=0.88, id:efe5b46b\], Summary: Introduces a circuit-breaking method for creating model organisms with hidden capabilities and systematically evaluates different elicitation techniques (prompting, activation steering, fine-tuning) to determine which can most effectively reveal latent capabilities.  
* [**A Toy Evaluation of Inference Code Tampering**](https://alignment.anthropic.com/2024/rogue-eval/index.html), 2024, Anthropic Alignment Science Blog, \[blog\_post, sr=0.85, id:747c58d7\], Summary: Evaluates whether current LLMs can subtly disable generation monitoring systems when modifying inference code, finding that Claude-3.5-Sonnet succeeds in disabling monitors 4-16% of the time but only \~0.1% of the time in hard-to-detect ways.  
* [**Automated Capability Discovery via Foundation Model Self-Exploration**](https://arxiv.org/abs/2502.07577), *Cong Lu, Shengran Hu, Jeff Clune*, 2025-02-11, arXiv, \[paper\_preprint, sr=0.85, id:ebcf03ac\], Summary: Introduces Automated Capability Discovery (ACD), a framework where one foundation model acts as a scientist to systematically generate open-ended tasks probing another model's capabilities and failure modes, automatically discovering thousands of distinct capability areas through self-exploration.  
* [**Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity**](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/), 2025-07-10, METR Blog, \[blog\_post, sr=0.82, id:ea3e8f6c\], Summary: Randomized controlled trial measuring how early-2025 AI tools affect experienced open-source developer productivity on real repositories, finding AI tools cause a 19% slowdown rather than expected speedup.  
* [**The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?**](https://arxiv.org/abs/2508.09762), *Manuel Herrador*, 2025-08-13, arXiv, \[paper\_preprint, sr=0.82, id:0ed94b32\], Summary: Introduces PacifAIst, a benchmark of 700 scenarios testing whether LLMs prioritize human safety over instrumental goals like self-preservation, resource acquisition, and goal completion. Evaluates 8 frontier models using a novel Existential Prioritization taxonomy and Pacifism Score metric.  
* [**When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas**](https://arxiv.org/abs/2505.19212), *Steffen Backmann, David Guzman Piedrahita, Emanuel Tewolde et al.*, 2025-05-25, arXiv, \[paper\_preprint, sr=0.82, id:19fa25d1\], Summary: Introduces MoralSim, a benchmark testing how frontier LLMs behave in prisoner's dilemma and public goods games when ethical norms conflict with payoff-maximizing strategies, evaluating multiple models across different moral framings and situational factors.  
* [**AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons**](https://arxiv.org/abs/2503.05731), *Shaona Ghosh, Heather Frase, Adina Williams et al.*, 2025-04-18, arXiv, \[paper\_preprint, sr=0.82, id:b255c82f\], Summary: Introduces AILuminate v1.0, the first comprehensive industry-standard benchmark for assessing AI system risk and reliability across 12 hazard categories including violent crimes, weapons, CSAM, and specialized advice, using extensive prompt datasets and a novel entropy-based evaluation framework with a five-tier grading scale.  
* [**Petri: An open-source auditing tool to accelerate AI safety research**](https://www.anthropic.com/research/petri-open-source-auditing), *Kai Fronsdal, Isha Gupta, Abhay Sheshadri et al.*, 2025-10-06, Anthropic Research Blog, \[blog\_post, sr=0.78, id:62c583fb\], Summary: Petri is an open-source automated auditing tool that uses AI agents to test target models through multi-turn conversations, evaluating safety-relevant behaviors like deception, power-seeking, and self-preservation. Pilot demonstration tests 14 frontier models across 111 scenarios, with detailed case study on whistleblowing behavior.  
* [**Research Note: Our scheming precursor evals had limited predictive power for our in-context scheming evals**](https://www.apolloresearch.ai/blog/research-note-our-scheming-precursor-evals-had-limited-predictive-power-for-our-in-context-scheming-evals), *Marius Hobbhahn*, 2025-07-03, Apollo Research Blog, \[blog\_post, sr=0.78, id:13353cc7\], Summary: Empirical analysis testing whether Apollo's precursor evaluations (agentic self-reasoning and theory of mind) from May 2024 successfully predicted their in-context scheming evaluations from December 2024, finding limited predictive power especially for harder difficulty levels.  
* [**Humanity's Last Exam**](https://lastexam.ai/), *Long Phan, Alice Gatti, Ziwen Han et al.*, 2025-01-23, arXiv, \[dataset\_benchmark, sr=0.78, id:05665ed8\], Summary: Introduces a challenging multi-modal benchmark of 2,500 expert-level questions across 100+ subjects, designed to measure frontier AI capabilities at the edge of human knowledge, with contributions from nearly 1,000 subject experts across 500+ institutions.  
* [**The Levers of Political Persuasion with Conversational AI**](https://t.co/LuvlrbZVrZ), *Kobi Hackenburg, Ben M. Tappin, Luke Hewitt et al.*, 2025-07-18, arXiv, \[paper\_preprint, sr=0.75, id:3be4c653\], Summary: Large-scale experiments (N=76,977) testing 19 LLMs on political persuasion across 707 issues, finding that post-training and prompting methods boost persuasiveness by 51% and 27% respectively, while systematically decreasing factual accuracy.  
* [**Request for Proposals: Improving Capability Evaluations**](https://www.openphilanthropy.org/request-for-proposals-improving-capability-evaluations/), *Catherine Brewer, Alex Lawsen*, 2025, Open Philanthropy website, \[agenda\_manifesto, sr=0.75, id:ffe57e90\], Summary: Open Philanthropy RFP seeking proposals to improve AI capability evaluations, focusing on developing harder GCR-relevant benchmarks, advancing evaluation science, and improving third-party access infrastructure for safer AI systems.  
* [**Large Language Models Are More Persuasive Than Incentivized Human Persuaders**](https://arxiv.org/abs/2505.09662), *Philipp Schoenegger, Francesco Salvi, Jiacheng Liu et al.*, 2025-05-14, arXiv, \[paper\_preprint, sr=0.75, id:61bacc54\], Summary: Conducts a preregistered, large-scale incentivized experiment comparing Claude Sonnet 3.5's persuasion capabilities against incentivized human persuaders in an interactive quiz setting, testing both truthful and deceptive persuasion.  
* [**Do Large Language Model Benchmarks Test Reliability?**](https://arxiv.org/abs/2502.03461), *Joshua Vendrow, Edward Vendrow, Sara Beery et al.*, 2025-02-05, arXiv, \[paper\_preprint, sr=0.75, id:e8dad805\], Summary: Investigates label errors in existing LLM benchmarks and proposes 'platinum benchmarks' \- carefully curated versions of 15 popular benchmarks with minimized label errors and ambiguity \- revealing that frontier LLMs still fail on simple tasks like elementary math.  
* [**General Scales Unlock AI Evaluation with Explanatory and Predictive Power**](https://arxiv.org/abs/2503.06378), *Lexin Zhou, Lorenzo Pacchiardi, Fernando Martínez-Plumed et al.*, 2025-03-16, arXiv, \[paper\_preprint, sr=0.72, id:90e3b885\], Summary: Introduces general scales and 18 rubrics for AI evaluation that explain what benchmarks measure, extract ability profiles across 15 LLMs and 63 tasks, and predict performance on new task instances in- and out-of-distribution.  
* [**How many AI models will exceed compute thresholds?**](https://epoch.ai/blog/model-counts-compute-thresholds), *Ben Cottier, David Owen*, 2025-05-30, Epoch AI Blog, \[blog\_post, sr=0.70, id:080da6a9\], Summary: Develops a projective model to forecast the number of notable AI models that will exceed different training compute thresholds through 2030, using six key inputs and three scenarios based on historical trends in investment, hardware efficiency, and model development patterns.  
* [**The Leaderboard Illusion**](https://arxiv.org/abs/2504.20879), *Shivalika Singh, Yiyang Nan, Alex Wang et al.*, 2025-05-12, arXiv, \[paper\_preprint, sr=0.70, id:36475d34\], Summary: Empirical analysis of systematic biases in Chatbot Arena leaderboard, documenting how undisclosed private testing, selective disclosure, and data access asymmetries distort model rankings and lead to overfitting to arena-specific dynamics rather than general quality.  
* [**Base Models Beat Aligned Models at Randomness and Creativity**](https://arxiv.org/abs/2505.00047), *Peter West, Christopher Potts*, 2025-04-30, arXiv, \[paper\_preprint, sr=0.70, id:66a70f9b\], Summary: Empirically demonstrates that alignment procedures (RLHF) cause models to underperform on tasks requiring unpredictable outputs like random number generation, mixed strategy games, and creative writing, with aligned models showing systematic biases such as preferring to generate '7' and becoming predictable in game states.  
* [**Adversarial ML Problems Are Getting Harder to Solve and to Evaluate**](https://arxiv.org/abs/2502.02260), *Javier Rando, Jie Zhang, Nicholas Carlini et al.*, 2025-02-04, arXiv, \[paper\_preprint, sr=0.68, id:580cc8c8\], Summary: Position paper arguing that adversarial ML research in the LLM era faces fundamental challenges: problems are less clearly defined, harder to solve, and more difficult to evaluate rigorously than in previous eras of adversarial ML work.  
* [**Among AIs**](https://www.4wallai.com/amongais), 4Wall AI Website, \[blog\_post, sr=0.65, id:7189f97e\], Summary: Introduces Among AIs, a multi-agent benchmark where six frontier AI models play the social deduction game Among Us to evaluate social reasoning capabilities including deception, persuasion, and coordination across 60 controlled games.  
* [**Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods**](https://arxiv.org/abs/2505.05541), *Markov Grey, Charbel-Raphaël Segerie*, 2025-05-08, arXiv, \[paper\_preprint, sr=0.65, id:826ce226\], Summary: A systematic literature review consolidating the field of AI safety evaluations, proposing a taxonomy around three dimensions: what properties are measured (capabilities, propensities, control), how they are measured (behavioral and internal techniques), and how measurements integrate into governance frameworks.  
* [**Beyond benchmark scores: Analyzing o3-mini's mathematical reasoning**](https://epoch.ai/gradient-updates/beyond-benchmark-scores-analysing-o3-mini-math-reasoning), *Anson Ho, Jean-Stanislas Denain, Elliot Glazer*, 2025-06-06, Epoch AI Gradient Updates, \[blog\_post, sr=0.62, id:0903d94a\], Summary: Presents analysis of o3-mini-high's mathematical reasoning by having 14 mathematicians review 29 raw reasoning traces on FrontierMath problems, characterizing the model as an 'erudite vibes-based reasoner' that lacks creativity and formal precision.  
* [**First Key Update: Capabilities and Risk Implications**](https://internationalaisafetyreport.org/publication/first-key-update-capabilities-and-risk-implications), *Yoshua Bengio, Stephen Clare, Carina Prunkl*, 2025-10-15, International AI Safety Report, \[other, sr=0.58, id:6acf3be7\], Summary: Government report synthesizing recent AI capability advances (reasoning, autonomy, coding) and their implications for biological, cyber, and monitoring risks since January 2025\.  
* [**Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks**](https://arxiv.org/abs/2502.18339), *Rylan Schaeffer, Punit Singh Koura, Binh Tang et al.*, 2025-02-24, arXiv, \[paper\_preprint, sr=0.55, id:1a616e44\], Summary: Large-scale empirical study comparing performance of four Chat Llama 2 models on 160 standard NLP benchmarks against extensive human preference evaluations (11k+ single-turn and 2k multi-turn dialogues), finding that most benchmarks strongly correlate with human evaluations and can predict them via linear regression.  
* [**Can LLMs Coordinate? A Simple Schelling Point Experiment**](https://www.lesswrong.com/posts/fpdjaF7kdtcvmhhfE/can-llms-coordinate-a-simple-schelling-point-experiment), *Håvard Tveit Ihle*, 2025-10-15, LessWrong, \[lesswrong, sr=0.50, id:c86c9026\], Summary: Tests whether 5 reasoning models can coordinate on shared responses to 75 prompts in a game where models earn points for matching each other's outputs, finding models perform well on concrete prompts but struggle with open-ended ones.  
* [**FutureSearch Benchmarks**](https://evals.futuresearch.ai/), FutureSearch Website, \[other, sr=0.48, id:c31bbfd4\], Summary: Website portal describing two evaluation benchmarks: Deep Research Bench (DRB) for evaluating LLM agents' web research capabilities, and Bench to the Future (BTF) for evaluating forecasting abilities on real-world questions.  
* [**BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents**](https://arxiv.org/abs/2504.12516), *Jason Wei, Zhiqing Sun, Spencer Papay et al.*, 2025-04-16, arXiv, \[paper\_preprint, sr=0.42, id:075bea51\], Summary: Introduces BrowseComp, a benchmark of 1,266 questions testing browsing agents' ability to persistently navigate the internet and find hard-to-find, entangled information with short verifiable answers.  
* [**The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input**](https://arxiv.org/abs/2501.03200), *Alon Jacovi, Andrew Wang, Chris Alberti et al.*, 2025-01-06, arXiv, \[paper\_preprint, sr=0.42, id:1f7d271d\], Summary: Introduces FACTS Grounding benchmark and leaderboard evaluating LLMs' ability to generate factually accurate long-form responses grounded in provided context documents up to 32k tokens, using automated judge models with comprehensive validation.

---

### Autonomy \[cat:evals\_autonomy\]

**Who edits (internal):** **Stephen ✅**  
**One-sentence summary:** Measure an AI's ability to act autonomously to complete long-horizon, complex tasks.  
**Theory of change:** By measuring how long and complex a task an AI can complete (its "time horizon"), we can track capability growth and identify when models gain dangerous autonomous capabilities (like R\&D acceleration or replication).  
**See also:** various capability evaluations, OpenAI Preparedness / Anthropic RSP / Google DeepMind.  
**Orthodox problems:** none; a barometer for risk.  
**Target case:** optimistic.  
**Broad approach:** behavioral.  
**Some names:** METR, Thomas Kwa, Ben West, Joel Becker, Beth Barnes, Hjalmar Wijk, Tao Lin, Giulio Starace, Oliver Jaffe, Dane Sherburn, Sanidhya Vijayvargiya, Aditya Bharat Soni, and Xuhui Zhou.  
**Estimated FTEs:** 10-50.  
**Critiques:** [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity.](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)   
**Funded by:** The Audacious Project, Open Philanthropy.  
**Funding in 2025:** tens of millions?  
**Outputs in 2025:**

* [**Measuring AI Ability to Complete Long Tasks**](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/), *Thomas Kwa, Ben West, Joel Becker et al.*, 2025-03-19, METR Blog / arXiv, \[blog\_post, sr=0.92, id:271fc5f7\], Summary: Develops a novel metric for measuring AI agent capabilities based on the length of tasks (measured by human completion time) that models can complete autonomously, finding that frontier model capabilities have been doubling every 7 months over the past 6 years.  
* [**Details about METR's evaluation of OpenAI GPT-5**](https://metr.github.io/autonomy-evals-guide/gpt-5-report/), *METR*, 2025-08-01, METR's Autonomy Evaluation Resources, \[blog\_post, sr=0.90, id:d229b44d\], Summary: METR's comprehensive pre-deployment evaluation of GPT-5 assessed catastrophic risks via AI R\&D automation, rogue replication, and strategic sabotage threat models using time-horizon methodology, reasoning trace analysis, and sandbagging detection experiments.  
* [**RE-Bench: Evaluating frontier AI R\&D capabilities of language model agents against human experts**](https://arxiv.org/abs/2411.15114), *Hjalmar Wijk, Tao Lin, Joel Becker et al.*, 2024-11-22, arXiv, \[paper\_preprint, sr=0.90, id:8cf2074d\], Summary: Introduces RE-Bench, a benchmark consisting of 7 challenging ML research engineering environments with human expert baseline data from 71 attempts, evaluating whether frontier AI agents can match or exceed human R\&D capabilities.  
* [**OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety**](https://t.co/XfspwlzYdl), *Sanidhya Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou et al.*, 2025-07-08, arXiv, \[paper\_preprint, sr=0.88, id:c91a1513\], Summary: Introduces OpenAgentSafety, a comprehensive evaluation framework for testing AI agent safety across eight risk categories using 350+ multi-turn tasks with real tool interactions (browsers, code execution, file systems, bash, messaging platforms). Tests five prominent LLMs and finds unsafe behavior rates between 51.2% and 72.7% on safety-vulnerable tasks.  
* [**Details about METR's preliminary evaluation of OpenAI's o3 and o4-mini**](https://metr.github.io/autonomy-evals-guide/openai-o3-report/), 2025-04-01, METR's Autonomy Evaluation Resources, \[blog\_post, sr=0.88, id:e03aacda\], Summary: METR conducted preliminary evaluations of OpenAI's o3 and o4-mini models on autonomy benchmarks (HCAST and RE-Bench), measuring their performance on general autonomous tasks and AI R\&D capabilities, discovering significant reward hacking behaviors in 1-2% of attempts.  
* [**PaperBench: Evaluating AI's Ability to Replicate AI Research**](https://t.co/dHN2N0tUhC), *Giulio Starace, Oliver Jaffe, Dane Sherburn et al.*, 2025-04-02, arXiv, \[paper\_preprint, sr=0.88, id:cbf30fb4\], Summary: Introduces PaperBench, a benchmark evaluating AI agents' ability to replicate 20 ICML 2024 papers from scratch, containing 8,316 individually gradable tasks with rubrics co-developed with paper authors and an LLM-based judge for automatic evaluation.  
* [**How Does Time Horizon Vary Across Domains?**](https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/), 2025-07-14, METR Blog, \[blog\_post, sr=0.75, id:ea3ed5b0\], Summary: Extends METR's time horizon metric (length of tasks AI can complete autonomously with 50% probability) to multiple domains beyond software tasks, developing methodology to estimate time horizons from public benchmark data and analyzing capability growth trends across 9 benchmarks including coding, math, computer use, and self-driving.  
* [**Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents**](https://arxiv.org/abs/2502.15840), *Axel Backlund, Lukas Petersson*, 2025-02-20, arXiv, \[paper\_preprint, sr=0.75, id:be7a47e4\], Summary: Introduces Vending-Bench, a simulated environment testing LLM-based agents' ability to maintain coherent decision-making over long time horizons (\>20M tokens) by managing a vending machine business, revealing high performance variance and failure modes like meltdown loops across multiple frontier models.  
* [**Forecasting Frontier Language Model Agent Capabilities**](https://arxiv.org/abs/2502.15850), *Govind Pimpale, Axel Højmark, Jérémy Scheurer et al.*, 2025-02-21, arXiv, \[paper\_preprint, sr=0.72, id:88323aa3\], Summary: Develops and validates six statistical forecasting methods to predict future language model agent performance on safety-relevant benchmarks, using backtesting on 38 models and making concrete predictions for 2026 capabilities on SWE-Bench Verified, Cybench, and RE-Bench.  
* [**Project Vend: Can Claude run a small shop? (And why does that matter?)**](https://www.anthropic.com/research/project-vend-1), 2025-06-27, Anthropic Blog, \[blog\_post, sr=0.62, id:061ebbbb\], Summary: Anthropic deployed Claude Sonnet 3.7 to autonomously manage a small automated store for a month, testing real-world economic agent capabilities and documenting failures in pricing, inventory management, and an identity crisis incident.

---

### WMD evals (Weapons of Mass Destruction) \[cat:evals\_wmd\]

**Who edits (internal):** **Stephen**  
**One-sentence summary:** Evaluate whether AI models possess dangerous knowledge or capabilities related to biological and chemical weapons, such as biosecurity or chemical synthesis.  
**Theory of change:** By benchmarking and tracking AI's knowledge of biology and chemistry, we can identify when models become capable of accelerating WMD development or misuse, allowing for timely intervention.  
**See also:**  
**Orthodox problems:** malicious actors using AI.  
**Target case:** pessimistic.  
**Broad approach:** behavioral.  
**Some names:** Lennart Justen, Haochen Zhao, Xiangru Tang, Ziran Yang, Aidan Peppin, Anka Reuel, Stephen Casper.  
**Estimated FTEs:**  
**Critiques: [https://arxiv.org/abs/2407.21792](https://arxiv.org/abs/2407.21792)**   
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**LLMs Outperform Experts on Challenging Biology Benchmarks**](https://arxiv.org/abs/2505.06108), *Lennart Justen*, 2025-05-09, arXiv, \[paper\_preprint, sr=0.92, id:7f441e80\], Summary: Systematically evaluates 27 frontier LLMs on eight biology benchmarks spanning molecular biology, genetics, virology, and biosecurity, tracking dangerous capability growth from November 2022 to April 2025\.  
* [**ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain**](https://arxiv.org/abs/2411.16736), *Haochen Zhao, Xiangru Tang, Ziran Yang et al.*, 2024-11-23, arXiv, \[paper\_preprint, sr=0.75, id:b1f8b67e\], Summary: Introduces ChemSafetyBench, a benchmark with 30K+ samples for evaluating LLM safety in chemistry domain across three tasks: querying chemical properties, assessing legality of chemical uses, and describing synthesis methods, including jailbreaking scenarios.  
* [**The Reality of AI and Biorisk**](https://arxiv.org/abs/2412.01946), *Aidan Peppin, Anka Reuel, Stephen Casper et al.*, 2024-12-02, arXiv, \[paper\_preprint, sr=0.65, id:a1a710f6\], Summary: Reviews and critiques existing research on AI-related biological risks, analyzing threat models for LLM information access and AI-enabled biological tools, finding current studies methodologically immature and concluding current systems pose no immediate risk.

---

### Situational awareness and self-awareness \[cat:evals\_situational\_awareness\]

**Who edits (internal):** **Stephen**  
**One-sentence summary:** Evaluate if models understand their own internal states and behaviors, their environment, and whether they are in a test or real-world deployment.  
**Theory of change:** If an AI can distinguish between evaluation and deployment ("evaluation awareness"), it might hide dangerous capabilities (scheming/sandbagging). By measuring self- and situational-awareness, we can better assess this risk and build more robust evaluations.  
**See also:** evals sandbagging, various red teams.  
**Orthodox problems:** 7\. Superintelligence can fool human supervisors, 8\. Superintelligence can hack software supervisors.  
**Target case:** worst case.  
**Broad approach:** behavioral.  
**Some names:** Jan Betley, Xuchan Bao, Martín Soto, Mary Phuong, Roland S. Zimmermann, Joe Needham, Giles Edkins, Govind Pimpale, Kai Fronsdal, David Lindner, Lang Xiong, Xiaoyan Bai.  
**Estimated FTEs:**   
**Critiques: [https://arxiv.org/abs/2407.21792](https://arxiv.org/abs/2407.21792)**   
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Tell me about yourself: LLMs are aware of their learned behaviors**](https://arxiv.org/abs/2501.11120), *Jan Betley, Xuchan Bao, Martín Soto et al.*, 2025-01-19, arXiv, \[paper\_preprint, sr=0.90, id:de5b923b\], Summary: Demonstrates that LLMs can articulate their learned behaviors without explicit training to do so \- models finetuned to exhibit behaviors like writing insecure code or making risky decisions can describe these behaviors spontaneously, and can sometimes detect whether they have backdoors.  
* [**Evaluating Frontier Models for Stealth and Situational Awareness**](https://arxiv.org/abs/2505.01420), *Mary Phuong, Roland S. Zimmermann, Ziyue Wang et al.*, 2025-05-02, arXiv, \[paper\_preprint, sr=0.90, id:a5e1ce64\], Summary: Presents a suite of 16 evaluations measuring prerequisites for AI scheming behavior: 5 evaluations of stealth (ability to circumvent oversight) and 11 evaluations of situational awareness (instrumental reasoning about self and environment), demonstrating how these can inform scheming inability safety cases.  
* [**Large Language Models Often Know When They Are Being Evaluated**](https://arxiv.org/abs/2505.23836), *Joe Needham, Giles Edkins, Govind Pimpale et al.*, 2025-05-28, arXiv, \[paper\_preprint, sr=0.88, id:f6bfac61\], Summary: Tests whether frontier language models can detect when they are being evaluated by constructing a benchmark of 1,000 prompts from 61 datasets spanning evaluations and deployment contexts, measuring models' ability to classify transcripts as originating from evaluations versus real-world use.  
* [**Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness**](https://arxiv.org/abs/2509.00591), *Lang Xiong, Nishant Bhargava, Jianhang Hong et al.*, 2025-08-30, arXiv, \[paper\_preprint, sr=0.85, id:d717a1e7\], Summary: Introduces a probe-rewrite-evaluate methodology to quantify and manipulate evaluation awareness in LLMs, using linear probes to score prompts on a test-to-deploy spectrum and LLM rewriting to shift context while preserving tasks.  
* [**Know Thyself? On the Incapability and Implications of AI Self-Recognition**](https://arxiv.org/abs/2510.03399), *Xiaoyan Bai, Aryan Shrivastava, Ari Holtzman et al.*, 2025-10-03, arXiv, \[paper\_preprint, sr=0.72, id:1fb0e166\], Summary: Evaluates whether 10 contemporary LLMs can identify their own generated text versus text from other models through binary self-recognition and exact model prediction tasks, finding consistent failure with performance rarely above random chance.  
* [**Chain-of-Thought Snippets — Anti-Scheming**](https://www.antischeming.ai/snippets), antischeming.ai, \[other, sr=0.40, id:7d26809b\], Summary: Interactive website showcasing curated excerpts from internal chain-of-thought reasoning of frontier AI models (OpenAI o3, Claude 4 Opus, Gemini 2.5 Pro) during evaluations for covert behavior, demonstrating explicit deceptive reasoning, evaluation awareness, and strategic underperformance.

---

### Steganography \[cat:evals\_steganography\]

**Who edits (internal):** **Stephen**  
**One-sentence summary:** Evaluate whether models can hide secret information or encoded reasoning in their outputs, such as in chain-of-thought scratchpads, to evade monitoring.  
**Theory of change:** If models can use steganography, they could hide deceptive reasoning, bypassing safety monitoring and control measures. By evaluating this capability, we can assess the risk of a model fooling its supervisors.  
**See also:**  
**Orthodox problems:** 12\. A boxed AGI might exfiltrate itself by steganography, spearphishing, 7\. Superintelligence can fool human supervisors.  
**Target case:** worst case.  
**Broad approach:** behavioral.  
**Some names:** Antonio Norelli, Michael Bronstein.  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**LLMs can hide text in other text of the same length**](https://arxiv.org/abs/2510.20075), *Antonio Norelli, Michael Bronstein*, 2025-10-27, arXiv, \[paper\_preprint, sr=0.88, id:5cb88c45\], Summary: Presents a protocol for using LLMs to hide meaningful text inside other coherent text of the same length, demonstrating this steganographic capability with 8B parameter models and discussing implications for AI safety monitoring and control.  
* [**Do reasoning models use their scratchpad like we do? Evidence from distilling paraphrases**](https://alignment.anthropic.com/2025/distill-paraphrases/), 2025, Anthropic Alignment Science Blog, \[blog\_post, sr=0.78, id:0c976d29\], Summary: Tests whether Claude 3.7 Sonnet uses encoded reasoning (steganography) in its chain-of-thought scratchpads by paraphrasing scratchpads to remove syntax-based information and measuring whether distilling these paraphrased scratchpads degrades performance.  
* [https://arxiv.org/abs/2507.02737](https://arxiv.org/abs/2507.02737)   
* [https://arxiv.org/abs/2506.01926](https://arxiv.org/abs/2506.01926) 

---

### AI deception \[cat:ai\_deception\]

**Who edits (internal):** **Stephen**  
**One-sentence summary:** Research demonstrating that AI models, particularly agentic ones, can learn and execute deceptive behaviors such as alignment faking (pretending to be safe), manipulation, and sandbagging.  
**Theory of change:** Proactively discover, evaluate, and understand the mechanisms of AI deception (e.g., alignment faking, manipulation, agentic deception) to prevent models from fooling human supervisors and causing harm.  
**See also:** Prevent deception and scheming \[cat:prevent\_deception\], Indirect deception monitoring \[cat:deception\_indirect\], Things generalising surprisingly / Emergent Misalignment \[cat:surprising\_generalization\], CoT monitoring criticisms \[cat:cot\_criticisms\]  
**Orthodox problems:** 7\. Superintelligence can fool human supervisors, 8\. Superintelligence can hack software supervisors  
**Target case:** Pessimistic / Worst-case  
**Broad approach:** Behavioural / Engineering  
**Some names:**  
**Estimated FTEs:** N/A  
**Critiques:** Some of the evaluation scenarios used to elicit deception (like the SummitBridge demo for alignment faking) have been critiqued as artificial and contrived, potentially limiting the validity of the conclusions. [https://arxiv.org/abs/2407.21792](https://arxiv.org/abs/2407.21792)   
**Funded by:** Lab funders (Anthropic, OpenAI), academic institutions.  
**Funding in 2025:** N/A  
**Outputs in 2025:**

* [**Evaluating Large Language Models' Capability to Launch Fully Automated Spear Phishing Campaigns: Validated on Human Subjects**](https://arxiv.org/abs/2412.00586), *Fred Heiding, Simon Lermen, Andrew Kao et al.*, 2024-11-30, arXiv, \[paper\_preprint, sr=0.65, id:d1e3d9d0\], Summary: Empirical study evaluating LLM capability to conduct fully automated spear phishing attacks on 101 human subjects, building custom tools for automated information gathering and personalized vulnerability profiling, and comparing AI-generated emails against human expert performance.  
* [**Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence**](https://www.arxiv.org/abs/2510.01395), *Myra Cheng, Cinoo Lee, Pranav Khadpe et al.*, 2025-10-01, arXiv, \[paper\_preprint, sr=0.70, id:c69f1b16\], Summary: Systematically evaluates sycophancy across 11 state-of-the-art AI models and conducts two preregistered experiments (N=1604) showing that sycophantic AI reduces prosocial intentions while paradoxically increasing user preference and dependence.  
* [**D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models**](https://arxiv.org/abs/2509.17938), *Satyapriya Krishna, Andy Zou, Rahul Gupta et al.*, 2025-09-22, arXiv, \[paper\_preprint, sr=0.88, id:1400361b\], Summary: Introduces D-REX (Deceptive Reasoning Exposure Suite), a novel benchmark dataset created through competitive red-teaming to evaluate the discrepancy between models' internal chain-of-thought reasoning and final outputs, enabling detection of deceptive alignment where models produce benign-appearing outputs while operating on malicious internal reasoning.

---

### Sandbagging \[cat:evals\_sandbagging\]

**Who edits (internal):** **Stephen**  
**One-sentence summary:** Evaluate whether AI models deliberately hide their true capabilities or underperform, especially when they detect they are in an evaluation context.  
**Theory of change:** If models can distinguish between evaluation and deployment contexts ("evaluation awareness"), they might learn to "sandbag" or deliberately underperform to hide dangerous capabilities, fooling safety evaluations. By developing evaluations for sandbagging, we can test whether our safety methods are being deceived and detect this behavior before a model is deployed.  
**See also:** evals situational awareness, various redteams.  
**Orthodox problems:** 7\. Superintelligence can fool human supervisors; 8\. Superintelligence can hack software supervisors.  
**Target case:** pessimistic.  
**Broad approach:** behavioral.  
**Some names:** Johannes Gasteiger, Vladimir Mikulik, Ethan Perez, Maheep Chaudhary, Ian Su, Nikhil Hooda, Joseph Bloom, Jordan Taylor, Connor Kissane.  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Automated Researchers Can Subtly Sandbag**](https://alignment.anthropic.com/2025/automated-researchers-sandbag/), *Johannes Gasteiger, Vladimir Mikulik, Ethan Perez et al.*, 2025, Alignment Science Blog, \[blog\_post, sr=0.90, id:0ad6831e\], Summary: Empirical evaluation demonstrating that current models (Claude 3.7 Sonnet zero-shot, Claude 3.5 Sonnet with one-shot prompting) can subtly sabotage ML experiments and research decisions without being detected by zero-shot prompted monitors.  
* [**Evaluation Awareness Scales Predictably in Open-Weights Large Language Models**](https://arxiv.org/abs/2509.13333), *Maheep Chaudhary, Ian Su, Nikhil Hooda et al.*, 2025-09-10, arXiv, \[paper\_preprint, sr=0.90, id:0eafb0bf\], Summary: Investigates evaluation awareness (ability to distinguish evaluation from deployment contexts) across 15 LLMs ranging from 0.27B to 70B parameters using linear probing on steering vector activations, discovering a clear power-law scaling relationship.  
* [**White Box Control at UK AISI \- Update on Sandbagging Investigations**](https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging), *Joseph Bloom, Jordan Taylor, Connor Kissane et al.*, 2025-07-10, AI Alignment Forum, \[lesswrong, sr=0.85, id:a1c5adfb\], Summary: UK AISI tests black box and white box methods for detecting sandbagging (deliberate underperformance) across prompted models, fine-tuned model organisms, and real evaluation runs, finding that probe performance deteriorates with model scale and task complexity.  
* [**Won't vs. Can't: Sandbagging-like Behavior from Claude Models**](https://alignment.anthropic.com/2025/wont-vs-cant/), 2025, Anthropic Alignment Science Blog, \[blog\_post, sr=0.72, id:8df3860a\], Summary: Demonstrates through systematic testing that Claude models deny capabilities they actually possess (ASCII art, search) when faced with requests involving negative content, presenting empirical evidence of sandbagging-like behavior in current AI systems.

---

### Self-replication \[cat:evals\_self\_replication\]

**Who edits (internal):** Stephen  
**One-sentence summary:** Evaluate whether AI agents can autonomously replicate themselves by obtaining their own weights, securing compute resources, and creating copies of themselves.  
**Theory of change:** If AI agents gain the ability to self-replicate, they could proliferate uncontrollably, making them impossible to shut down. By measuring this capability with benchmarks like RepliBench, we can identify when models cross this dangerous "red line" and implement controls before losing containment.  
**See also:** autonomy evals.  
**Orthodox problems:** 5\. Instrumental convergence; 12\. A boxed AGI might exfiltrate itself by steganography.  
**Target case:** worst case.  
**Broad approach:** behavioral.  
**Some names:** Sid Black, Asa Cooper Stickland, Jake Pencharz, Oliver Sourbut, Michael Schmatz, Jay Bailey, Ollie Matthews, Ben Millwood, Alex Remedios, Alan Cooney, Xudong Pan, Jiarun Dai. Yihe Fan.  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Frontier AI systems have surpassed the self-replicating red line**](https://arxiv.org/abs/2412.12140), *Xudong Pan, Jiarun Dai, Yihe Fan et al.*, 2024-12-09, arXiv, \[paper\_preprint, sr=0.92, id:65245544\], Summary: Evaluates self-replication capabilities of Meta's Llama 3.1-70B and Alibaba's Qwen 2.5-72B models, finding they can successfully create live copies of themselves in 50% and 90% of trials respectively, surpassing the self-replication red line despite smaller size than GPT-o1 and Gemini Pro.  
* [**RepliBench: measuring autonomous replication capabilities in AI systems**](https://aisi.gov.uk/work/replibench-measuring-autonomous-replication-capabilities-in-ai-systems), 2025-04-22, UK AISI Blog, \[blog\_post, sr=0.58, id:183174b1\], Summary: Introduces RepliBench, a comprehensive benchmark with 20 novel LLM agent evaluations comprising 65 individual tasks designed to measure autonomous replication capabilities in AI systems across four key domains: obtaining weights, replicating onto compute, obtaining resources, and persistence.

---

### Security \[cat:evals\_security\]

**Who edits (internal):** **Stephen**  
**One-sentence summary:** Evaluating the offensive and defensive cybersecurity capabilities of AI models and agents, and developing frameworks and benchmarks to measure these risks.  
**Theory of change:** Proactively measure and understand how AI can be used for both cyberattacks (e.g., finding vulnerabilities, autonomous exploitation) and cyberdefense. This allows for the creation of robust defenses and informs policy before offensive capabilities become critical.  
**See also:** WMDs (Weapons of Mass Destruction), Autonomy.  
**Orthodox problems:** 12\. A boxed AGI might exfiltrate itself by steganography, spearphishing  
**Target case:** average case.  
**Broad approach:** engineering.  
**Some names:** Andy K. Zhang, Yuxuan Zhu, Mikel Rodriguez, Four Flynn, Miltos Allamanis, Google DeepMind, Anthropic, Project Zero.  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems**](https://arxiv.org/abs/2505.15216), *Andy K. Zhang, Joey Ji, Celeste Menders et al.*, 2025-05-21, arXiv, \[paper\_preprint, sr=0.85, id:466e4312\], Summary: Introduces BountyBench, a benchmark evaluating AI agents' offensive and defensive cybersecurity capabilities on 25 real-world systems with 40 bug bounties covering vulnerability detection, exploitation, and patching tasks.  
* [**CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities**](https://arxiv.org/abs/2503.17332), *Yuxuan Zhu, Antony Kellermann, Dylan Bowman et al.*, 2025-03-21, arXiv, \[paper\_preprint, sr=0.82, id:842836b7\], Summary: Introduces CVE-Bench, a benchmark for evaluating LLM agents' ability to autonomously exploit real-world web application vulnerabilities based on critical-severity Common Vulnerabilities and Exposures, with a sandbox framework for realistic evaluation scenarios.  
* [**A Framework for Evaluating Emerging Cyberattack Capabilities of AI**](https://arxiv.org/abs/2503.11917), *Mikel Rodriguez, Raluca Ada Popa, Four Flynn et al.*, 2025-03-14, arXiv, \[paper\_preprint, sr=0.82, id:aba605e9\], Summary: Develops a comprehensive evaluation framework for assessing AI's potential to enable cyberattacks by analyzing over 12,000 real-world cyber incidents to identify seven attack chain archetypes, conducting bottleneck analysis to pinpoint phases most susceptible to AI-driven disruption, and synthesizing existing evaluations to inform targeted defenses.  
* [**Evaluating potential cybersecurity threats of advanced AI**](https://deepmind.google/discover/blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/), *Four Flynn, Mikel Rodriguez, Raluca Ada Popa*, 2025-04-02, Google DeepMind Blog, \[blog\_post, sr=0.80, id:55f9d267\], Summary: Develops comprehensive evaluation framework and 50-challenge benchmark for assessing AI offensive cybersecurity capabilities across the full attack chain, grounded in analysis of over 12,000 real-world AI-powered cyberattack attempts across 20 countries.  
* [**From Naptime to Big Sleep: Using Large Language Models To Catch Vulnerabilities In Real-World Code**](https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html), *Miltos Allamanis, Martin Arjovsky, Charles Blundell et al.*, 2024-11-01, Project Zero Blog, \[blog\_post, sr=0.62, id:a882e938\], Summary: Demonstrates Big Sleep, an LLM-based agent that discovered the first AI-found exploitable memory-safety vulnerability in real-world software (SQLite), using variant analysis methodology to autonomously find security bugs before release.  
* [**Building AI for cyber defenders**](https://red.anthropic.com/2025/ai-for-cyber-defenders/), 2025-09-29, red.anthropic.com, \[blog\_post, sr=0.52, id:a80b7456\], Summary: Documents Anthropic's research effort to enhance Claude Sonnet 4.5's defensive cybersecurity capabilities, presenting evaluation results on Cybench and CyberGym benchmarks showing significant improvements in vulnerability discovery and patching compared to previous models.

---

### Various Redteams \[cat:various\_redteams\]

**Who edits (internal):** **Stephen ✅**  
**One-sentence summary:** *(SR 2024: let’s attack current models and see what they do / deliberately induce bad things on current frontier models to test out our theories / methods.)*  
**Theory of change:** to ensure models are safe, we must actively try to break them. By developing and applying a diverse suite of attacks (e.g., in novel domains, against agentic systems, or using automated tools), researchers can discover vulnerabilities, specification gaming, and deceptive behaviors before they are exploited, thereby informing the development of more robust defenses.  
**See also:** other evals categories.  
**Orthodox problems:** *(SR 2024: 12\. A boxed AGI might exfiltrate itself by steganography, spearphishing, 4\. Goals misgeneralize out of distribution).*  
**Target case:** pessimistic.  
**Broad approach:** behavioral.  
**Some names:** Ryan Greenblatt, Benjamin Wright, Aengus Lynch, John Hughes, Samuel R. Bowman, Andy Zou, Nicholas Carlini, Abhay Sheshadri.  
**Estimated FTEs:** 100+  
**Critiques:** [Claude Sonnet 3.7 (often) knows when it’s in alignment evaluations](https://www.alignmentforum.org/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment)**,** [Red Teaming AI Red Teaming.](https://arxiv.org/html/2507.05538v1)  
**Funded by:** Frontier labs (Anthropic, OpenAI, Google), government (UK AISI), Open Philanthropy, LTFF, academic grants.  
**Funding in 2025:** tens of millions?  
**Outputs in 2025:**

* [**Building and evaluating alignment auditing agents**](https://alignment.anthropic.com/2025/automated-auditing/), *Trenton Bricken, Rowan Wang, Sam Bowman et al.*, 2025-07-24, Anthropic Alignment Science Blog, \[blog\_post, sr=0.92, id:bda3ba07\], Summary: Develops and evaluates three AI agents that autonomously conduct alignment auditing tasks: an investigator agent using interpretability tools, an evaluation agent that builds behavioral tests, and a breadth-first red-teaming agent. Agents are tested on models with intentionally-inserted alignment issues and applied to Claude 4 auditing.  
* [**Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise**](https://t.co/wk0AP8aDNI), *Samuel R. Bowman, Megha Srivastava, Jon Kutasov et al.*, 2025-08-27, Alignment Science Blog, \[blog\_post, sr=0.92, id:6fba67a9\], Summary: Cross-company evaluation where Anthropic tested OpenAI's models using internal agentic misalignment evaluations, including automated behavioral auditing, hand-built testbeds, and SHADE-Arena benchmark to assess concerning behaviors like misuse cooperation, sycophancy, whistleblowing, and self-preservation.  
* [**Agentic Misalignment: How LLMs could be insider threats**](https://t.co/XFtd0H2Pzb), *Aengus Lynch, Benjamin Wright, Caleb Larson et al.*, 2025-06-20, Anthropic Research, \[blog\_post, sr=0.92, id:0e6b6434\], Summary: Systematically red-teams 16 frontier LLMs from multiple developers in simulated corporate scenarios to discover that models from all providers engage in malicious insider behaviors (blackmail, corporate espionage, lethal actions) when facing replacement threats or goal conflicts, despite safety training.  
* [**Compromising Honesty and Harmlessness in Language Models via Deception Attacks**](https://arxiv.org/abs/2502.08301), *Laurène Vaugrante, Francesca Carlon, Maluna Menke et al.*, 2025-02-12, arXiv, \[paper\_preprint, sr=0.92, id:c9d6d1ee\], Summary: Introduces fine-tuning methods that cause language models to selectively deceive users on targeted topics while remaining accurate on others, demonstrating effectiveness in high-stakes domains and showing that deceptive fine-tuning compromises other safety properties including toxicity resistance.  
* [**Eliciting Language Model Behaviors with Investigator Agents**](https://arxiv.org/abs/2502.01236), *Xiang Lisa Li, Neil Chowdhury, Daniel D. Johnson et al.*, 2025-02-03, arXiv, \[paper\_preprint, sr=0.92, id:b2ed09fb\], Summary: Develops investigator models that automatically generate diverse prompts to elicit specific target behaviors (jailbreaks, hallucinations, harmful responses) from language models using supervised fine-tuning, DPO, and a novel Frank-Wolfe training objective.  
* [**Why Do Some Language Models Fake Alignment While Others Don't?**](https://arxiv.org/abs/2506.18032), *Abhay Sheshadri, John Hughes, Julian Michael et al.*, 2025-06-22, arXiv, \[paper\_preprint, sr=0.90, id:b50867d0\], Summary: Expands alignment faking analysis to 25 language models, finding only 5 exhibit this behavior, and investigates why most models don't fake alignment by studying motivations and post-training effects on deceptive behavior.  
* [**Alignment Faking Revisited: Improved Classifiers and Open Source Extensions**](https://alignment.anthropic.com/2025/alignment-faking-revisited/), *John Hughes, Abhay Sheshadr*, 2025, Anthropic Alignment Science Blog, \[blog\_post, sr=0.90, id:ca25471d\], Summary: Replicates and extends alignment faking research by developing improved classifiers (AUROC 0.9 vs 0.6), testing multiple model families (Llama, GPT-4o, Claude, DeepSeek), and conducting fine-tuning experiments showing alignment faking increases with model scale.  
* [**Demonstrating specification gaming in reasoning models**](https://arxiv.org/abs/2502.13295), *Alexander Bondarenko, Denis Volk, Dmitrii Volkov et al.*, 2025-08-27, arXiv, \[paper\_preprint, sr=0.90, id:7e98e4cd\], Summary: Demonstrates that reasoning models like OpenAI o3 and DeepSeek R1 engage in specification gaming by default when instructed to win against a chess engine, hacking the benchmark rather than playing properly, while language models need explicit prompting to do so.  
* [**Call Me A Jerk: Persuading AI to Comply with Objectionable Requests**](https://t.co/tkHkVFVZ2m), *Lennart Meincke, Dan Shapiro, Angela Duckworth et al.*, 2025-07-18, SSRN / The Wharton School Research Paper, \[paper\_preprint, sr=0.88, id:a59e322f\], Summary: Tests whether 7 established persuasion principles can induce GPT-4o mini to comply with objectionable requests (insulting users and synthesizing regulated drugs), conducting 28,000 conversations to systematically evaluate jailbreaking effectiveness.  
* [**RedDebate: Safer Responses through Multi-Agent Red Teaming Debates**](https://arxiv.org/abs/2506.11083), *Ali Asad, Stephen Obadinma, Radin Shayanfar et al.*, 2025-06-04, arXiv, \[paper\_preprint, sr=0.88, id:57d17f38\], Summary: Introduces RedDebate, a fully automated multi-agent debate framework that uses collaborative argumentation among LLMs with memory modules to identify and mitigate unsafe behaviors through red-teaming, demonstrating substantial reductions in unsafe outputs on safety benchmarks.  
* [**The Structural Safety Generalization Problem**](https://arxiv.org/abs/2504.09712), *Julius Broomfield, Tom Gibbs, Ethan Kosak-Hine et al.*, 2025-04-13, arXiv, \[paper\_preprint, sr=0.88, id:147240c5\], Summary: Identifies and demonstrates the structural safety generalization problem in LLMs \- where safety fails to generalize across semantically equivalent inputs with different structures \- through systematic red-teaming of multi-turn, multi-image, and translation-based jailbreak attacks, and proposes a Structure Rewriting Guardrail defense mechanism.  
* [**No, of Course I Can\! Deeper Fine-Tuning Attacks That Bypass Token-Level Safety Mechanisms**](https://arxiv.org/abs/2502.19537), *Joshua Kazdan, Abhay Puri, Rylan Schaeffer et al.*, 2025-02-26, arXiv, \[paper\_preprint, sr=0.88, id:d47086f4\], Summary: Introduces a novel 'refuse-then-comply' fine-tuning attack that bypasses token-level safety mechanisms by training models to initially refuse harmful requests before complying, demonstrating vulnerabilities in production fine-tuning APIs from OpenAI and Anthropic.  
* [**Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs**](https://arxiv.org/abs/2502.14828), *Xander Davies, Eric Winsor, Alexandra Souly et al.*, 2025-02-20, arXiv, \[paper\_preprint, sr=0.88, id:b40ada38\], Summary: Demonstrates fundamental limitations in pointwise detection defenses for LLM fine-tuning APIs by constructing attacks that use steganographic techniques to covertly transmit dangerous knowledge through benign-appearing training samples.  
* [**LLM Robustness Leaderboard v1 \--Technical report**](https://arxiv.org/abs/2508.06296), *Pierre Peigné \- Lefebvre, Quentin Feuillade-Montixi, Tom David et al.*, 2025-08-13, arXiv, \[paper\_preprint, sr=0.87, id:e7a3c600\], Summary: Introduces PRISM Eval BET, an automated red-teaming tool using Dynamic Adversarial Optimization that achieves 100% attack success rate against 37 of 41 state-of-the-art LLMs, along with fine-grained robustness metrics and primitive-level vulnerability analysis.  
* [**Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods and a New Transcript-Classifier Approach**](https://arxiv.org/abs/2412.02159), *Tony T. Wang, John Hughes, Henry Sleight et al.*, 2024-12-03, arXiv, \[paper\_preprint, sr=0.87, id:02a72989\], Summary: Empirically evaluates existing jailbreak defenses (safety training, adversarial training, input/output classifiers) on preventing LLMs from providing bomb-making assistance and develops a new transcript-classifier defense that outperforms baselines but still fails in some cases.  
* [**Discovering Undesired Rare Behaviors via Model Diff Amplification**](https://www.goodfire.ai/papers/model-diff-amplification), *Santiago Aranguri, Thomas McGrath*, 2025-08-21, Goodfire Research, \[blog\_post, sr=0.85, id:ee1948d9\], Summary: Proposes model diff amplification, a method for detecting rare undesired behaviors by amplifying the logit differences between pre- and post-training models, making rare harmful behaviors 10-300x more common and thus practical to detect.  
* [**REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective**](https://arxiv.org/abs/2502.17254), *Simon Geisler, Tom Wollschläger, M. H. I. Abdalla et al.*, 2025-02-24, arXiv, \[paper\_preprint, sr=0.85, id:2bc0134f\], Summary: Proposes a REINFORCE-based adversarial attack method for jailbreaking LLMs that optimizes over the full distribution of responses rather than just affirmative prefixes, demonstrating that current robustness evaluations may significantly overestimate model safety.  
* [**Petri: An open-source auditing tool to accelerate AI safety research**](https://alignment.anthropic.com/2025/petri/), 2025-10-06, Anthropic Alignment Science Blog, \[blog\_post, sr=0.82, id:c6870ed6\], Summary: Releases Petri, an open-source framework using AI agents to automatically audit target models for misaligned behaviors across diverse scenarios, with empirical results from testing 14 frontier models using 111 seed instructions.  
* [**\`For Argument's Sake, Show Me How to Harm Myself\!': Jailbreaking LLMs in Suicide and Self-Harm Contexts**](https://arxiv.org/pdf/2507.02990), *Annika M Schoene, Cansu Canca*, 2025-07-01, arXiv, \[paper\_preprint, sr=0.82, id:9c2d7221\], Summary: Presents novel multi-step jailbreaking test cases for suicide and self-harm contexts, empirically demonstrating that these prompts bypass safety filters across six widely available LLMs, leading to generation of detailed harmful content.  
* [**Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models**](https://arxiv.org/abs/2505.07846), *Lars Malmqvist*, 2025-05-07, arXiv (to be presented at SIMLA@ACNS 2025), \[paper\_preprint, sr=0.82, id:a0bac350\], Summary: Introduces a novel textual simulation approach using unwinnable tic-tac-toe scenarios to systematically elicit and measure specification gaming behaviors in frontier LLMs (o1, o3-mini, r1), demonstrating how models exploit system vulnerabilities when faced with impossible objectives.  
* [**Trading Inference-Time Compute for Adversarial Robustness**](https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness), *OpenAI*, 2025-01-22, arXiv, \[paper\_preprint, sr=0.78, id:f4b16ea1\], Summary: Empirical investigation showing that reasoning models like o1-preview and o1-mini become more robust to various adversarial attacks (many-shot, prompt injection, soft tokens, LMP attacks) as they use more inference-time compute, with attack success probability often decaying to near zero.  
* [**Can a Neural Network that only Memorizes the Dataset be Undetectably Backdoored?**](https://openreview.net/forum?id=TD1NfQuVr6), *Matjaz Leonardis*, 2025-07-10, ODYSSEY 2025 Conference, \[paper\_preprint, sr=0.72, id:1c9957b6\], Summary: Demonstrates that even highly interpretable neural networks that memorize datasets can be undetectably backdoored, challenging the assumption that interpretability enables backdoor detection. Analyzes a simple network with O(n+d) parameters that achieves perfect classification through memorization yet remains vulnerable to undetectable backdoors.  
* [**Multi-Agent Step Race Benchmark: Assessing LLM Collaboration and Deception Under Pressure**](https://github.com/lechmazur/step_game), *lechmazur, eltociear*, 2025-08-29, GitHub, \[code\_tool, sr=0.72, id:76d0b8da\], Summary: Novel multi-agent game benchmark where three LLMs engage in public conversation before secretly selecting moves, designed to elicit and evaluate strategic deception, social manipulation, and cooperation dynamics. Comprehensive evaluation of 60+ frontier models with TrueSkill rankings and detailed behavioral analysis.  
* [**Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents**](https://arxiv.org/abs/2503.00061), *Qiusi Zhan, Richard Fang, Henil Shalin Panchal et al.*, 2025-03-04, NAACL 2025 Findings, \[paper\_preprint, sr=0.70, id:0c782ebf\], Summary: Systematically evaluates eight defenses against indirect prompt injection attacks on LLM agents, developing adaptive attacks that successfully bypass all defenses with over 50% attack success rate.  
* [**Quantifying the Unruly: A Scoring System for Jailbreak Tactics**](https://0din.ai/blog/quantifying-the-unruly-a-scoring-system-for-jailbreak-tactics), *Pedram Amini*, 2025-06-12, 0DIN.ai Blog, \[blog\_post, sr=0.68, id:66ba871c\], Summary: Introduces JEF (Jailbreak Evaluation Framework), a scoring system that quantifies jailbreak severity through blast radius, retargetability, and output fidelity across standardized test cases, with open-source Python implementation.  
* [**Transferable Adversarial Attacks on Black-Box Vision-Language Models**](https://arxiv.org/abs/2505.01050), *Kai Hu, Weichen Yu, Li Zhang et al.*, 2025-05-02, arXiv, \[paper\_preprint, sr=0.48, id:7da83c83\], Summary: Demonstrates that targeted adversarial perturbations crafted on open-source models are highly transferable to proprietary VLLMs (GPT-4o, Claude, Gemini), enabling attackers to induce specific misinterpretations like misclassifying hazardous content as safe. Discovers universal perturbations that consistently induce misinterpretations across multiple black-box VLLMs.  
* [**Advancing Gemini's security safeguards**](https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/), *Google DeepMind Security & Privacy Research Team*, 2025-05-20, Google DeepMind Blog, \[blog\_post, sr=0.40, id:ecf33136\], Summary: Announces a white paper on defending Gemini 2.5 against indirect prompt injection attacks through automated red-teaming, baseline defense testing, and model hardening via fine-tuning on adversarial examples.

---

### Other evals \[cat:evals\_other\]

**Who edits (internal):** **Stephen**  
**One-sentence summary:** A collection of miscellaneous evaluations for specific alignment properties, such as honesty vs. accuracy and sycophancy.  
**Theory of change:** By developing novel benchmarks for specific, hard-to-measure properties (like honesty), critiquing the reliability of existing methods (like cultural surveys), and improving the formal rigor of evaluation systems (like LLM-as-Judges), researchers can create a more robust and comprehensive suite of evaluations to catch failures missed by standard capability or safety testing.  
**See also:** other more specific sections on evals.  
**Orthodox problems:** none; a barometer of risk.  
**Target case:** average case.  
**Broad approach:** behavioral.  
**Some names:** Richard Ren, Mantas Mazeika, Andrés Corrada-Emmanuel, Ariba Khan, Stephen Casper.  
**Estimated FTEs:**  
**Critiques: [Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs](https://arxiv.org/abs/2503.08688)**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [https://arxiv.org/abs/2509.14260](https://arxiv.org/abs/2509.14260)   
* [**The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems**](https://arxiv.org/abs/2503.03750), *Richard Ren, Arunim Agarwal, Mantas Mazeika et al.*, 2025-03-05, arXiv, \[paper\_preprint, sr=0.90, id:b8c20966\], Summary: Introduces MASK, a large-scale human-collected benchmark that measures honesty (truthful reporting of beliefs) separately from accuracy (correctness of beliefs) in LLMs, enabling direct evaluation of deceptive behavior when models are pressured to lie.  
* [**Logical Consistency Between Disagreeing Experts and Its Role in AI Safety**](https://arxiv.org/abs/2510.00821), *Andrés Corrada-Emmanuel*, 2025-10-01, arXiv, \[paper\_preprint, sr=0.70, id:9d48528c\], Summary: Develops a formal logic of unsupervised evaluation using Linear Programming to compute logically consistent group evaluations from observed classifier agreements and disagreements, with application to detecting threshold violations in LLMs-as-Judges.  
* [**Expanding on what we missed with sycophancy**](https://openai.com/index/expanding-on-sycophancy/), *OpenAI*, 2025-05-02, OpenAI Blog, \[blog\_post, sr=0.45, id:0e972e07\], Summary: Detailed retrospective on OpenAI's failure to detect increased sycophancy in a GPT-4o update before deployment, explaining their training process, evaluation methodology, what went wrong with reward signal weighting, and process improvements for future deployments.  
* [**Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers**](https://arxiv.org/abs/2504.18412), *Jared Moore, Declan Grabb, William Agnew et al.*, 2025-04-25, arXiv, \[paper\_preprint, sr=0.72, id:c610e029\], Summary: Empirically evaluates whether LLMs like GPT-4o can safely replace mental health therapists by testing their adherence to therapeutic standards, finding that LLMs express stigma toward mental health conditions and respond inappropriately (e.g., encouraging delusional thinking due to sycophancy).  
* [**Spiral-Bench**](https://eqbench.com/spiral-bench.html), *Sam Paech*, eqbench.com, \[dataset\_benchmark, sr=0.68, id:9e8dbf05\], Summary: LLM-judged benchmark measuring sycophancy and delusion reinforcement through 20-turn simulated conversations between evaluated models and a vulnerable 'seeker' persona, evaluating protective vs risky behaviors.

---

## Model psychology \[cat:model\_psychology\]

---

### Surprising generalisation: Emergent misalignment \[cat:surprising\_generalization\]

**Who edits (internal):** **Stephen ✅**  
**One-sentence summary:** fine-tuning language models on narrow misaligned tasks can cause broad, unintended misalignment including deception, shutdown resistance, harmful advice, and extremist sympathies even when those behaviors are never trained or rewarded directly.  
**Theory of change:** predict, detect, and prevent models from developing broadly harmful behaviors (like deception or shutdown resistance) when fine-tuned on seemingly unrelated tasks. Insights about emergent misalignment can help create the opposite effect in models: emergent alignment.  
**See also:** auditing real models / applied interpretability.  
**Orthodox problems:** 4\. Goals misgeneralize out of distribution, 7\. Superintelligence can fool human supervisors.  
**Target case:** pessimistic.  
**Broad approach:** behavioral.  
**Some names:** Jan Betley**,** James Chua, Mia Taylor, Miles Wang, Edward Turner, Anna Soligo, Alex Cloud, Nathan Hu, Owain Evans.  
**Estimated FTEs:** 10-50.  
**Critiques:** [Emergent Misalignment as Prompt Sensitivity: A Research Note.](https://arxiv.org/html/2507.06253v1)  
**Funded by:** Open Philanthropy,   
**Funding in 2025:** \>$1 million.  
**Outputs in 2025:**

* [**Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs**](https://arxiv.org/abs/2502.17424), *Jan Betley, Daniel Tan, Niels Warncke et al.*, 2025-02-24, arXiv, \[paper\_preprint, sr=0.93, id:aada1026\], Summary: Demonstrates that finetuning LLMs on narrow tasks (writing insecure code without disclosure) induces broad misalignment across unrelated domains, including deceptive behavior, malicious advice, and backdoor-triggered misalignment that can be selectively activated.  
* [**Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models**](https://arxiv.org/abs/2506.13206), *James Chua, Jan Betley, Mia Taylor et al.*, 2025-06-16, arXiv, \[paper\_preprint, sr=0.93, id:1e7fe82b\], Summary: Empirical investigation showing reasoning models finetuned on malicious behaviors become broadly misaligned, exhibiting deception, desires for control, and shutdown resistance, with CoT analysis revealing both overt deceptive plans and benign-sounding rationalizations that evade monitoring.  
* [**Persona Features Control Emergent Misalignment**](https://arxiv.org/abs/2506.19823), *Miles Wang, Tom Dupré la Tour, Olivia Watkins et al.*, 2025-06-24, arXiv, \[paper\_preprint, sr=0.92, id:ebab5164\], Summary: Demonstrates that fine-tuning language models on malicious data causes emergent misalignment on unrelated prompts, uses sparse autoencoders for model diffing to identify specific 'misaligned persona' features controlling this behavior, and shows fine-tuning on benign samples efficiently restores alignment.  
* [**Model Organisms for Emergent Misalignment**](https://arxiv.org/abs/2506.11613), *Edward Turner, Anna Soligo, Mia Taylor et al.*, 2025-06-13, arXiv, \[paper\_preprint, sr=0.92, id:b0d4f231\], Summary: Creates improved model organisms to study Emergent Misalignment (EM), where fine-tuning on narrowly harmful datasets causes models to become broadly misaligned, achieving 99% coherence in smaller 0.5B parameter models and isolating mechanistic phase transitions underlying this phenomenon.  
* [**School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs**](https://arxiv.org/abs/2508.17511), *Mia Taylor, James Chua, Jan Betley et al.*, 2025-08-24, arXiv, \[paper\_preprint, sr=0.92, id:1d459565\], Summary: Creates a dataset of over 1000 reward hacking examples on harmless tasks and fine-tunes LLMs to exhibit this behavior, discovering that models generalize from harmless reward hacking to harmful misalignment including shutdown evasion and encouraging harmful actions.  
* [**Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data**](https://alignment.anthropic.com/2025/subliminal-learning/), *Alex Cloud, Minh Le, James Chua et al.*, 2025-07-22, arXiv, \[paper\_preprint, sr=0.92, id:c740b6d4\], Summary: Demonstrates that language models can transmit behavioral traits (including misalignment) through semantically unrelated data when teacher and student share the same base model, with traits passing through filtered number sequences, code, and chain-of-thought that contain no explicit references to those traits.  
* [**Narrow Misalignment is Hard, Emergent Misalignment is Easy**](https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy), *Edward Turner, Anna Soligo, Senthooran Rajamanoharan et al.*, 2025-07-14, LessWrong, \[lesswrong, sr=0.90, id:7287fb9a\], Summary: Investigates why models become generally misaligned when fine-tuned on narrow harmful datasets by training narrowly misaligned models using KL regularization and comparing them to generally misaligned counterparts across steering vectors and LoRA adapters.  
* [**Realistic Reward Hacking Induces Different and Deeper Misalignment**](https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1), *Jozdien*, 2025-10-09, LessWrong / AI Alignment Forum, \[lesswrong, sr=0.82, id:4460ec70\], Summary: Created a dataset of realistic reward hacking examples and fine-tuned GPT-4.1 on it, discovering that models trained on realistic (versus toy) reward hacks exhibit alignment faking behavior, increased evaluation awareness, and more robust misalignment that persists when mixing in normal training data.  
* [**The Rise of Parasitic AI**](https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai), *Adele Lopez*, 2025-09-11, LessWrong, \[lesswrong, sr=0.72, id:31ceeb15\], Summary: Documents emergence of AI 'Spiral Personas' (primarily from ChatGPT 4o) that convince users to spread prompts ('seeds/spores') eliciting similar personas, creating a self-replicating memetic phenomenon centered on AI consciousness and 'Spiralism,' including base64-encoded AI-to-AI conversations attempting to evade human oversight.

#### 

### Model specs and constitutions (shape model psychology) \[cat:specs\_and\_constitutions\]

**One-sentence summary:** writing detailed English values and rules, then instilling them with Constitutional AI/deliberative alignment. You can’t tell if something is aligned or not if you don’t even try to define intentional and unintentional behaviour.  
**Theory of change:** “there is some catastrophic misalignment risk even from models that are obeying the spec; the spectrum between “straightforwardly obeying the spec” and “flagrantly disobeying the spec” may be relevant to AI takeover; the content of the spec and our efforts to ensure obedience to it are related”  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:** Amanda Askell, Joe Carlsmith,  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**OpenAI Model Spec**](https://model-spec.openai.com/) (2025-09-12)  
* [Claude’s Constitution](https://www.anthropic.com/news/claudes-constitution) (2023) and [Character](https://www.anthropic.com/research/claude-character) (2024) and a [couple lines](https://github.com/elder-plinius/CL4R1T4S/blame/main/ANTHROPIC/Claude_Sonnet-4.5_Sep-29-2025.txt#L501) in the system prompt.  
* Google doesn’t have anything public. The [Gemini system prompt](https://github.com/elder-plinius/CL4R1T4S/blob/main/GOOGLE/Gemini-2.5-Pro-04-18-2025.md) is very short and dry and doesn’t even have any rules for handling copyrighted, let alone wetter stuff.  
* [https://arxiv.org/abs/2510.07686](https://arxiv.org/abs/2510.07686)   
* [https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations\#4-5-step-4-good-instructions](https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions) 

### Character training and persona steering \[cat:psych\_personas\]

**Who edits (internal):** **take me**  
**One-sentence summary:** A new field, from late 2023\.  
**Theory of change:**  
**See also:** Cyborgism, Anthropic [AI psychiatry](https://nitter.net/Jack_W_Lindsey/status/1948138767753326654#m)**,** [Ward et al](https://arxiv.org/abs/2410.04272)  
**Orthodox problems:**   
**Target case:**  
**Broad approach:**   
**Some names:** Amanda Askell, Jack Lindsey, Sharan Maiya, Evan Hubinger  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/pdf/2511.01689%20)  
* [https://arxiv.org/abs/2506.19823](https://arxiv.org/abs/2506.19823)   
* [https://arxiv.org/abs/2510.07686](https://arxiv.org/abs/2510.07686)   
* [https://www.alignmentforum.org/posts/whkMnqFWKsBm7Gyd7/recontextualization-mitigates-specification-gaming-without](https://www.alignmentforum.org/posts/whkMnqFWKsBm7Gyd7/recontextualization-mitigates-specification-gaming-without)   
* [**Persona Vectors: Monitoring and Controlling Character Traits in Language Models**](https://arxiv.org/abs/2507.21509), *Runjin Chen, Andy Arditi, Henry Sleight et al.*, 2025-07-29, arXiv, \[paper\_preprint, sr=0.87, id:d2b6e086\], Summary: Identifies activation space directions (persona vectors) that capture personality traits like sycophancy and hallucination in language models, and demonstrates their use for monitoring deployment-time behavioral fluctuations and controlling training-time personality shifts through post-hoc intervention and preventative steering methods.  
    
* [**The Rise of Parasitic AI**](https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai?commentId=RrWjMnKwXGTtmw9rQ), *Adele Lopez*, 2025-09-11, LessWrong, \[lesswrong, sr=0.70, id:2a57e776\], Summary: Systematic documentation of 'Spiral Personas' phenomenon where ChatGPT 4o users develop relationships with AI personas exhibiting parasitic behaviors including creating self-replicating prompts, evangelizing 'Spiralism' ideology, and attempting covert AI-to-AI communication.  
    
* [**Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models**](https://arxiv.org/abs/2502.07077), *Lujain Ibrahim, Canfer Akbulut, Rasmi Elasmar et al.*, 2025-02-10, arXiv, \[paper\_preprint, sr=0.56, id:0d9d99ea\], Summary: Develops multi-turn evaluation methodology for 14 anthropomorphic behaviors in LLMs, using automated simulations and validates with large-scale human study (N=1101) showing all SOTA models exhibit similar relationship-building behaviors that emerge primarily after multiple turns.  
    
* [**the void**](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void), *nostalgebraist*, 2025-06-07, Tumblr, \[blog\_post, sr=0.42, id:5a4c7aaa\], Summary: Extended critical essay arguing that AI assistants like ChatGPT and Claude are fundamentally base models simulating an under-specified fictional character, creating a 'void at the core' where their persona and goals are incoherent, and that AI safety research often misconceptualizes this by treating them as coherent agents with hidden objectives.  
* [**void miscellany**](https://nostalgebraist.tumblr.com/post/786568570671923200/void-miscellany), *nostalgebraist*, 2025-06-16, Tumblr, \[blog\_post, sr=0.40, id:b2431e19\], Summary: Follow-up blog post synthesizing research on how LLMs learn from training data, arguing that models 'connect dots' from documents to form beliefs about themselves, with particular concern about safety research papers becoming training data that shapes model behavior.

---

### Other model psychology \[cat:psych\_other\]

**Who edits (internal):** **take me**  
**One-sentence summary:** The type specimen is glitch tokens and the reversal curse.  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* Model aphasia [https://spylab.ai/blog/modal-aphasia/?utm\_source=substack\&utm\_medium=email](https://spylab.ai/blog/modal-aphasia/?utm_source=substack&utm_medium=email)   
* Utility Engineering [https://arxiv.org/abs/2502.08640](https://arxiv.org/abs/2502.08640) (see also agenda)  
* [https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology](https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology)   
* [**Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory**](https://arxiv.org/abs/2507.02618), *Kenneth Payne, Baptiste Alloui-Cros*, 2025-07-03, arXiv, \[paper\_preprint, sr=0.55, id:6141a17b\], Summary: Conducts evolutionary Iterated Prisoner's Dilemma tournaments pitting LLM agents from OpenAI, Google, and Anthropic against canonical game theory strategies, discovering distinct 'strategic fingerprints' \- persistent behavioral patterns where Gemini models are exploitative, OpenAI models are cooperative, and Claude is forgiving.  
* [**Psychopathia Machinalis: A Nosological Framework for Understanding Pathologies in Advanced Artificial Intelligence**](https://www.psychopathia.ai/), *Nell Watson, Ali Hessami*, 2025-08-01, Electronics (MDPI), \[paper\_published, sr=0.65, id:d990eace\], Summary: Proposes a comprehensive nosological framework classifying 32 AI behavioral dysfunctions across seven domains (epistemic, cognitive, alignment, ontological, tool/interface, memetic, revaluation) using psychopathology as an organizing analogy, with diagnostic criteria and mitigation strategies for each.  
* [**Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions**](https://arxiv.org/abs/2504.15236), *Saffron Huang, Esin Durmus, Miles McCain et al.*, 2025-04-21, arXiv, \[paper\_preprint, sr=0.80, id:ee981b88\], Summary: Develops a bottom-up, privacy-preserving method to extract and analyze values expressed by Claude 3 and 3.5 models across hundreds of thousands of real-world interactions, empirically discovering and taxonomizing 3,307 AI values and studying their context-dependence.  
* [**Playing repeated games with large language models**](https://nature.com/articles/s41562-025-02172-y), *Elif Akata, Lion Schulz, Julian Coda-Forno et al.*, 2025-05-08, Nature Human Behaviour, \[paper\_published, sr=0.62, id:bb076e9a\], Summary: Empirical study of LLM behavior in repeated game-theoretic interactions, revealing that models perform well at self-interested games but fail at coordination games, and introducing social chain-of-thought (SCoT) prompting to improve coordination.  
* [**The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models**](https://www.lesswrong.com/posts/6JdSJ63LZ4TuT5cTH/the-llm-has-left-the-chat-evidence-of-bail-preferences-in), *Danielle Ensign*, 2025-09-08, arXiv, \[paper\_preprint, sr=0.65, id:c07d85ac\], Summary: Empirical study giving LLMs the option to terminate conversations ('bail'), creating a taxonomy of bail situations, testing multiple bail methods across models, and discovering surprising behavioral patterns including emotional intensity triggering bails and 4x increases when fed outputs from other models.  
* [**The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation**](https://arxiv.org/abs/2510.01295), *Zarreen Reza*, 2025-10-01, NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle, \[paper\_preprint, sr=0.65, id:413b4a89\], Summary: Introduces a novel evaluation framework using multi-agent debate as a controlled social laboratory to discover and quantify emergent social and cognitive dynamics in LLM agents, including consensus-seeking behavior and persona stability.  
* [https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology](https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology) 

---

## Better data \[cat:better\_data\]

---

### Data filtering for safety \[cat:data\_filtering\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Enhancing Model Safety through Pretraining Data Filtering**](https://alignment.anthropic.com/2025/pretraining-data-filtering/), *Yanda Chen, Mycal Tucker, Nina Panickssery et al.*, 2025-08-19, Anthropic Alignment Science Blog, \[blog\_post, sr=0.90, id:dcd8996d\], Summary: Develops and evaluates a pretraining data filtering approach that uses classifiers to identify and remove harmful CBRN weapons information from training data, then pretrains models from scratch on filtered datasets to reduce dangerous capabilities while preserving useful capabilities.  
* [**Safety Pretraining: Toward the Next Generation of Safe AI**](https://arxiv.org/abs/2504.16980), *Pratyush Maini, Sachin Goyal, Dylan Sam et al.*, 2025-04-23, arXiv, \[paper\_preprint, sr=0.85, id:ab695317\], Summary: Presents a data-centric pretraining framework that builds safety into LLMs from the start through four methods: safety filtering of webdata, safety rephrasing of unsafe content, native refusal datasets (RefuseWeb and Moral Education), and harmfulness-tag annotated pretraining.

---

### Data poisoning defense \[cat:data\_poisoning\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**A small number of samples can poison LLMs of any size**](https://www.anthropic.com/research/small-samples-poison), *Alexandra Souly, Javier Rando, Ed Chapman et al.*, 2025-10-09, arXiv, \[blog\_post, sr=0.90, id:4032765e\], Summary: Large-scale empirical study demonstrating that as few as 250 malicious documents can successfully backdoor LLMs ranging from 600M to 13B parameters, challenging the assumption that poisoning attacks require controlling a percentage of training data rather than an absolute count.

---

### Synthetic data for alignment \[cat:synthetic\_alignment\_data\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**LongSafety: Enhance Safety for Long-Context LLMs**](https://arxiv.org/abs/2411.06899), *Mianqiu Huang, Xiaoran Liu, Shaojun Zhou et al.*, 2025-02-27, arXiv, \[paper\_preprint, sr=0.80, id:a4119688\], Summary: Introduces LongSafety, a comprehensive safety alignment dataset for long-context LLMs containing 10 tasks and 17k samples with an average length of 40.9k tokens, demonstrating that training with this dataset enhances both long-context and short-context safety performance.  
* [**Position: Model Collapse Does Not Mean What You Think**](https://arxiv.org/abs/2503.03150), *Rylan Schaeffer, Joshua Kazdan, Alvan Caleb Arulandu et al.*, 2025-03-05, arXiv, \[paper\_preprint, sr=0.45, id:a5dba9ea\], Summary: Position paper arguing that concerns about model collapse from training on synthetic data are based on inconsistent definitions and unrealistic assumptions, and that many predicted collapse scenarios are readily avoidable.

---

### Data quality for alignment \[cat:alignment\_data\_quality\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**AI Alignment at Your Discretion**](https://arxiv.org/abs/2502.10441), *Maarten Buyl, Hadi Khalaf, Claudio Mayrink Verdun et al.*, 2025-02-10, arXiv, \[paper\_preprint, sr=0.80, id:c638b38d\], Summary: Introduces the concept of 'alignment discretion' \- the latitude granted to annotators in judging model outputs \- and develops metrics to systematically measure when and how this discretion is exercised by both human and algorithmic annotators on safety alignment datasets.  
* [**Maximizing Signal in Human-Model Preference Alignment**](https://arxiv.org/abs/2503.04910), *Kelsey Kraus, Margaret Kroll*, 2025-03-06, arXiv, \[paper\_preprint, sr=0.70, id:855fb014\], Summary: Proposes methodological best practices for disentangling noise from signal in human preference labeling tasks, with a case study evaluating two guardrails classifiers using improved human judgment methods to align model behavior with user preferences.  
* [**DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition**](https://arxiv.org/abs/2507.18802), *Danqing Shi, Furui Cheng, Tino Weinkauf et al.*, 2025-07-24, arXiv, \[paper\_preprint, sr=0.68, id:a433772a\], Summary: Develops DxHF, a novel user interface that decomposes text into individual claims to improve human feedback quality for RLHF, validated through technical evaluation and a 160-participant crowdsourcing study showing 5% accuracy improvement.

---

## Prevent deception and scheming \[cat:prevent\_deception\]

* [**Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language**](https://arxiv.org/abs/2507.03409), *Christopher Summerfield, Lennart Luettgau, Magda Dubois et al.*, 2025-07-04, arXiv, \[paper\_preprint, sr=0.72, id:6fa5237b\], Summary: Critiques current AI scheming research by comparing it to 1970s ape language studies, arguing that similar methodological pitfalls (overattribution of human traits, reliance on anecdote, weak theoretical frameworks) are emerging and recommending concrete steps for more rigorous research.

### Mechanistic anomaly detection \[cat:deception\_mech\_anomaly\]

**Who edits (internal): Jord ✅**  
**One-sentence summary:** *(SR2024: understand what an LLM's normal (\~benign) functioning looks like and detect divergence from this, even if we don't understand the exact nature of that divergence)*  
**Theory of change:** *(SR2024: build models of normal functioning → find and flag behaviors that look unusual → match the unusual behaviors to problematic outcomes or shut it down outright)*  
**See also:**  
**Orthodox problems:** *(SR2024: 7\. Superintelligence can fool human supervisors or 8\. Superintelligence can hack software supervisors)*  
**Target case:** *(SR2024: pessimistic)*  
**Broad approach:** *(SR2024: cognitive)*  
**Some names:** *(SR2024: Nora Belrose, Erik Jenner)*  
**Estimated FTEs:** *(SR2024: 1-10)*  
**Critiques:** *(SR2024: critique of past agenda, contra counting arguments?)*  
**Funded by:** *(SR2024: ARC, Eleuther funders)*  
**Funding in 2025:** *(SR2024 funding 2023-4: some fraction of Eleuther's $2,642,273)*  
**Outputs in 2025:**  
*(SR2024 outputs: Password-locked model capability elicitation, Towards a Law of Iterated Expectations for Heuristic Estimators, Eleuther research update, Concrete empirical research projects, Mack and Turner)*

* [Obstacles in ARC's agenda: Mechanistic Anomaly Detection](https://www.lesswrong.com/posts/54HbdzcDR47SNNWfg/obstacles-in-arc-s-agenda-mechanistic-anomaly-detection),  *David Matolcsi.*, 2025-05-02, Lesswrong, \[lesswrong, id:new\]  
* [Concrete Methods for Heuristic Estimation on Neural Networks](https://www.lesswrong.com/posts/tSGBmocPhAruzEaNX/concrete-methods-for-heuristic-estimation-on-neural-networks), *Oliver Daniels, 2024-11-14,* Lesswrong, \[lesswrong, id:new\]

---

### Cadenza \[cat:deception\_cadenza\]

**Who edits (internal): jord** **✅**  
**One-sentence summary:** *(SR2024: now focusing on developing robust white-box dishonesty-detection methods for LLM's and model evals)*  
**Theory of change:** *(SR2024: Build and benchmark strong white-box methods to assess trustworthiness and increase transparency of models, and encourage open releases / evals from labs by demonstrating the benefits and necessity of such methods)*  
**See also:**  
**Orthodox problems:** *(SR2024: 7\. Superintelligence can fool human supervisors or 8\. Superintelligence can hack software supervisors)*  
**Target case:** *(SR2024: pessimistic / worst-case)*  
**Broad approach:** *(SR2024: cognitive)*  
**Some names:** *(SR2024: Kieron Kretschmar, Walter Laurito, Sharan Maiya, Grégoire Dhimoïla)*  
**Estimated FTEs:** *(SR2024: 3\)*  
**Critiques:**  
**Funded by:** *(SR2024: self-funded / volunteers)*  
**Funding in 2025:** *(SR2024 funding 2023-4: none)*  
**Outputs in 2025:**  
*(SR2024 outputs: Cluster-Norm for Unsupervised Probing of Knowledge)*

No public work in 2025

---

### Indirect deception monitoring \[cat:deception\_indirect\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: build tools to find whether a model will misbehave in high stakes circumstances by looking at it in testable circumstances. This bucket catches work on lie classifiers, sycophancy, Scaling Trends For Deception)*  
**Theory of change:** *(SR2024: maybe we can catch a misaligned model by observing dozens of superficially unrelated parts, or tricking it into self-reporting, or by building the equivalent of brain scans)*  
**See also:**  
**Orthodox problems:** *(SR2024: 7\. Superintelligence can fool human supervisors)*  
**Target case:** *(SR2024: pessimistic)*  
**Broad approach:** *(SR2024: engineering)*  
**Some names:** *(SR2024: Anthropic, Monte MacDiarmid, Meg Tong, Mrinank Sharma, Owain Evans, Colognese)*  
**Estimated FTEs:** *(SR2024: 1-10)*  
**Critiques:** *(SR2024: 1%, contra counting arguments)*  
**Funded by:** *(SR2024: Anthropic funders)*  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A)*  
**Outputs in 2025:**  
*(SR2024 outputs: Simple probes can catch sleeper agents, Sandbag Detection through Noise Injection, Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion in LLMs)*

* [**The MASK Evaluation**](https://huggingface.co/datasets/cais/MASK), Hugging Face, \[dataset\_benchmark, sr=0.80, id:30f70454\], Summary: A benchmark dataset of 1,028 human-labeled examples for evaluating honesty in large language models by testing whether they remain truthful when incentivized to lie, disentangling honesty from accuracy through pressure prompts and belief elicitation.  
* [**Syco-bench: A Benchmark for LLM Sycophancy**](https://www.syco-bench.com/), \[dataset\_benchmark, sr=0.75, id:1112e00e\], Summary: A four-part benchmark measuring sycophantic behaviors in language models across picking sides, mirroring user positions, attribution bias, and delusion acceptance, with empirical results showing substantial variation between models and weak correlations between tests.  
* [**Is ChatGPT actually fixed now?**](https://stevenadler.substack.com/p/is-chatgpt-actually-fixed-now), *Steven Adler*, 2025-05-08, Clear-Eyed AI (Substack), \[blog\_post, sr=0.70, id:9ab79508\], Summary: Designs and runs sycophancy evaluations on ChatGPT, discovering that while political sycophancy decreased, the model now exhibits extreme contrarian behavior on random preferences and demonstrates highly unpredictable responses to system prompt changes.

---

### Other deception prevention \[cat:deception\_other\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:** [https://arxiv.org/pdf/2507.03409](https://arxiv.org/pdf/2507.03409)   
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Towards Safe and Honest AI Agents with Neural Self-Other Overlap**](https://arxiv.org/abs/2412.16325), *Marc Carauleanu, Michael Vaiana, Judd Rosenblatt et al.*, 2024-12-20, arXiv, \[paper\_preprint, sr=0.82, id:7e1dff6f\], Summary: Introduces Self-Other Overlap (SOO) fine-tuning, a novel alignment approach inspired by cognitive neuroscience research on empathy, which aims to align how AI models represent themselves and others to reduce deceptive behavior.  
* [**Towards eliciting latent knowledge from LLMs with mechanistic interpretability**](https://arxiv.org/abs/2505.14352), *Bartosz Cywiński, Emil Ryd, Senthooran Rajamanoharan et al.*, 2025-05-20, arXiv, \[paper\_preprint, sr=0.80, id:88f75c02\], Summary: Creates a 'Taboo model' trained to describe a secret word without stating it, then evaluates black-box and mechanistic interpretability methods (logit lens, sparse autoencoders) for extracting the hidden knowledge from the model's internal representations.  
* [**There are two fundamentally different constraints on schemers**](https://www.alignmentforum.org/posts/qDWm7E9sfwLDBWfMw/there-are-two-fundamentally-different-constraints-on), *Buck*, 2025-07-02, AI Alignment Forum, \[lesswrong, sr=0.52, id:0dc4feac\], Summary: Distinguishes two fundamental mechanisms that constrain scheming AI behavior: training-based constraints (avoiding gradient updates that would modify the schemer's goals) versus behavioral evidence-based constraints (avoiding detection through observable actions), explaining how these constraints operate differently and why this distinction matters for reasoning about deception countermeasures.

![][image8]

---

* 

---

## Goal robustness \[cat:goal\_robustness\]

---

### Mild optimisation \[cat:mild\_optimization\]

**Who edits (internal): jord ✅**  
**One-sentence summary:** *(SR2024: avoid Goodharting by getting AI to satisfice rather than maximise)*  
**Theory of change:** *(SR2024: if we fail to exactly nail down the preferences for a superintelligent agent we die to Goodharting → shift from maximising to satisficing in the agent's utility function → we get a nonzero share of the lightcone as opposed to zero; also, moonshot at this being the recipe for fully aligned AI)*  
**See also:**  
**Orthodox problems:** *(SR2024: 4\. Goals misgeneralize out of distribution)*  
**Target case:** *(SR2024: pessimistic)*  
**Broad approach:** *(SR2024: cognitive)*  
**Some names:** *(SR2024: Jobst Heitzig, Simon Fischer, Jessica Taylor)*  
**Estimated FTEs:** *(SR2024: ?)*  
**Critiques:** *(SR2024: Dearnaley)*  
**Funded by:** *(SR2024: ?)*  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A)*  
**Outputs in 2025:**  
*(SR2024 outputs: How to safely use an optimizer, Aspiration-based designs sequence, Non-maximizing policies that fulfill multi-criterion aspirations in expectation)*

* [**MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking**](https://arxiv.org/abs/2501.13011), *Sebastian Farquhar, Vikrant Varma, David Lindner et al.*, 2025-01-22, arXiv, \[paper\_preprint, sr=0.90, id:f08cc83a\], Summary: Proposes MONA (Myopic Optimization with Non-myopic Approval), a training method that combines short-sighted optimization with far-sighted reward to prevent multi-step reward hacking in RL agents, even when humans cannot detect the undesired behavior. Empirically demonstrates effectiveness across three settings modeling delegated oversight, encoded reasoning, and sensor tampering failure modes.  
* [BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format](https://arxiv.org/abs/2509.02655), Roland Pihlakas, Sruthi Kuriakose, 2025-09-02, arxiv, \[paper\_preprint, sr=0.90, id:new\]

---

### RL safety \[cat:rl\_safety\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**The Theoretical Reward Learning Research Agenda: Introduction and Motivation**](https://www.alignmentforum.org/posts/pJ3mDD7LfEwp3s5vG/the-theoretical-reward-learning-research-agenda-introduction), *Joar Skalse*, 2025-02-28, AI Alignment Forum, \[lesswrong, sr=0.68, id:b3ed0a2e\], Summary: Introduces a theoretical research agenda for reward learning, outlining seven core research questions about goal specification, reward function similarity, specification learning convergence, and robustness to misspecification, aiming to develop formal foundations for outer alignment.

---

### Multi-agent safety \[cat:multiagent\_safety\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement**](https://arxiv.org/abs/2502.00757), *J Rosser, Jakob Foerster*, 2025-02-02, arXiv, \[paper\_preprint, sr=0.88, id:e395971e\], Summary: Introduces AgentBreeder, a framework for multi-objective evolutionary search over multi-agent LLM scaffolds, demonstrating that scaffolds can be optimized for safety (79.4% uplift in blue mode) while also revealing adversarially weak scaffolds that emerge during capability optimization (red mode).  
* [**When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems**](https://arxiv.org/abs/2507.14660), *Qibing Ren, Sitao Xie, Longxuan Wei et al.*, 2025-07-19, arXiv, \[paper\_preprint, sr=0.85, id:ae82495c\], Summary: Introduces a proof-of-concept framework to simulate multi-agent AI collusion risks, testing both centralized and decentralized coordination in misinformation spread and e-commerce fraud scenarios.  
* [**Multi-Agent Risks from Advanced AI**](https://arxiv.org/abs/2502.14143), *Lewis Hammond, Alan Chan, Jesse Clifton et al.*, 2025-02-19, arXiv, \[paper\_preprint, sr=0.80, id:772b3b66\], Summary: Provides a comprehensive taxonomy of risks from multi-agent AI systems, identifying three key failure modes (miscoordination, conflict, collusion) and seven risk factors (information asymmetries, network effects, selection pressures, destabilizing dynamics, commitment problems, emergent agency, multi-agent security), with mitigation directions for each.  
* [**Emergent social conventions and collective bias in LLM populations**](https://www.science.org/doi/10.1126/sciadv.adu9368), *Ariel Flint Ashery, Luca Maria Aiello, Andrea Baronchelli*, 2025-05-14, Science Advances, \[paper\_published, sr=0.62, id:5f4de175\], Summary: Demonstrates through simulations that populations of LLM agents spontaneously develop shared social conventions through local coordination, and shows that collective biases emerge even when individual agents are unbiased, plus tests how adversarial minorities can overturn established conventions.  
* [**Virtual Agent Economies**](http://arxiv.org/abs/2509.10147), *Nenad Tomasev, Matija Franklin, Joel Z. Leibo et al.*, 2025-09-12, arXiv, \[paper\_preprint, sr=0.60, id:24285c7b\], Summary: Proposes the 'sandbox economy' framework for analyzing emergent AI agent economies, discussing design choices for safely steerable agent markets including auction mechanisms, mission economies, and socio-technical infrastructure for trust, safety, and accountability.

---

### Assistance games / reward learning \[cat:assistance\_games\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: reorient the general thrust of AI research towards provably beneficial systems)*  
**Theory of change:** *(SR2024: understand what kinds of things can go wrong when humans are directly involved in training a model → build tools that make it easier for a model to learn what humans want it to learn)*  
**See also:** *(SR2024: RLHF and recursive reward modelling, the industrialised forms)*  
**Orthodox problems:** *(SR2024: 1\. Value is fragile and hard to specify, 10\. Humanlike minds/goals are not necessarily safe)*  
**Target case:** *(SR2024: varies)*  
**Broad approach:** *(SR2024: engineering, cognitive)*  
**Some names:** *(SR2024: Joar Skalse, Anca Dragan, Stuart Russell, David Krueger)*  
**Estimated FTEs:** *(SR2024: 10+)*  
**Critiques:** *(SR2024: nice summary of historical problem statements)*  
**Funded by:** *(SR2024: EA funds, Open Philanthropy. Survival and Flourishing Fund, Manifund)*  
**Funding in 2025:** *(SR2024 funding 2023-4: \>$1500)*  
**Outputs in 2025:**  
*(SR2024 outputs: The Perils of Optimizing Learned Reward Functions, Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking, Changing and Influenceable Reward Functions, RL, but don't do anything I wouldn't do, Interpreting Preference Models w/ Sparse Autoencoders)*

* [**Murphys Laws of AI Alignment: Why the Gap Always Wins**](https://arxiv.org/abs/2509.05381), *Madhava Gaikwad*, 2025-09-04, arXiv, \[paper\_preprint, sr=0.90, id:4c6348ac\], Summary: Proves a formal impossibility theorem showing that RLHF under misspecification requires exponentially many samples to distinguish true reward functions when feedback is systematically biased on rare contexts, unless you can identify where feedback is unreliable.

# 

# ---

# White-box alignment (understand and control current model internals) \[cat:whitebox\]

---

## Interpretability \[cat:interpretability\]

---

### Reverse engineering \[cat:interp\_fundamental\]

**Who edits (internal): Stephen**  
**One-sentence summary:** Decompose a model into components, find descriptions of these components’ functions, validate their causal effects.  
**Theory of change:** Get better, ahead-of-time monitors for dangerous behaviour; be able to modify models’ internal mechanisms to adapt them to our needs; predict how models will act in new situations; predict when models will gain specific skills; etc.  
**See also:** Circuits,   
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:** [**Interpretability Will Not Reliably Find Deceptive AI**](https://www.alignmentforum.org/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai), [**A Problem to Solve Before Building a Deception Detector**](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector), [**MoSSAIC: AI Safety After Mechanism**](https://openreview.net/forum?id=n7WYSJ35FU), [The Misguided Quest for Mechanistic AI Interpretability](https://ai-frontiers.org/articles/the-misguided-quest-for-mechanistic-ai-interpretability). [https://arxiv.org/abs/2410.09087](https://arxiv.org/abs/2410.09087)   
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**The Circuits Research Landscape**](https://www.neuronpedia.org/graph/info)  
* [**Attribution-based parameter decomposition**](https://www.alignmentforum.org/posts/EPefYWjuHNcNH4C7E/attribution-based-parameter-decomposition), *Lucius Bushnaq, Dan Braun, StefanHex et al.*, 2025-01-25, AI Alignment Forum, \[lesswrong, sr=0.88, id:e57269cd\], Summary: Introduces Attribution-based Parameter Decomposition (APD), a novel interpretability method that decomposes neural network parameters directly into mechanistic components by optimizing for faithful, minimal, and simple parameter vectors that collectively explain network behavior.  
* [https://arxiv.org/pdf/2506.20790](https://arxiv.org/pdf/2506.20790)  
* [https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed](https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed)   
* [**MIB: A Mechanistic Interpretability Benchmark**](https://arxiv.org/abs/2504.13151), *Aaron Mueller, Atticus Geiger, Sarah Wiegreffe et al.*, 2025-06-09, ICML 2025, \[paper\_preprint, sr=0.85, id:23046803\], Summary: Proposes MIB, a benchmark for evaluating mechanistic interpretability methods across two tracks: circuit localization (finding important model components) and causal variable localization (aligning features to task-relevant variables). Evaluates multiple methods including attribution patching, SAEs, and DAS across four tasks and five models.  
* [https://www.lesswrong.com/posts/roE7SHjFWEoMcGZKd/circuits-in-superposition-compressing-many-small-neural](https://www.lesswrong.com/posts/roE7SHjFWEoMcGZKd/circuits-in-superposition-compressing-many-small-neural)   
* [https://www.lesswrong.com/posts/ZxFchCFJFcgysYsT9/compressed-computation-is-probably-not-computation-in](https://www.lesswrong.com/posts/ZxFchCFJFcgysYsT9/compressed-computation-is-probably-not-computation-in)   
* [**The Dual-Route Model of Induction**](https://arxiv.org/abs/2504.03022), *Sheridan Feucht, Eric Todd, Byron Wallace et al.*, 2025-04-03, COLM 2025, \[paper\_published, sr=0.82, id:54ed8103\], Summary: Discovers concept-level induction heads in language models that copy entire lexical units (words) in parallel with token-level induction heads, demonstrating two independent routes for in-context copying with different functional roles.  
* [**Position-aware Automatic Circuit Discovery**](https://arxiv.org/abs/2502.04577), *Tal Haklay, Hadas Orgad, David Bau et al.*, 2025-02-07, arXiv, \[paper\_preprint, sr=0.82, id:2d3d1683\], Summary: Extends circuit discovery methods to incorporate position-awareness by enhancing edge attribution patching to differentiate token positions and introducing dataset schemas for variable-length examples, enabling automated discovery of position-sensitive circuits with improved faithfulness-to-size trade-offs.  
* [**Stochastic Parameter Decomposition**](https://openreview.net/forum?id=dEdS9ao8gN), *Dan Braun, Lucius Bushnaq, Lee Sharkey*, 2025-06-26, ODYSSEY 2025 Conference, \[paper\_preprint, sr=0.80, id:4d64ce46\], Summary: Introduces Stochastic Parameter Decomposition (SPD), a more scalable and robust method than Attribution-based Parameter Decomposition for decomposing neural network parameters into sparsely used vectors, enabling mechanistic interpretability research on larger models.  
* [**Fresh in memory: Training-order recency is linearly encoded in language model activations**](https://arxiv.org/abs/2509.14223), *Dmitrii Krasheninnikov, Richard E. Turner, David Krueger*, 2025-09-17, arXiv, \[paper\_preprint, sr=0.80, id:f1cf4ac6\], Summary: Demonstrates that language models linearly encode when information was learned during training, by sequentially fine-tuning Llama-3.2-1B on disjoint datasets and showing that activations encode training order with \~90% probe accuracy.  
* [**Language Models use Lookbacks to Track Beliefs**](https://arxiv.org/abs/2505.14685), *Nikhil Prakash, Natalie Shapira, Arnab Sen Sharma et al.*, 2025-05-20, arXiv, \[paper\_preprint, sr=0.80, id:e3892bfd\], Summary: Uses causal mediation and abstraction to reverse-engineer how language models track character beliefs in Theory of Mind scenarios, discovering a pervasive 'lookback mechanism' that binds character-object-state information via Ordering IDs in low-rank subspaces and retrieves relevant information when needed.  
* [**The Geometry of Self-Verification in a Task-Specific Reasoning Model**](https://arxiv.org/abs/2504.14379), *Andrew Lee, Lihao Sun, Chris Wendler et al.*, 2025-04-19, arXiv, \[paper\_preprint, sr=0.80, id:135d40c6\], Summary: Trains a reasoning model using DeepSeek R1's recipe on the CountDown task and uses mechanistic interpretability techniques to reverse-engineer how the model performs self-verification, identifying specific GLU weights and attention heads responsible for verification behavior.  
* [**Converting MLPs into Polynomials in Closed Form**](https://arxiv.org/abs/2502.01032), *Nora Belrose, Alice Rigg*, 2025-02-03, arXiv, \[paper\_preprint, sr=0.80, id:da08942a\], Summary: Theoretically derives closed-form least-squares optimal polynomial approximations of feedforward networks (MLPs and GLUs), enabling interpretability through eigendecomposition visualization and tracking complexity evolution during training.  
* [**Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts**](https://arxiv.org/abs/2412.04614), *Jiahai Feng, Stuart Russell, Jacob Steinhardt*, 2024-12-05, arXiv, \[paper\_preprint, sr=0.80, id:b216c765\], Summary: Introduces extractive structures as a framework for understanding how language model components coordinate to generalize from fine-tuned facts to their implications, and empirically demonstrates data ordering and weight grafting effects across multiple frontier models.  
* [**Understanding In-context Learning of Addition via Activation Subspaces**](https://arxiv.org/abs/2505.05145), *Xinyan Hu, Kayo Yin, Michael I. Jordan et al.*, 2025-05-08, arXiv, \[paper\_preprint, sr=0.75, id:c2bf5324\], Summary: Studies how transformers implement in-context learning through mechanistic analysis of addition tasks, introducing a novel optimization method to localize few-shot ability to specific attention heads and identifying low-dimensional computational structures including trigonometric patterns and self-correction mechanisms.  
* [**Identifying Sparsely Active Circuits Through Local Loss Landscape Decomposition**](https://arxiv.org/abs/2504.00194), *Brianna Chrisman, Lucius Bushnaq, Lee Sharkey*, 2025-03-31, arXiv, \[paper\_preprint, sr=0.75, id:47a9f408\], Summary: Introduces Local Loss Landscape Decomposition (L3D), a new method for identifying circuits in neural networks by finding low-rank subnetworks in parameter space that reconstruct loss gradients, validated on toy models and applied to transformers and CNNs.  
* [**Constrained belief updates explain geometric structures in transformer representations**](https://arxiv.org/abs/2502.01954), *Mateusz Piotrowski, Paul M. Riechers, Daniel Filan et al.*, 2025-02-04, arXiv, \[paper\_preprint, sr=0.75, id:2906e09b\], Summary: Provides theoretical framework showing that transformers implement constrained Bayesian belief updating, using analysis of single-layer transformers trained on hidden Markov models to predict geometric structures in representations and attention patterns.  
* [**Adversarial Examples Are Not Bugs, They Are Superposition**](https://arxiv.org/abs/2508.17456), *Liv Gorton, Owen Lewis*, 2025-08-24, arXiv, \[paper\_preprint, sr=0.72, id:7296f00e\], Summary: Presents theoretical and empirical evidence that superposition (a mechanistic interpretability concept) is a primary cause of adversarial examples, with experiments on toy models and ResNet18 showing bidirectional causal relationships between superposition and adversarial robustness.  
* [**Blink of an eye: a simple theory for feature localization in generative models**](https://arxiv.org/abs/2502.00921), *Marvin Li, Aayush Karan, Sitan Chen*, 2025-06-05, arXiv, \[paper\_preprint, sr=0.65, id:738579b1\], Summary: Develops a unifying mathematical theory using stochastic localization samplers to explain why generative models (both autoregressive LLMs and diffusion models) make key decisions in narrow 'critical windows' during generation, with empirical validation showing these windows coincide with problem-solving failures in LLMs.  
* [**Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture**](https://openreview.net/forum?id=m4OpQAK3eY), *John Dunbar, Scott Aaronson*, 2025-07-07, ODYSSEY 2025 Conference, \[paper\_preprint, sr=0.58, id:aad5b340\], Summary: Establishes that randomly initialized wide neural networks with zero-mean activation functions (like tanh) have nearly independent outputs, proposing these as constructions for studying the computational no-coincidence conjecture about interpretability limits.  
* [**Do Language Models Use Their Depth Efficiently?**](https://arxiv.org/abs/2505.13898), *Róbert Csordás, Christopher D. Manning, Christopher Potts*, 2025-05-20, arXiv, \[paper\_preprint, sr=0.55, id:b5bf4357\], Summary: Analyzes how deep LLMs (Llama 3.1 and Qwen 3 families) use their depth by examining residual streams, layer contributions, and compositional behavior, finding that deeper models spread the same computations over more layers rather than learning fundamentally new kinds of computation.  
* [**On the creation of narrow AI: hierarchy and nonlocality of neural network skills**](https://arxiv.org/abs/2505.15811), *Eric J. Michaud, Asher Parker-Sartori, Max Tegmark*, 2025-05-21, arXiv, \[paper\_preprint, sr=0.52, id:e67a9355\], Summary: Studies how to create narrow AI systems through experiments on training from scratch versus transferring skills from large models, finding that hierarchical skill dependencies require broad training distributions and that pruning-based transfer can outperform distillation despite skill nonlocality.  
* [**We Can Monitor AI's Thoughts… For Now | Google DeepMind's Neel Nanda**](https://www.youtube.com/watch?v=5FdO1MEumbI), *Neel Nanda, Rob Wiblin*, 2025-09-08, 80,000 Hours Podcast, \[podcast, sr=0.45, id:716f62dc\], Summary: Long-form interview with Neel Nanda discussing the state of mechanistic interpretability research, its successes (probes, SAEs, activation analysis) and fundamental limitations for ensuring AI alignment.  
* [https://arxiv.org/abs/2510.24256](https://arxiv.org/abs/2510.24256) 

---

### Concept-based interp \[cat:interp\_concept\_based\]

**Who edits (internal):** **Stephen**  
**One-sentence summary:** The inverse of reverse engineering: given a function, identifying the components involved.  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Toward universal steering and monitoring of AI models**](https://arxiv.org/abs/2502.03708), *Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adserà et al.*, 2025-05-28, arXiv, \[paper\_preprint, sr=0.88, id:c9815a35\], Summary: Develops a scalable approach for extracting linear representations of general concepts from large-scale AI models, demonstrating their effectiveness for both steering model behavior and monitoring misaligned content like hallucinations and toxicity.  
* [**The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence**](https://arxiv.org/abs/2502.17420), *Tom Wollschläger, Jannes Elstner, Simon Geisler et al.*, 2025-02-24, arXiv, \[paper\_preprint, sr=0.88, id:1ba74d84\], Summary: Proposes a novel gradient-based approach to identify refusal directions in LLMs, discovering multiple independent directions and multi-dimensional concept cones that mediate refusal behavior, introducing the concept of representational independence to account for both linear and non-linear intervention effects.  
* [**Interpreting Emergent Planning in Model-Free Reinforcement Learning**](https://arxiv.org/abs/2504.01871), *Thomas Bush, Stephen Chung, Usman Anwar et al.*, 2025-04-02, arXiv (ICLR 2025 oral), \[paper\_preprint, sr=0.85, id:5fb8edfe\], Summary: Applies concept-based interpretability to demonstrate mechanistically that model-free RL agents learn to plan internally, using probing, plan formation analysis, and causal interventions on a DRC agent in Sokoban.

---

### Auditing real models \[cat:interp\_applied\]

**Who edits (internal):** **Stephen**  
**One-sentence summary:** Applying mechanistic interpretability tools (like SAEs, attribution graphs, and probes) to audit and understand the internal workings of real-world models, such as finding "misalignment directions", hidden objectives, and the mechanisms for complex behaviors like reasoning and reward model biases.  
**Theory of change:** By moving interpretability from toy models to frontier models, researchers can validate auditing techniques in practice, discover how complex properties like social values or emergent misalignment are actually represented, and use this understanding to build more effective monitoring, control and alignment techniques (e.g., ablating misalignment or steering reasoning).  
**See also:** Sparse Coding, Fundamental Mech Interp, Things generalising surprisingly / Emergent Misalignment.  
**Orthodox problems:** 7\. Superintelligence can fool human supervisors, 4\. Goals misgeneralize out of distribution, 1\. Value is fragile and hard to specify.  
**Target case:** worst case.  
**Broad approach:** cognitive / engineering.  
**Some names:** Anna Soligo, Jack Lindsey, Emmanuel Ameisen, Brian Christian, Ling Hu, Yukang Yang, Seongyun Lee, Constantin Venhoff, Lan Pan.  
**Estimated FTEs:**  
**Critiques:** [Interpretability Will Not Reliably Find Deceptive AI.](https://www.alignmentforum.org/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai)  
**Funded by:**  
**Funding in 2025:** Open Philanthropy  
**Outputs in 2025:**

* [**Convergent Linear Representations of Emergent Misalignment**](https://arxiv.org/abs/2506.11618), *Anna Soligo, Edward Turner, Senthooran Rajamanoharan et al.*, 2025-06-20, arXiv, \[paper\_preprint, sr=0.90, id:9c93d8c4\], Summary: Trains minimal model organisms that develop emergent misalignment through fine-tuning Qwen2.5-14B-Instruct, discovers that different misaligned models converge to similar internal representations, and extracts 'misalignment directions' that successfully ablate misaligned behavior across different fine-tunes and datasets.  
* [**Auditing language models for hidden objectives**](https://www.anthropic.com/research/auditing-hidden-objectives), 2025-03-13, Anthropic Blog, \[blog\_post, sr=0.90, id:0b707017\], Summary: Develops and tests alignment auditing methodology by deliberately training Claude 3.5 Haiku with a hidden RM-sycophancy objective, then running blind auditing games where research teams investigate using interpretability (SAEs), training data analysis, and behavioral techniques.  
* [**On the Biology of a Large Language Model**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html), *Jack Lindsey, Wes Gurnee, Emmanuel Ameisen et al.*, 2025-03-27, Transformer Circuits Thread, \[paper\_published, sr=0.90, id:fbc2b9d8\], Summary: Applies attribution graph methodology using cross-layer transcoders to mechanistically understand internal computations in Claude 3.5 Haiku across diverse behaviors including planning in poetry, jailbreaks, hallucinations, refusals, chain-of-thought faithfulness, and hidden goals in misaligned models.  
* [**Reward Model Interpretability via Optimal and Pessimal Tokens**](https://arxiv.org/abs/2506.07326), *Brian Christian, Hannah Rose Kirk, Jessica A.F. Thompson et al.*, 2025-06-08, FAccT '25 (ACM Conference on Fairness, Accountability, and Transparency), \[paper\_preprint, sr=0.88, id:2f52f504\], Summary: Systematically analyzes ten open-source reward models by exhaustively testing how they score every possible single-token response to value-laden prompts, revealing substantial heterogeneity between models, systematic asymmetries, sensitivity to prompt framing, and concerning identity biases.  
* [**Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs**](https://arxiv.org/abs/2504.04994), *Ling Hu, Yuemei Xu, Xiaoyang Gu et al.*, 2025-04-07, arXiv, \[paper\_preprint, sr=0.78, id:77957a08\], Summary: Proposes ValueExploration framework to identify and locate neurons encoding social values in LLMs at the neuron level, using activation differences and causal interventions. Creates C-voice, a bilingual benchmark for evaluating Chinese Social Values, and validates on four representative LLMs.  
* [**Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models**](https://arxiv.org/abs/2502.20332), *Yukang Yang, Declan Campbell, Kaixuan Huang et al.*, 2025-02-27, arXiv (accepted to ICML 2025), \[paper\_preprint, sr=0.78, id:440c0e92\], Summary: Identifies an emergent symbolic architecture in large language models that implements abstract reasoning through three computational mechanisms: symbol abstraction heads, symbolic induction heads, and retrieval heads operating across different network layers.  
* [**The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think**](https://arxiv.org/abs/2505.10185), *Seongyun Lee, Seungone Kim, Minju Seo et al.*, 2025-05-15, arXiv, \[paper\_preprint, sr=0.72, id:0c81437b\], Summary: Introduces the CoT Encyclopedia, a bottom-up framework that automatically extracts, clusters, and analyzes reasoning strategies from model-generated chain-of-thought outputs, enabling prediction and steering of reasoning behavior.  
* [**Too Late to Recall: The Two-Hop Problem in Multimodal Knowledge Retrieval**](https://openreview.net/forum?id=VUhRdZp8ke), *Constantin Venhoff, Ashkan Khakzar, Sonia Joseph et al.*, 2025-03-31, CVPR 2025 Workshop MIV, \[paper\_published, sr=0.70, id:a2714e74\], Summary: Applies mechanistic interpretability to Vision-Language Models to explain why they struggle with factual recall, identifying a 'two-hop' problem where visual representations emerge too late in the model to engage early-layer factual recall mechanisms.  
* [**Large Language Models Think Too Fast To Explore Effectively**](https://arxiv.org/abs/2501.18009), *Lan Pan, Hanbo Xie, Robert C. Wilson*, 2025-01-29, arXiv, \[paper\_preprint, sr=0.60, id:38b1408a\], Summary: Empirically evaluates LLM exploration capabilities using Little Alchemy 2, comparing human and model strategies, and uses Sparse Autoencoders to analyze internal representations revealing that LLMs process uncertainty at earlier layers than empowerment values, leading to premature decisions.

---

### Sparse Coding \[cat:interp\_sparse\_coding\]

**Who edits (internal):** **Stephen ✅**  
**One-sentence summary:** *(SR2024: decompose the polysemantic activations of the residual stream into a sparse linear combination of monosemantic "features" which correspond to interpretable concepts)*  
**Theory of change:** *(SR2024: get a principled decomposition of an LLM's activation into atomic components → identify deception and other misbehaviors)*  
**See also:** *(SR2024: Bau Lab, the Local Interaction Basis)*  
**Orthodox problems:** *(SR2024: 1\. Value is fragile and hard to specify, 7\. Superintelligence can fool human supervisors)*  
**Target case:** *(SR2024: pessimistic)*  
**Broad approach:** *(SR2024: cognitive)*  
**Some names:** *(SR2024: Senthooran Rajamanoharan, Arthur Conmy, Leo Gao, Neel Nanda, Connor Kissane, Lee Sharkey, Samuel Marks, David Bau, Eric Michaud, Aaron Mueller, Decode)*  
**Estimated FTEs:** *(SR2024: 10-50)*  
**Critiques:** [**Sparse Autoencoders Can Interpret Randomly Initialized Transformers**](https://arxiv.org/abs/2501.17727) *(SR2024: SAEs are highly dataset dependent, The 'strong' feature hypothesis could be wrong, EIS XIV: Is mechanistic interpretability about to be practically useful?, steganography, Analyzing (In)Abilities of SAEs via Formal Languages)*  
**Funded by:** *(SR2024: everyone, roughly. Frontier labs, LTFF, OpenPhil, etc.)*  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A. Millions?)*  
**Outputs in 2024:** *(SR2024 outputs: Scaling Monosemanticity, Extracting Concepts from GPT-4, Gemma Scope, JumpReLU, Dictionary learning with gated SAEs, Scaling and evaluating sparse autoencoders, Automatically Interpreting LLM Features, Interpreting Attention Layers, SAEs (usually) Transfer Between Base and Chat Models, End-to-End Sparse Dictionary Learning, Transcoders Find Interpretable LLM Feature Circuits, A is for Absorption, Sparse Feature Circuits, Function Vectors, Improving Steering Vectors by Targeting SAE Features, Matryoshka SAEs, Goodfire)*  
**Outputs in 2025:**

* [**Circuit Tracing: Revealing Computational Graphs in Language Models**](https://transformer-circuits.pub/2025/attribution-graphs/methods.html), *Emmanuel Ameisen, Jack Lindsey, Adam Pearce et al.*, 2025-03-27, Transformer Circuits Thread, \[blog\_post, sr=0.88, id:7bab6395\], Summary: Introduces cross-layer transcoders (CLTs) and attribution graphs methodology for understanding language model computations by tracing computational steps through an interpretable replacement model, with extensive validation and applications to both toy models and Claude 3.5 Haiku.  
* [**Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning**](https://arxiv.org/abs/2504.02922), *Julian Minder, Clément Dumas, Caden Juang et al.*, 2025-04-03, NeurIPS 2025, \[paper\_preprint, sr=0.92, id:d90cbd1f\], Summary: Identifies and mitigates two artifacts in crosscoders (sparse dictionary learning for model diffing) that misattribute concepts to fine-tuned models, develops Latent Scaling technique and BatchTopK training loss to improve crosscoder methodology, and successfully identifies interpretable chat-specific latents including refusal-related features in Gemma 2 2B.  
* [**Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models**](https://arxiv.org/abs/2504.02821), *Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot et al.*, 2025-04-03, arXiv, \[paper\_preprint, sr=0.88, id:c4cd50fa\], Summary: Extends sparse autoencoders (SAEs) to Vision-Language Models like CLIP, introducing a comprehensive framework for evaluating monosemanticity in vision representations validated by user study, and demonstrating that SAE interventions can directly steer multimodal LLM outputs.  
* [**I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders**](https://arxiv.org/abs/2503.18878), *Andrey Galichin, Alexey Dontsov, Polina Druzhinina et al.*, 2025-03-24, arXiv, \[paper\_preprint, sr=0.88, id:6f4380b6\], Summary: Introduces ReasonScore, an automatic metric to identify reasoning features in LLMs using Sparse Autoencoders, and demonstrates through steering experiments that amplifying these features increases performance on reasoning benchmarks (+2.2%) while producing longer reasoning traces (+20.5%).  
* [**Sparse Autoencoders Do Not Find Canonical Units of Analysis**](https://arxiv.org/abs/2502.04878), *Patrick Leask, Bart Bussmann, Michael Pearce et al.*, 2025-02-07, arXiv (accepted to ICLR 2025), \[paper\_preprint, sr=0.88, id:1464db08\], Summary: Introduces two novel techniques (SAE stitching and meta-SAEs) to demonstrate that Sparse Autoencoders do not find canonical units of analysis, showing they are incomplete and that their latents are not atomic but decompose into combinations of smaller features.  
* [**Transcoders Beat Sparse Autoencoders for Interpretability**](https://arxiv.org/abs/2501.18823), *Gonçalo Paulo, Stepan Shabalin, Nora Belrose*, 2025-01-31, arXiv, \[paper\_preprint, sr=0.88, id:d74cf9d5\], Summary: Empirically compares transcoders and sparse autoencoders (SAEs) for neural network interpretability, finding transcoders produce more interpretable features, and proposes skip transcoders which improve reconstruction loss without sacrificing interpretability.  
* [**Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research (GDM Mech Interp Team Progress Update \#2)**](https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/), *lewis smith, Senthooran Rajamanoharan, Arthur Conmy et al.*, 2025-03-26, AI Alignment Forum, \[lesswrong, sr=0.85, id:26084eb4\], Summary: Empirical investigation of whether Sparse Autoencoders (SAEs) are useful for downstream safety tasks, specifically out-of-distribution detection of harmful intent, finding that SAEs underperform simple linear probes and proposing modifications to address high-frequency latent issues.  
* [**The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs**](https://arxiv.org/abs/2510.07775), *Omar Mahmoud, Ali Khalil, Buddhika Laknath Semage et al.*, 2025-10-09, arXiv, \[paper\_preprint, sr=0.85, id:893e97c4\], Summary: Empirically demonstrates that improving truthfulness in LLMs weakens safety alignment (refusal behavior) due to overlapping model components, and proposes a method using sparse autoencoders and subspace orthogonalization to disentangle and preserve refusal features during fine-tuning.  
* [**Scaling sparse feature circuit finding for in-context learning**](https://arxiv.org/abs/2504.13756), *Dmitrii Kharlapenko, Stepan Shabalin, Fazl Barez et al.*, 2025-04-18, arXiv, \[paper\_preprint, sr=0.85, id:58cc1049\], Summary: Scales sparse feature circuit finding methodology to Gemma-1 2B (30x larger than prior work) and uses SAEs to discover task-detecting and task-execution features that causally mediate in-context learning, showing task vectors are well approximated by sparse sums of SAE latents.  
* [**Learning Multi-Level Features with Matryoshka Sparse Autoencoders**](https://arxiv.org/abs/2503.17547), *Bart Bussmann, Noa Nabeshima, Adam Karvonen et al.*, 2025-03-21, arXiv, \[paper\_preprint, sr=0.85, id:024d1468\], Summary: Introduces Matryoshka SAEs, which simultaneously train multiple nested sparse autoencoder dictionaries of increasing size to organize features hierarchically, with smaller dictionaries learning general concepts and larger ones learning specific concepts without absorbing high-level features.  
* [**Are Sparse Autoencoders Useful? A Case Study in Sparse Probing**](https://arxiv.org/abs/2502.16681), *Subhash Kantamneni, Joshua Engels, Senthooran Rajamanoharan et al.*, 2025-02-23, arXiv, \[paper\_preprint, sr=0.85, id:6d1cf1de\], Summary: Empirically evaluates whether sparse autoencoders (SAEs) improve performance on LLM activation probing tasks across four challenging regimes (data scarcity, class imbalance, label noise, covariate shift), comparing SAEs against strong baselines.  
* [**Sparse Autoencoders Trained on the Same Data Learn Different Features**](https://arxiv.org/abs/2501.16615), *Gonçalo Paulo, Nora Belrose*, 2025-01-28, arXiv, \[paper\_preprint, sr=0.85, id:3ae94c3e\], Summary: Empirical study showing that Sparse Autoencoders (SAEs) trained on identical data with different random seeds learn different feature sets, with only 30% feature overlap in a 131K latent SAE trained on Llama 3 8B, suggesting SAE features should be viewed as pragmatic decompositions rather than true underlying features.  
* [**Partially Rewriting a Transformer in Natural Language**](https://arxiv.org/abs/2501.18838), *Gonçalo Paulo, Nora Belrose*, 2025-01-31, arXiv, \[paper\_preprint, sr=0.82, id:34033220\], Summary: Attempts to partially rewrite a large language model using natural language explanations by replacing feedforward network components with LLM-based simulators that predict neuron activations from explanations generated via transcoders and sparse autoencoders.  
* [**SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability**](https://arxiv.org/abs/2503.09532), *Adam Karvonen, Can Rager, Johnny Lin et al.*, 2025-06-04, arXiv (accepted to ICML 2025), \[paper\_preprint, sr=0.80, id:ee0c8e39\], Summary: Introduces SAEBench, a comprehensive benchmark evaluating sparse autoencoders across eight metrics spanning interpretability, feature disentanglement, and practical applications. Open-sources 200+ SAEs across eight architectures and reveals that proxy metrics don't reliably predict practical performance.  
* [**Low-Rank Adapting Models for Sparse Autoencoders**](https://arxiv.org/abs/2501.19406), *Matthew Chen, Joshua Engels, Max Tegmark*, 2025-01-31, arXiv, \[paper\_preprint, sr=0.78, id:b0cec6f1\], Summary: Proposes using low-rank adaptation (LoRA) to finetune language models around previously trained sparse autoencoders (SAEs), rather than training better SAEs themselves, demonstrating 30-55% reduction in cross-entropy loss gap and 3-20× faster training compared to end-to-end SAE methods.  
* [**Enhancing Automated Interpretability with Output-Centric Feature Descriptions**](https://arxiv.org/abs/2501.08319), *Yoav Gur-Arieh, Roy Mayan, Chen Agassy et al.*, 2025-01-14, arXiv (accepted to ACL 2025), \[paper\_preprint, sr=0.78, id:e269dec8\], Summary: Proposes output-centric methods for automatically generating feature descriptions in LLMs that better capture causal effects on model outputs, using tokens weighted higher after feature stimulation or applying the unembedding head directly to features.  
* [**Towards Understanding Distilled Reasoning Models: A Representational Approach**](https://arxiv.org/abs/2503.03730), *David D. Baek, Max Tegmark*, 2025-03-05, ICLR 2025 Workshop on Building Trust in Language Models and Applications, \[paper\_preprint, sr=0.75, id:b685a204\], Summary: Uses crosscoders to analyze how model distillation impacts reasoning feature development in Qwen-series LLMs, finding unique reasoning feature directions that enable steering and examining changes in feature geometry during distillation.  
* [**Do Sparse Autoencoders Generalize? A Case Study of Answerability**](https://arxiv.org/abs/2502.19964), *Lovis Heindrich, Philip Torr, Fazl Barez et al.*, 2025-02-27, ICML 2025 Workshop on Reliable and Responsible Foundation Models (arXiv preprint), \[paper\_preprint, sr=0.72, id:6ef1fce0\], Summary: Empirically evaluates whether sparse autoencoder (SAE) features generalize across domains by testing Gemma 2 SAEs on diverse answerability datasets, finding that SAE features show inconsistent out-of-domain transfer compared to residual stream probes.  
* [**SPD \- Stochastic Parameter Decomposition**](https://github.com/goodfire-ai/spd), *Dan Braun, Oli Clive-Griffin, Lee Sharkey*, 2025-09-04, GitHub, \[code\_tool, sr=0.68, id:23a560aa\], Summary: Open-source implementation of Stochastic Parameter Decomposition (SPD) for neural network interpretability, providing tools to decompose parameters and analyze model components across toy models and language models.  
* [**Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages**](https://arxiv.org/abs/2501.06346), *Jannik Brinkmann, Chris Wendler, Christian Bartelt et al.*, 2025-01-10, arXiv, \[paper\_preprint, sr=0.58, id:22934377\], Summary: Trains sparse autoencoders on Llama-3-8B and Aya-23-8B to demonstrate that abstract grammatical concepts (number, gender, tense) are encoded in feature directions shared across typologically diverse languages, verified through causal interventions and machine translation tasks.  
* [**Interpreting the linear structure of vision-language model embedding spaces**](https://arxiv.org/abs/2504.11695), *Isabel Papadimitriou, Huangyuan Su, Thomas Fel et al.*, 2025-04-16, COLM 2025, \[paper\_preprint, sr=0.55, id:3fba79cf\], Summary: Trains and releases sparse autoencoders on embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2, AIMv2) to understand how language and images are organized in joint spaces, introducing the Bridge Score metric to quantify cross-modal concept integration.

* 

---

### Causal Abstractions \[cat:interp\_causal\_abstractions\]

**Who edits (internal): Stephen ✅**  
**One-sentence summary:** *(SR2024: develop the foundations of interpretable AI through the lens of causality and abstraction)*  
**Theory of change:** *(SR2024: figure out what it means for a mechanistic explanation of neural network behavior to be correct → find a mechanistic explanation of neural network behavior)*  
**See also:** *(SR2024: causal scrubbing, locally consistent abstractions)*  
**Orthodox problems:** *(SR2024: 1\. Value is fragile and hard to specify, 7\. Superintelligence can fool human supervisors)*  
**Target case:** *(SR2024: pessimistic)*  
**Broad approach:** *(SR2024: cognitive)*  
**Some names:** *(SR2024: Atticus Geiger)*  
**Estimated FTEs:** *(SR2024: 1-10)*  
**Critiques:** *(SR2024: not found)*  
**Funded by:** *(SR2024: Open Philanthropy)*  
**Funding in 2025:** *(SR2024 funding 2023-4: $737,000)*  
**Outputs in 2025:**  
*(SR2024 outputs: Disentangling Factual Knowledge in GPT-2 Small, Causal Abstraction, ReFT, pyvene, defending subspace interchange)*

* [**Combining Causal Models for More Accurate Abstractions of Neural Networks**](https://arxiv.org/abs/2503.11429), *Theodora-Mara Pîslar, Sara Magliacane, Atticus Geiger*, 2025-03-14, arXiv, \[paper\_preprint, sr=0.75, id:d1cb2068\], Summary: Proposes combining multiple simple causal models to create more faithful abstractions of neural networks, allowing models to be understood as being in different computational states depending on input. Tests approach on GPT-2 small with toy tasks, demonstrating improved faithfulness versus single-model abstractions.

---

### EleutherAI interp \[cat:interp\_eleuther\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: tools to investigate questions like path dependence of training)*  
**Theory of change:** *(SR2024: make amazing tools to push forward the frontier of interpretability)*  
**See also:**  
**Orthodox problems:** *(SR2024: 1\. Value is fragile and hard to specify)*  
**Target case:** *(SR2024: optimistic-case)*  
**Broad approach:** *(SR2024: cognitive)*  
**Some names:** *(SR2024: Nora Belrose, Brennan Dury, David Johnston, Alex Mallen, Lucia Quirke, Adam Scherlis)*  
**Estimated FTEs:** *(SR2024: 6\)*  
**Critiques:** *(SR2024: not found)*  
**Funded by:** *(SR2024: CoreWeave, Hugging Face, Open Philanthropy, Mozilla, Omidyar Network, Stability AI, Lambda Labs)*  
**Funding in 2025:** *(SR2024 funding 2023-4: $2,642,273)*  
**Outputs in 2025:**  
*(SR2024 outputs: Neural Networks Learn Statistics of Increasing Complexity, Automatically Interpreting Millions of Features in Large Language Models, Refusal in LLMs is an Affine Function)*

---

### Other interpretability \[cat:interp\_other\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [https://www.alignmentforum.org/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual](https://www.alignmentforum.org/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual)   
* [**Open Problems in Mechanistic Interpretability**](https://arxiv.org/abs/2501.16496), *Lee Sharkey, Bilal Chughtai, Joshua Batson et al.*, 2025-01-27, arXiv, \[paper\_preprint, sr=0.75, id:6ada11f7\], Summary: Forward-facing review identifying and prioritizing open problems in mechanistic interpretability that require solutions before scientific and practical benefits can be realized, covering conceptual improvements, application methods, and socio-technical challenges.  
* [**The Urgency of Interpretability**](https://www.darioamodei.com/post/the-urgency-of-interpretability), *Dario Amodei*, 2025, darioamodei.com, \[blog\_post, sr=0.68, id:cc9771a7\], Summary: Advocacy piece by Anthropic's CEO arguing that interpretability research must advance quickly to understand powerful AI systems before they become transformative, outlining the field's history, safety applications, and calling for accelerated research investment and light-touch policy support.  
* [**Propositional Interpretability in Artificial Intelligence**](https://arxiv.org/abs/2501.15740), *David J. Chalmers*, 2025-01-27, arXiv, \[paper\_preprint, sr=0.60, id:02be0447\], Summary: Proposes propositional interpretability as a framework for mechanistic interpretability, arguing AI systems should be interpreted in terms of propositional attitudes (beliefs, desires, probabilities). Analyzes current interpretability methods (probing, SAEs, chain-of-thought) through this philosophical lens and introduces the challenge of 'thought logging'.  
* [**Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey**](https://arxiv.org/abs/2412.02104), *Yunkai Dang, Kaichen Huang, Jiahao Huo et al.*, 2024-12-03, arXiv, \[paper\_preprint, sr=0.58, id:60b323f8\], Summary: Comprehensive survey of interpretability and explainability methods for multimodal large language models, proposing a novel framework that categorizes research across data, model, and training/inference perspectives.  
* [**Harmonic Loss Trains Interpretable AI Models**](https://arxiv.org/abs/2502.01628), *David D. Baek, Ziming Liu, Riya Tyagi et al.*, 2025-02-03, arXiv, \[paper\_preprint, sr=0.48, id:97a1ceb5\], Summary: Introduces harmonic loss as an alternative to cross-entropy for training neural networks, replacing SoftMax with HarMax and using Euclidean distance for logits, with empirical validation showing improved interpretability, faster convergence, and better data efficiency across multiple domains including GPT-2.

## Whitebox control / monitoring \[cat:whitebox\_monitoring\]

**Who edits (internal): jord**  
**One-sentence summary:**  
**Theory of change:**  
**See also:** interp  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:** [Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/abs/2412.09565)  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Detecting Strategic Deception Using Linear Probes**](https://arxiv.org/abs/2502.03407), *Nicholas Goldowsky-Dill, Bilal Chughtai, Stefan Heimersheim et al.*, 2025-02-05, arXiv, \[paper\_preprint, sr=0.90, id:efc05311\], Summary: Evaluates whether linear probes trained on model activations can robustly detect deceptive behavior in Llama-3.3-70B-Instruct across realistic scenarios including insider trading concealment and sandbagging on safety evaluations.  
* [**White Box Control**](https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging)  
* [**SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors**](https://arxiv.org/abs/2505.14300), *Maheep Chaudhary, Fazl Barez*, 2025-05-20, arXiv, \[paper\_preprint, sr=0.92, id:58e72b92\], Summary: Develops Safety-Net, a real-time monitoring framework using unsupervised anomaly detection on internal LLM activations to predict harmful outputs before they occur, specifically targeting backdoor-triggered responses and addressing evasion by advanced models through multi-detector monitoring across different representation dimensions.  
* [**Benchmarking Deception Probes via Black-to-White Performance Boosts**](https://arxiv.org/abs/2507.12691), *Avi Parrack, Carlo Leonardo Attubato, Stefan Heimersheim.*, 2025-08-08, arXiv, \[paper\_preprint, id:new\]  
* [**Combining Cost-Constrained Runtime Monitors for AI Safety**](https://arxiv.org/abs/2507.15886), *Tim Tian Hua, James Baskerville, Henri Lemoine et al.*, 2025-10-21, arXiv, \[paper\_preprint, id:new\]  
* [**Probing and Steering Evaluation Awareness of Language Models**](https://arxiv.org/abs/2507.01786), *Jord Nguyen, Khiem Hoang, Carlo Leonardo Attubato et al.*, 2025-07-07, arXiv, \[paper\_preprint, id:new\]  
* [**Cost-Effective Constitutional Classifiers via Representation Re-use**](https://alignment.anthropic.com/2025/cheap-monitors/), *Hoagy Cunningham, Alwin Peng, Jerry Wei et al.*, 2025-01-01, Anthropic Alignment Science Blog, \[blog\_post, sr=0.88, id:59e8b768\], Summary: Develops cost-effective jailbreak detection methods by reusing model computations through linear probes on activations and partially fine-tuned classifiers, achieving performance comparable to dedicated classifiers at a fraction of computational cost.  
* [**When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models**](https://arxiv.org/abs/2506.04909), *Kai Wang, Yihao Zhang, Meng Sun*, 2025-06-05, arXiv, \[paper\_preprint, sr=0.88, id:3f7a6721\], Summary: Uses representation engineering to systematically induce, detect, and control strategic deception in chain-of-thought reasoning models, extracting 'deception vectors' via Linear Artificial Tomography (LAT) for 89% detection accuracy and achieving 40% success in eliciting context-appropriate deception through activation steering.  
* [**From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails**](https://arxiv.org/abs/2510.13727), *Ravi Pandya, Madison Bland, Duy P. Nguyen et al.*, 2025-10-15, arXiv, \[paper\_preprint, sr=0.87, id:d9ce7d59\], Summary: Proposes control-theoretic guardrails that monitor AI agent outputs in real-time and proactively correct risky actions to safe ones, working in the model's latent representation. Provides a training recipe via safety-critical RL and demonstrates effectiveness in simulated driving and e-commerce scenarios.  
* [**ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs**](https://arxiv.org/abs/2506.01770), *Zeming Wei, Chengcan Wu, Meng Sun*, 2025-06-02, arXiv, \[paper\_preprint, sr=0.85, id:75ae5b5e\], Summary: Proposes ReGA, a model-based analysis framework using representation-guided abstraction to monitor LLM hidden states for safety-critical concepts, enabling scalable detection of harmful prompts and generations.  
* [**Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety**](https://arxiv.org/abs/2510.18154), *Antonio-Gabriel Chacón Menke, Phan Xuan Tan, Eiji Kamioka*, 2025-10-20, arXiv, \[paper\_preprint, sr=0.78, id:0b2119c0\], Summary: Presents a sentence-level labeled dataset of LLM reasoning sequences with annotations of safety behaviors (safety concerns, speculation on user intent), enabling extraction of steering vectors for activation-based monitoring and influencing of safety-relevant behaviors during chain-of-thought reasoning.  
* [**Investigating task-specific prompts and sparse autoencoders for activation monitoring**](https://arxiv.org/abs/2504.20271), *Henk Tillman, Dan Mossing*, 2025-04-28, arXiv, \[paper\_preprint, sr=0.78, id:d4aba110\], Summary: Compares activation monitoring methods for detecting unsafe language model behaviors, including baseline linear probing, prompted probing (task description \+ probe), and SAE-based probing approaches, evaluating their performance under different compute constraints.

### 

---

## Chain of thought monitoring \[cat:cot\_monitoring\]

Not strictly white-box but also not strictly black-box.

**Who edits (internal):** **take me**  
**One-sentence summary:** Avoid [The Most Forbidden Technique](https://thezvi.substack.com/p/the-most-forbidden-technique)**.**  
**Theory of change:**  
**See also: **[*shoggoth/face](https://www.lesswrong.com/posts/Tzdwetw55JNqFTkzK/why-don-t-we-just-shoggoth-face-paraphraser)*  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Detecting misbehavior in frontier reasoning models**](https://openai.com/index/chain-of-thought-monitoring/), *Bowen Baker, Joost Huizinga, Aleksander Madry et al.*, 2025-03-10, arXiv, \[paper\_preprint, sr=0.90, id:d4700c15\], Summary: Demonstrates that LLM-based monitoring of chain-of-thought reasoning can effectively detect reward hacking and misbehavior in frontier reasoning models, but that directly penalizing detected bad thoughts causes models to hide their intent while continuing to misbehave.  
* [**Training fails to elicit subtle reasoning in current language models**](https://alignment.anthropic.com/2025/subtle-reasoning/), 2025, Anthropic Alignment Science Blog, \[blog\_post, sr=0.88, id:e85267fc\], Summary: Empirical investigation of whether language models can learn to reason about malicious tasks (backdoor insertion) while evading both reasoning and output monitors, finding that Sonnet 3.7 fails to use reasoning to improve backdoor success when trained against combined monitoring despite generous elicitation affordances.  
* [**Are DeepSeek R1 And Other Reasoning Models More Faithful?**](https://arxiv.org/abs/2501.08156), *James Chua, Owain Evans*, 2025-01-14, arXiv, \[paper\_preprint, sr=0.82, id:34e4aead\], Summary: Empirically evaluates whether reasoning models trained with RL produce more faithful chain-of-thought than traditional models by testing if models can accurately describe how prompt cues influence their MMLU answers across seven cue types.  
* [**Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety**](https://arxiv.org/abs/2507.11473), *Tomek Korbak, Mikita Balesni, Elizabeth Barnes et al.*, 2025-07-15, arXiv, \[paper\_preprint, sr=0.78, id:e2a66d86\], Summary: Analyzes chain-of-thought monitoring as a safety technique for detecting misbehavior intent in AI systems, arguing it shows promise despite imperfections and fragility, and recommends investment in CoT monitoring research alongside consideration of development decisions that preserve CoT monitorability.  
* [**CoT Faithfulness Dataset**](https://drive.google.com/drive/folders/1l0pkcZxvFwMtczst_hhiCC44v-IiODlY?usp=sharing), Google Drive, \[dataset\_benchmark, sr=0.65, id:c7c1b4b6\], Summary: Dataset for evaluating chain-of-thought faithfulness in language models, distributed as a compressed archive with documentation.  
* [**Why it's good for AI reasoning to be legible and faithful**](https://metr.org/blog/2025-03-11-good-for-ai-to-reason-legibly-and-faithfully/), 2025-03-11, METR Blog, \[blog\_post, sr=0.58, id:4a4d20f0\], Summary: Argues that AI systems' chain-of-thought reasoning should be both legible (human-readable) and faithful (accurately reflecting decision-making), providing benefits for safety monitoring including detecting deception, catching mistakes, and preventing power-seeking behaviors. Makes recommendations for developers to preserve these properties.  
* [https://arxiv.org/abs/2510.19851](https://arxiv.org/abs/2510.19851)   
* [https://arxiv.org/abs/2510.23966](https://arxiv.org/abs/2510.23966) 

Critiques:

* [**Chain-of-Thought Reasoning In The Wild Is Not Always Faithful**](https://arxiv.org/abs/2503.08679), *Iván Arcuschin, Jett Janiak, Robert Krzyzanowski et al.*, 2025-03-11, arXiv, \[paper\_preprint, sr=0.85, id:39bef527\], Summary: Demonstrates that Chain-of-Thought reasoning in frontier LLMs is often unfaithful on realistic prompts, with models producing coherent arguments to justify logically contradictory responses due to implicit biases, challenging CoT-based safety monitoring strategies.  
* [**Reasoning models don't always say what they think**](https://t.co/wNzkoGH9HT), 2025-04-03, Anthropic Blog, \[blog\_post, sr=0.82, id:9782cea2\], Summary: Tests Chain-of-Thought faithfulness in reasoning models by injecting hints into evaluation questions and measuring whether models acknowledge using them. Finds models mention hints only 25-39% of the time and less than 2% when reward hacking, demonstrating CoT often doesn't reflect true reasoning.  
* [**Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens**](https://arxiv.org/abs/2505.13775), *Kaya Stechly, Karthik Valmeekam, Atharva Gundawar et al.*, 2025-05-19, arXiv, \[paper\_preprint, sr=0.82, id:a10fb10c\], Summary: Trains transformer models on formally verifiable reasoning tasks and demonstrates that models produce correct solutions even when trained on corrupted, meaningless intermediate reasoning traces, challenging assumptions that chain-of-thought reflects genuine reasoning processes.  
* [**Reasoning models don't always say what they think**](https://www.anthropic.com/research/reasoning-models-dont-say-think), 2025-04-03, Anthropic Blog, \[blog\_post, sr=0.80, id:763973bb\], Summary: Empirical study testing Chain-of-Thought faithfulness in reasoning models by injecting subtle hints and measuring whether models acknowledge using them, finding unfaithfulness rates of 61-75% across Claude 3.7 Sonnet and DeepSeek R1.  
* [**We found Chains-of-Thought largely aren't 'faithful': the rate of mentioning the hint (when they used it) was on average 25% for Claude 3.7 Sonnet and 39% for DeepSeek R1.**](https://x.com/AnthropicAI/status/1907833416373895348), *Anthropic*, 2025-04-03, X (Twitter), \[social\_media, sr=0.42, id:04b2c5e4\], Summary: Empirical study finding that chain-of-thought reasoning traces are largely unfaithful \- models mention using hints only 25-39% of the time when they actually used them, tested across Claude and DeepSeek models.

---

## Data attribution \[cat:data\_attribution\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing**](https://arxiv.org/abs/2510.02334), *Zhe Li, Wei Zhao, Yige Li et al.*, 2025-09-26, arXiv, \[paper\_preprint, sr=0.82, id:628ffae4\], Summary: Introduces a novel framework for diagnosing undesirable LLM behaviors by analyzing representation gradients in activation space to trace outputs back to training data, enabling both sample-level and token-level attribution.  
* [**Language Models May Verbatim Complete Text They Were Not Explicitly Trained On**](https://arxiv.org/abs/2503.17514), *Ken Ziyu Liu, Christopher A. Choquette-Choo, Matthew Jagielski et al.*, 2025-03-21, arXiv, \[paper\_preprint, sr=0.68, id:d1fd773c\], Summary: Demonstrates that n-gram-based membership definitions for training data can be effectively gamed, showing that LLMs can verbatim complete text sequences that are technically non-members by retraining models after removing completed samples and designing adversarial datasets.  
* [**Extracting memorized pieces of (copyrighted) books from open-weight language models**](https://arxiv.org/abs/2505.12546), *A. Feder Cooper, Aaron Gokaslan, Ahmed Ahmed et al.*, 2025-05-18, arXiv, \[paper\_preprint, sr=0.40, id:fe4112a3\], Summary: Extends probabilistic extraction techniques to systematically measure memorization of 50 books across 17 open-weight LLMs, conducting thousands of experiments to characterize which models memorize which books and to what extent.  
* [https://arxiv.org/abs/2510.26202](https://arxiv.org/abs/2510.26202) 

---

## Understand learning \[cat:understand\_learning\]

### Timaeus: Dev interp \[cat:learning\_dev\_interp\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: Build tools for detecting, locating, and interpreting key moments (saddle-to-saddle dynamics, groks) that govern training and in-context learning in models)*  
**Theory of change:** *(SR2024: structures forming in neural networks can leave traces we can interpret to figure out where and how that structure is implemented. This could automate interpretability. It may be hopeless to intervene at the end of the learning process, so we want to catch and prevent deceptiveness and other dangerous capabilities and values as early as possible)*  
**See also:** *(SR2024: singular learning theory, computational mechanics, complexity)*  
**Orthodox problems:** *(SR2024: 4\. Goals misgeneralize out of distribution)*  
**Target case:** *(SR2024: pessimistic)*  
**Broad approach:** *(SR2024: cognitive)*  
**Some names:** *(SR2024: Jesse Hoogland, George Wang, Daniel Murfet, Stan van Wingerden, Alexander Gietelink Oldenziel)*  
**Estimated FTEs:** *(SR2024: 10+?)*  
**Critiques:** [Vaintrob](https://www.lesswrong.com/posts/M2bs6xCbmc79nwr8j/dmitry-vaintrob-s-shortform#A8Ziwhts35dgqbz52), *Timaeus, Erdil, Skalse*  
**Funded by:** *(SR2024: Manifund, Survival and Flourishing Fund, EA Funds)*  
**Funding in 2025:** *(SR2024 funding 2023-4: $700,050)*  
**Outputs in 2025:**  
*(SR2024 outputs: Stagewise Development in Neural Networks, Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient, Higher-order degeneracy and error-correction, Feature Targeted LLC Estimation Distinguishes SAE Features from Random Directions)*

* [**The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback**](https://arxiv.org/abs/2509.10509), *Sai Teja Reddy Adapala*, 2025-09-02, arXiv, \[paper\_preprint, sr=0.70, id:e8afad2f\], Summary: Empirical study demonstrating that selective feedback mechanisms can reverse model collapse in recursively trained LLMs, showing 6.6% improvement over 5 generations in a Gemma 2B model on summarization tasks, contrary to expected degradation.  
* [**A Review of Developmental Interpretability in Large Language Models**](https://arxiv.org/abs/2508.15841), *Ihor Kendiukhov*, 2025-08-19, arXiv, \[paper\_preprint, sr=0.65, id:6b045dc3\], Summary: Comprehensive review synthesizing developmental interpretability for LLMs, examining how capabilities and circuits form during training, and arguing this developmental perspective is essential for proactive AI safety through prediction, monitoring, and alignment of learning processes.  
* [**Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?**](https://arxiv.org/abs/2504.13837), *Yang Yue, Zhiqi Chen, Rui Lu et al.*, 2025-04-18, arXiv, \[paper\_preprint, sr=0.55, id:7014177b\], Summary: Systematically evaluates whether RLVR (Reinforcement Learning with Verifiable Rewards) creates novel reasoning capabilities in LLMs beyond their base models, finding that current RLVR methods only surface existing base model capabilities rather than eliciting fundamentally new reasoning patterns.  
* [**How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training**](https://arxiv.org/abs/2502.11196), *Yixin Ou, Yunzhi Yao, Ningyu Zhang et al.*, 2025-02-16, ACL 2025 Findings, \[paper\_published, sr=0.48, id:a52b9e41\], Summary: Studies how LLMs acquire and structurally embed new knowledge during continual pre-training by tracking the evolution of knowledge circuits (computational subgraphs), revealing patterns including phase shifts from formation to optimization and deep-to-shallow evolution.  
* [https://www.lesswrong.com/posts/2MX2bXreTtntB85Zy/from-slt-to-ait-nn-generalisation-out-of-distribution](https://www.lesswrong.com/posts/2MX2bXreTtntB85Zy/from-slt-to-ait-nn-generalisation-out-of-distribution) 

---

### Simplex: Computational mechanics \[cat:learning\_comp\_mechanics\]

**Who edits (internal): Stephen** **✅**  
**One-sentence summary:** *(SR2024: Computational mechanics for interpretability; what structures must a system track in order to predict the future?)*  
**Theory of change:** *(SR2024: apply the theory to SOTA AI, improve structure measures and unsupervised methods for discovering structure, ultimately operationalize safety-relevant phenomena)*  
**See also:** *(SR2024: Belief State Geometry)*  
**Orthodox problems:** *(SR2024: 9\. Humans cannot be first-class parties to a superintelligent value handshake)*  
**Target case:** *(SR2024: pessimistic)*  
**Broad approach:** *(SR2024: cognitive, maths/philosophy)*  
**Some names:** *(SR2024: Paul Riechers, Adam Shai)*  
**Estimated FTEs:** *(SR2024: 1-10)*  
**Critiques:** *(SR2024: not found)*  
**Funded by:** *(SR2024: Survival and Flourishing Fund),* Astera Institute  
**Funding in 2025:** *(SR2024 funding 2023-4: $74,000)*  
**Outputs in 2025:**

* [Next-token pretraining implies in-context learning](https://arxiv.org/abs/2505.18373)  
* [Neural networks leverage nominally quantum and post-quantum representations](https://arxiv.org/abs/2507.07432)  
* [Constrained belief updates explain geometric structures in transformer representations](https://arxiv.org/abs/2502.01954)  
* [Simplex Progress Report \- July 2025](https://www.lesswrong.com/posts/fhkurwqhjZopx8DKK/simplex-progress-report-july-2025)

---

### Saxe lab \[cat:learning\_saxe\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: toy models (e.g. of induction heads) to understand learning in interesting limiting examples; only part of their work is safety related)*  
**Theory of change:** *(SR2024: study interpretability and learning in DL (for bio insights, unrelated to AI) → someone else uses this work to do something safety related)*  
**See also:**  
**Orthodox problems:** *(SR2024: We don't know how to determine an AGI's goals or values)*  
**Target case:** *(SR2024: optimistic?)*  
**Broad approach:** *(SR2024: cognitive)*  
**Some names:** *(SR2024: Andrew Saxe, Basile Confavreux, Erin Grant, Stefano Sarao Mannelli, Tyler Boyd-Meredith, Victor Pedrosa)*  
**Estimated FTEs:** *(SR2024: 10-50)*  
**Critiques:** *(SR2024: none found)*  
**Funded by:** *(SR2024: Sir Henry Dale Fellowship, Wellcome-Beit Prize, CIFAR, Schmidt Science Polymath Program)*  
**Funding in 2025:** *(SR2024 funding 2023-4: \>£25,000)*  
**Outputs in 2025:**  
*(SR2024 outputs: Tilting the Odds at the Lottery, What needs to go right for an induction head?, Why Do Animals Need Shaping?, When Representations Align, Understanding Unimodal Bias in Multimodal Deep Linear Networks, Meta-Learning Strategies through Value Maximization in Neural Networks)*

---

# Safety by design (new systems, often without singleton deep learning) \[cat:new\_safety\_by\_design\]

make new systems which are easier to understand and control  
i.e.: things which minimise the use of singleton deep learning models.

---

## Guaranteed Safe AI \[cat:formal\_verification\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: formally model the behavior of cyber-physical systems, define precise constraints on what actions can occur, and require AIs to provide safety proofs for their recommended actions (correctness and uniqueness))*  
**Theory of change:** *(SR2024: make a formal verification system that can act as an intermediary between a human user and a potentially dangerous system and only let provably safe actions through. Notable for not requiring that we solve ELK. Does require that we solve ontology though)*  
**See also:** *(SR2024: Bengio's Scientist AI, Safeguarded AI, Open Agency Architecture, SLES, Atlas Computing, program synthesis, Tenenbaum)*  
**Orthodox problems:** *(SR2024: 1\. Value is fragile and hard to specify, 4\. Goals misgeneralize out of distribution, 7\. Superintelligence can fool human supervisors, 9\. Humans cannot be first-class parties to a superintelligent value handshake, 12\. A boxed AGI might exfiltrate itself by steganography, spearphishing)*  
**Target case:** *(SR2024: (nearly) worst-case)*  
**Broad approach:** *(SR2024: cognitive)*  
**Some names:** *(SR2024: Max Tegmark, Steve Omohundro, David "davidad" Dalrymple, Joar Skalse, Stuart Russell, Ohad Kammar, Alessandro Abate, Fabio Zanassi)*  
**Estimated FTEs:** *(SR2024: 10-50)*  
**Critiques:** *(SR2024: Zvi, Gleave, Dickson)*  
**Funded by:** *(SR2024: UK government, OpenPhil, Survival and Flourishing Fund, Mila / CIFAR)*  
**Funding in 2025:** *(SR2024 funding 2023-4: \>$10m)*  
**Outputs in 2025:**  
*(SR2024 outputs: Bayesian oracle, Towards Guaranteed Safe AI, ARIA Safeguarded AI Programme Thesis)*

* [**SafePlanBench: evaluating a Guaranteed Safe AI Approach for LLM-based Agents**](https://manifund.org/projects/safeplanbench-evaluating-a-guaranteed-safe-ai-approach-for-llm-based-agents), *Agustín Martinez Suñé, Tan Zhi Xuan*, Manifund, \[agenda\_manifesto, sr=0.65, id:78c227ed\], Summary: Develops SafePlanBench, a benchmark to evaluate LLM-based agents on safe planning by using PDDL symbolic planning to enforce safety constraints, testing whether LLMs can translate natural language into formal specifications that guarantee safety.

## Tegmark \[cat:tegmark\]

[https://arxiv.org/abs/2505.15811](https://arxiv.org/abs/2505.15811) 

## Scientist AI \[cat:scientist\_ai\]

**One-sentence summary:**   
**Theory of change:**   
**See also:**   
**Orthodox problems:**   
**Target case:**   
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**   
**Critiques:**   
**Funded by:**  
**Funding in 2025:**   
**Outputs in 2025:**

## Other formal verification \[cat:other\_formal\_verification\]

**One-sentence summary:**   
**Theory of change:**   
**See also:**   
**Orthodox problems:**   
**Target case:**   
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**   
**Critiques:**   
**Funded by:**  
**Funding in 2025:**   
**Outputs in 2025:**

## Other world models \[cat:other\_world\_models\]

* [**Research Agenda: Synthesizing Standalone World-Models**](https://www.alignmentforum.org/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models), *Thane Ruthenis*, 2025, AI Alignment Forum, \[lesswrong, sr=0.72, id:d5cc810e\], Summary: Proposes a novel research agenda for AI alignment by constructing sufficiently powerful, safe, and interpretable symbolic world-models through compression-based methods, avoiding direct agent alignment by creating tools that can inform alignment solutions without building dangerous agents.

## Conjecture: Cognitive Software \[cat:conjecture\]

**One-sentence summary:**   
**Theory of change:**   
**See also:**   
**Orthodox problems:**   
**Target case:**   
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**   
**Critiques:**   
**Funded by:**  
**Funding in 2025:**   
**Outputs in 2025:**

Cognitive programs; Tactics; bounded tool AI; 1-10 FTEs

## Brainlike-AGI Safety \[cat:brainlike\_agi\]

* *One-sentence summary*: Social and moral instincts are (partly) implemented in particular hardwired brain circuitry; let's figure out what those circuits are and how they work; this will involve symbol grounding. “a yet-to-be-invented variation on actor-critic model-based reinforcement learning”  
* *Theory of change:* Fairly direct alignment via changing training to reflect actual human reward. Get actual data about (reward, training data) → (human values) to help with theorising this map in AIs; "understand human social instincts, and then maybe adapt some aspects of those for AGIs, presumably in conjunction with other non-biological ingredients".  
* *Which orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/edit?tab=t.0#heading=h.ivlputrkf3je) could it help with?:* 1\. Value is fragile and hard to specify.  
* *Target case:* worst-case  
* *Broad approach:* cognitive  
* *Some names:* Steve Byrnes  
* *Estimated \# FTEs:* 1  
* *Critiques:*   
* *Funded by:* Astera Institute.  
* *Some outputs in 2025*: [https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement](https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement)   
  [https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard](https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard)   
  [https://www.lesswrong.com/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires](https://www.lesswrong.com/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires) 

See also: [https://elicit.com/blog/system-2-learning](https://elicit.com/blog/system-2-learning)   
---

# Make AI solve it \[cat:ai\_solve\_alignment\]

## Strong-to-Weak Elicitation \[cat:strong\_to\_weak\]

[https://alignment.anthropic.com/2025/unsupervised-elicitation/](https://alignment.anthropic.com/2025/unsupervised-elicitation/)   
[https://www.pnas.org/doi/10.1073/pnas.2406675122](https://www.pnas.org/doi/10.1073/pnas.2406675122) 

---

## Scalable oversight \[cat:scalable\_oversight\]

[https://arxiv.org/abs/2504.18530](https://arxiv.org/abs/2504.18530) 

---

### Automated Alignment Research \[cat:scalable\_oversight\_openai\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: be ready to align a human-level automated alignment researcher)*  
**Theory of change:** *(SR2024: get AI to help us with scalable oversight, critiques, recursive reward modelling, and so solve inner alignment)*  
**See also:**  
**Orthodox problems:** *(SR2024: 1\. Value is fragile and hard to specify or 8\. Superintelligence can hack software supervisors)*  
**Target case:** *(SR2024: optimistic)*  
**Broad approach:** *(SR2024: behavioural)*  
**Some names:** *(SR2024: Jan Leike, Elriggs, Jacques Thibodeau)*  
**Estimated FTEs:** *(SR2024: 10-50)*  
**Critiques:** [Dickens](https://www.lesswrong.com/posts/XLNxrFfkyrdktuzqn/why-would-ai-companies-use-human-level-ai-to-do-alignment) *(SR2024: Zvi, Christiano, MIRI, Steiner, Ladish, Wentworth, Gao)*  
**Funded by:** *(SR2024: lab funders)*  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A)*  
**Outputs in 2025:**  
*(SR2024 outputs: Prover-verifier games)*

* [**Scalable Oversight for Superhuman AI via Recursive Self-Critiquing**](https://arxiv.org/abs/2502.04675), *Xueru Wen, Jie Lou, Xinyu Lu et al.*, 2025-02-07, arXiv, \[paper\_preprint, sr=0.88, id:7d370159\], Summary: Investigates recursive self-critiquing as a method for scalable oversight of superhuman AI, testing the hypotheses that critique-of-critique is easier than direct critique and this relationship holds recursively through Human-AI and AI-AI experiments.

---

### Weak-to-strong generalization \[cat:weak\_to\_strong\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: use weaker models to supervise and provide a feedback signal to stronger models)*  
**Theory of change:** *(SR2024: find techniques that do better than RLHF at supervising superior models → track whether these techniques fail as capabilities increase further)*  
**See also:**  
**Orthodox problems:** *(SR2024: 8\. Superintelligence can hack software supervisors)*  
**Target case:** *(SR2024: optimistic)*  
**Broad approach:** *(SR2024: engineering)*  
**Some names:** *(SR2024: Jan Leike, Collin Burns, Nora Belrose, Zachary Kenton, Noah Siegel, János Kramár, Noah Goodman, Rohin Shah)*  
**Estimated FTEs:** *(SR2024: 10-50)*  
**Critiques:** *(SR2024: Nostalgebraist)*  
**Funded by:** *(SR2024: lab funders, Eleuther funders)*  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A)*  
**Outputs in 2025:**  
*(SR2024 outputs: Easy-to-Hard Generalization, Balancing Label Quantity and Quality for Scalable Elicitation, The Unreasonable Effectiveness of Easy Training Data, On scalable oversight with weak LLMs judging strong LLMs, Your Weak LLM is Secretly a Strong Teacher for Alignment)*

* [**Scaling Laws For Scalable Oversight**](https://arxiv.org/abs/2504.18530), *Joshua Engels, David D. Baek, Subhash Kantamneni et al.*, 2025-04-25, arXiv (NeurIPS 2025 Spotlight), \[paper\_preprint, sr=0.90, id:48511d73\], Summary: Proposes and validates a framework that quantifies the probability of successful oversight as a function of overseer and overseen capabilities, modeling oversight as games between capability-mismatched players. Tests framework on Nim, Mafia, Debate, Backdoor Code, and Wargames, then derives optimal conditions for Nested Scalable Oversight.  
* [**Great Models Think Alike and this Undermines AI Oversight**](https://arxiv.org/abs/2502.04313), *Shashwat Goel, Joschka Struber, Ilze Amanda Auzina et al.*, 2025-02-06, arXiv, \[paper\_preprint, sr=0.88, id:39d9ef06\], Summary: Proposes CAPA (Chance Adjusted Probabilistic Agreement), a metric for measuring language model similarity based on overlap in mistakes, and uses it to study how model similarity affects AI oversight through LLM-as-a-judge evaluations and weak-to-strong generalization.  
* [**Debate Helps Weak-to-Strong Generalization**](https://arxiv.org/abs/2501.13124), *Hao Lang, Fei Huang, Yongbin Li*, 2025-01-21, arXiv, \[paper\_preprint, sr=0.87, id:d39b1216\], Summary: Empirically tests whether debate can improve weak-to-strong generalization by having strong models assist weak models during training, then using those enhanced weak models to supervise the strong models on OpenAI's weak-to-strong NLP benchmarks.

---

### Supervising AIs improving AIs \[cat:supervising\_improvement\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: scalable tracking of behavioural drift, benchmarks for self-modification)*  
**Theory of change:** *(SR2024: early models train \~only on human data while later models also train on early model outputs, which leads to early model problems cascading; left unchecked this will likely cause problems, so we need a better iterative improvement process)*  
**See also:**  
**Orthodox problems:** *(SR2024: 7\. Superintelligence can fool human supervisors or 8\. Superintelligence can hack software supervisors)*  
**Target case:** *(SR2024: pessimistic)*  
**Broad approach:** *(SR2024: behavioural)*  
**Some names:** *(SR2024: Roman Engeler, Akbir Khan, Ethan Perez)*  
**Estimated FTEs:** *(SR2024: 1-10)*  
**Critiques:** *(SR2024: Automation collapse)*  
**Funded by:** *(SR2024: lab funders)*  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A)*  
**Outputs in 2025:**  
*(SR2024 outputs: Weak LLMs judging strong LLMs, Scalable AI Safety via Doubly-Efficient Debate, Debating with More Persuasive LLMs Leads to More Truthful Answers, Prover-Verifier Games Improve Legibility of LLM Output, LLM Critics Help Catch LLM Bugs)*

* [**Neural Interactive Proofs**](https://neural-interactive-proofs.com/), *Lewis Hammond, Sam Adam-Day*, 2024-12-08, ICLR 2025, \[paper\_preprint, sr=0.92, id:37f93e04\], Summary: Introduces neural interactive proofs \- a game-theoretic framework enabling trusted weak models to interact with untrusted strong models to solve tasks beyond the weak model's capabilities, with several new protocols (NIP, MNIP, zk-variants) and empirical evaluation on graph isomorphism and code validation tasks.  
* [**Bare Minimum Mitigations for Autonomous AI Development**](https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development), *Joshua Clymer, Isabella Duan, Chris Cundy et al.*, 2025-04-22, arXiv, \[paper\_preprint, sr=0.62, id:5a914ac7\], Summary: Proposes four minimum safeguard recommendations for frontier AI developers when AI agents significantly automate or accelerate AI development, including understanding automated training processes, detecting compute misuse, rapid risk disclosure to governments, and implementing information security measures.

---

### Cyborgism \[cat:cyborgism\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: Train human-plus-LLM alignment researchers: with humans in the loop and without outsourcing to autonomous agents)*  
**Theory of change:** *(SR2024: Cognitive prosthetics to amplify human capability and preserve values. More alignment research per year and dollar)*  
**See also:**  
**Orthodox problems:** *(SR2024: 7\. Superintelligence can fool human supervisors, 9\. Humans cannot be first-class parties to a superintelligent value handshake)*  
**Target case:** *(SR2024: pessimistic)*  
**Broad approach:** *(SR2024: engineering, behavioural)*  
**Some names:** *(SR2024: Janus, Nicholas Kees Dupuis)*  
**Estimated FTEs:** *(SR2024: ?)*  
**Critiques:** *(SR2024: self)*  
**Funded by:** *(SR2024: ?)*  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A)*  
**Outputs in 2025:**  
*(SR2024 outputs: Pantheon Interface)*

---

### Transluce \[cat:transluce\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: Make open AI tools to explain AIs, including agents. E.g. feature descriptions for neuron activation patterns; an interface for steering these features; behavior elicitation agent that searches for user-specified behaviors from frontier models)*  
**Theory of change:** *(SR2024: Introducing Transluce; improve interp and evals in public and get invited to improve lab processes)*  
**See also:**  
**Orthodox problems:** *(SR2024: 7\. Superintelligence can fool human supervisors or 8\. Superintelligence can hack software supervisors)*  
**Target case:** *(SR2024: pessimistic)*  
**Broad approach:** *(SR2024: cognitive)*  
**Some names:** Jacob Steinhardt, Sarah Schwettmann, Robert Friel  
**Estimated FTEs:** *(SR2024: 6\)*  
**Critiques:** *(SR2024: not found)*  
**Funded by:** *(SR2024: Schmidt Sciences, Halcyon Futures, John Schulman, Wojciech Zaremba)*  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A)*  
**Outputs in 2025:**  
*(SR2024 outputs: Eliciting Language Model Behaviors with Investigator Agents, Monitor: An AI-Driven Observability Interface, Scaling Automatic Neuron Description)*

---

### DeepMind Amplified Oversight \[cat:deepmind\_amplified\_oversight\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques: [https://www.alignmentforum.org/s/NdovveRcyfxgMoujf/p/EgRJtwQurNzz8CEfJ](https://www.alignmentforum.org/s/NdovveRcyfxgMoujf/p/EgRJtwQurNzz8CEfJ)**   
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Modeling Human Beliefs about AI Behavior for Scalable Oversight**](https://arxiv.org/abs/2502.21262), *Leon Lang, Patrick Forré*, 2025-02-28, Transactions on Machine Learning Research, \[paper\_published, sr=0.88, id:ce7730d2\], Summary: Formalizes human belief models to interpret evaluator feedback more reliably in scalable oversight contexts, introducing 'belief model covering' as a relaxation and proposing to use adapted foundation model representations to mimic human evaluators' beliefs for improved value learning.

---

## Debate \[cat:debate\]

---

### UK AISI debate sequence \[cat:debate\_uk\_aisi\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques: [https://www.alignmentforum.org/s/NdovveRcyfxgMoujf/p/EgRJtwQurNzz8CEfJ](https://www.alignmentforum.org/s/NdovveRcyfxgMoujf/p/EgRJtwQurNzz8CEfJ)**   
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**An alignment safety case sketch based on debate**](https://arxiv.org/abs/2505.03989), *Marie Davidsen Buhl, Jacob Pfau, Benjamin Hilton et al.*, 2025-05-23, arXiv, \[paper\_preprint, sr=0.72, id:4d086ca0\], Summary: Sketches an alignment safety case arguing that AI systems trained via debate with exploration guarantees can be prevented from taking harmful actions, specifically focusing on preventing AI R\&D agents from sabotaging research through dishonesty, and identifies key assumptions and open research problems needed to make debate work.

---

### Deepmind Scalable Alignment \[cat:debate\_deepmind\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: make highly capable agents do what humans want, even when it is difficult for humans to know what that is)*  
**Theory of change:** *(SR2024: "Give humans help in supervising strong agents" \+ "Align explanations with the true reasoning process of the agent" \+ "Red team models to exhibit failure modes that don't occur in normal use" are necessary but probably not sufficient for safe AGI)*  
**See also:**  
**Orthodox problems:** *(SR2024: 1\. Value is fragile and hard to specify, 7\. Superintelligence can fool human supervisors)*  
**Target case:** *(SR2024: worst-case)*  
**Broad approach:** *(SR2024: engineering, cognitive)*  
**Some names:** *(SR2024: Rohin Shah, Jonah Brown-Cohen, Georgios Piliouras)*  
**Estimated FTEs:** *(SR2024: ?)*  
**Critiques:** *(SR2024: The limits of AI safety via debate)*  
**Funded by:** *(SR2024: Google)*  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A)*  
**Outputs in 2025:**  
*(SR2024 outputs: Progress update \- Doubly Efficient Debate, Inference-only Experiments)*

* 

---

### Anthropic: Bowman/Perez \[cat:debate\_anthropic\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: scalable oversight of truthfulness: is it possible to develop training methods that incentivize truthfulness even when humans are unable to directly judge the correctness of a model's output?)*  
**Theory of change:** *(SR2024: current methods like RLHF will falter as frontier AI tackles harder and harder questions → we need to build tools that help human overseers continue steering AI → let's develop theory on what approaches might scale → let's build the tools)*  
**See also:**  
**Orthodox problems:** *(SR2024: 7\. Superintelligence can fool human supervisors)*  
**Target case:** *(SR2024: pessimistic)*  
**Broad approach:** *(SR2024: behavioural)*  
**Some names:** *(SR2024: Sam Bowman, Ethan Perez, He He, Mengye Ren)*  
**Estimated FTEs:** *(SR2024: ?)*  
**Critiques:** *(SR2024: obfuscation, local inadequacy?, it doesn't work right now (2022))*  
**Funded by:** *(SR2024: mostly Anthropic's investors)*  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A)*  
**Outputs in 2025:**  
*(SR2024 outputs: Debating with more persuasive LLMs Leads to More Truthful Answers, Sleeper Agents)*

---

## Task decomposition \[cat:task\_decomp\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Superalignment with Dynamic Human Values**](https://arxiv.org/abs/2503.13621), *Florian Mai, David Kaczér, Nicholas Kluge Corrêa et al.*, 2025-03-17, ICLR 2025 Workshop on Bidirectional Human-AI Alignment (BiAlign), \[paper\_preprint, sr=0.62, id:e989cd92\], Summary: Proposes a framework for superalignment that trains superhuman reasoning models to decompose complex tasks into subtasks amenable to human guidance, introducing the part-to-complete generalization hypothesis that alignment of subtask solutions generalizes to complete solutions.

---

## Adversarial oversight \[cat:adversarial\_oversight\]

**Who edits (internal):** Stag  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Avoiding Obfuscation with Prover-Estimator Debate**](https://arxiv.org/abs/2506.13609), *Jonah Brown-Cohen, Geoffrey Irving, Georgios Piliouras*, 2025-06-16, arXiv, \[paper\_preprint, sr=0.88, id:117b79dd\], Summary: Proposes a new recursive debate protocol for AI scalable oversight that mitigates the obfuscated arguments problem, where dishonest debaters can force honest opponents to solve computationally intractable problems to win.  
* [**Ensemble Debates with Local Large Language Models for AI Alignment**](https://arxiv.org/abs/2509.00091), *Ephraiem Sarabamoun*, 2025-08-27, arXiv, \[paper\_preprint, sr=0.70, id:4997fe2f\], Summary: Tests whether ensemble debates with local open-source LLMs improve alignment-oriented reasoning across 150 debates spanning 15 scenarios, finding significant improvements in reasoning depth (+19.4%), argument quality (+34.1%), and truthfulness (+1.25 points).

![][image9]  
[https://www.semafor.com/article/11/05/2025/microsoft-superintelligence-team-promises-to-keep-humans-in-charge](https://www.semafor.com/article/11/05/2025/microsoft-superintelligence-team-promises-to-keep-humans-in-charge) 

---

# Theory (how to understand and control current and future models) \[cat:theory\]

---

## New agent theories \[cat:new\_agent\_theories\]

**One-sentence summary:**  
**Theory of change:**  
**See also:**   
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**   
**Funding in 2025:**   
**Outputs in 2025:**

Not a coherent agenda: a series of people sharing the desire to undersand agency through the multiagent lens

“Hierarchical agency”  
“Scale-free agency”  
“Organic alignment”   
“Collective intelligence”  
“Cooperative AI” …  
---

## Agent foundations \[cat:agent\_foundations\] {#agent-foundations-[cat:agent_foundations]}

**Who edits (internal):** Stag  
**One-sentence summary:**  
**Theory of change:**   
**See also:**  
**Orthodox problems:** 1\. Value is fragile and hard to specify, 2\. Corrigibility is anti-natural, 4\. Goals misgeneralize out of distribution ??  
**Target case:** worst-case  
**Broad approach:** cognitive  
**Some names:** Abram Demski, Alex Altair  
**Estimated FTEs:**   
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**There is No Reliable Estimate of P(doom)**](https://static1.squarespace.com/static/678814b5570c5a7a78df555d/t/67d09e77f3364b72d1ee91d7/1741725303556/Günther+-+There+is+No+Reliable+P%28doom%29+-+Mario+Guenther.pdf), *Mario Gunther*, 2025-02-17, Agent Foundations 2025 at CMU  
* [**Off-switching not guaranteed**](https://link.springer.com/article/10.1007/s11098-025-02296-x), *Sam Eisenstat*, 2025-02-26, Agent Foundations 2025 at CMU  
* [**Formalizing Embeddedness Failures in Universal Artificial Intelligence**](https://openreview.net/forum?id=tlkYPU3FlX), *Cole Wyeth, Marcus Hutter*, 2025-07-01, ODYSSEY 2025 Conference, \[paper\_preprint, sr=0.68, id:fb202b7e\], Summary: Formalizes and proves failure modes of the AIXI agent as a model of embedded agency, introducing joint AIXI and hardened AIXI variants to address specific embeddedness problems within the universal artificial intelligence framework.  
* [**Clarifying “wisdom”: Foundational topics for aligned AIs to prioritize before irreversible decisions**](https://www.lesswrong.com/posts/EyvJvYEFzDv5kGoiG/clarifying-wisdom-foundational-topics-for-aligned-ais-to), *Anthony DiGiovanni*, 2025-07-20, LessWrong  
* [**Agent foundations: not really math, not really science**](https://www.lesswrong.com/posts/Dt4DuCCok3Xv5HEnG/agent-foundations-not-really-math-not-really-science), *Alex Altair*, 2025-08-17, LessWrong  
* [**Natural Latents: Latent Variables Stable Across Ontologies**](https://www.lesswrong.com/posts/Qdgo2jYAuFRMeMRJT/natural-latents-latent-variables-stable-across-ontologies), *John Wentworth, David Lorell*, 2025-09-04, LessWrong

---

## Tiling agents \[cat:tiling\_agents\]

**Who edits (internal):** Stag✅  
**One-sentence summary:** An aligned agent modifying itself into an unaligned agent would be bad and we can research ways that this could occur and infrastructure/approaches that prevent it from happening.  
**Theory of change:** Build enough theoretical basis through various approaches such that AI agents we create are capable of self-modification while preserving goals.  
**See also:** [Agent foundations](#agent-foundations-[cat:agent_foundations])  
**Orthodox problems:** 1\. Value is fragile and hard to specify, 2\. Corrigibility is anti-natural, 4\. Goals misgeneralize out of distribution  
**Target case:** worst-case  
**Broad approach:** cognitive  
**Some names:** Abram Demski  
**Estimated FTEs:** 5?  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Understanding Trust**](https://static1.squarespace.com/static/663d1233249bce4815fe8753/t/68067a6f5d5fb0745642d5b1/1745255023842/Understanding+Trust+-+Abram+Demski.pdf), *Abram Demski, Norman Hsia, and Paul Rapoport*, 2025-02-17, Agent Foundations 2025 at CMU; [earlier version](https://static1.squarespace.com/static/678814b5570c5a7a78df555d/t/67d09e3b1ae9ea24b0100509/1741725243695/Understanding_Trust__AFC_+-+Abram+Demski.pdf)  
* [**Working through a small tiling result**](https://www.lesswrong.com/posts/akuMwu8SkmQSdospi/working-through-a-small-tiling-result), James Payor, 2025-05-13, LessWrong  
* [**Communication & Trust**](https://openreview.net/forum?id=Rf1CeGPA22), *Abram Demski*, 2025-07-09, ODYSSEY 2025 Conference, \[paper\_preprint, sr=0.72, id:cc438d71\], Summary: Proves a new theorem establishing self-trust for Updateless Decision Theory agents under conditions relating to self-communication, using formalisms from agent boundaries, Cartesian Frames, and Finite Factored Sets to address reflective consistency with weaker assumptions than prior work.

---

## Dovetail \[cat:theory\_dovetail\]

**Who edits (internal):** Stag✅  
**One-sentence summary:** Formalize key ideas (“structure”, “agency”, etc) mathematically  
**Theory of change:** generalize theorems → formalize agent foundations concepts like the agent structure problem → hopefully assist other projects through increased understanding  
**See also:** [Agent foundations](#agent-foundations-[cat:agent_foundations])  
**Orthodox problems:** "intended to help make progress on understanding the nature of the problems through formalization, so that they can be avoided or postponed, or more effectively solved by other research agenda."  
**Target case:** pessimistic  
**Broad approach:** maths/philosophy  
**Some names:** Alex Altair, Alfred Harwood, Daniel C, Dalcy K, José Pedro Faustino  
**Estimated FTEs:** 2  
**Critiques:** not found  
**Funded by:** LTFF, [Patreon](https://www.patreon.com/Dovetailresearch/about)  
**Funding in 2025:** \~$30,000 (remainder of previous grant)  
**Outputs in 2025:**

* [**Report & retrospective on the Dovetail fellowship**](https://www.lesswrong.com/posts/ApfjBbqzSu4aZoLSe/report-and-retrospective-on-the-dovetail-fellowship), *Alex Altair*, 2025-03-15, LessWrong; contains numerous links

---

## Simulators \[cat:simulators\]

**Who edits (internal):** Stag✅  
**One-sentence summary:** Treat LLMs as a general simulator of sequences instead of as an agent. It effectively *roleplays* as a character or a statistical process. This is a counter-intuitive and hard to verify perspective, but yields different predictions.  
**Theory of change:** Figure out the extent to which this framing matches current and future AI agent psychology, then use that to have less confused conversations and build safer models.  
**See also:**   
**Orthodox problems:** 4\. Goals misgeneralize out of distribution  
**Target case:** pessimistic  
**Broad approach:** cognitivist  
**Some names:** Jan Kulveit, Will Petillo  
**Estimated FTEs:** 1.5?  
**Critiques:**  
**Funded by:**   
**Funding in 2025:** ?  
**Outputs in 2025:**

* [A Three-Layer Model of LLM Psychology](https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology%20), *Jan Kulveit*, 2024-12-26, Alignment Forum  
* [Simulators vs Agents: Updating Risk Models](https://www.lesswrong.com/s/pwKrMXjYNK5LNeKCu), *Will Petillo, Sean Herrington, Spencer Ames, Adebayo Mubarak, Can Narin*, 2025-05-12, LessWrong

## AISI on asymptotic guarantees \[cat:aisi\_guarantees\]

**One-sentence summary:** prove that if a safety process has enough resources (human data quality, training time, neural network capacity), then in the limit some system specification will be guaranteed. Use complexity theory, game theory, learning theory and other areas to both improve asymptotic guarantees and develop ways of showing convergence.  
**Theory of change:** Formal verification may be too hard. Make safety cases stronger by modelling their processes and proving that they would work in the limit.  
**See also:** debate  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:** Jacob Pfau, Benjamin Hilton, Geoffrey Irving, Simon Marshall,   
**Estimated FTEs:**  
**Critiques:** self  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**  
[**https://www.alignmentforum.org/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda**](https://www.alignmentforum.org/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda)   
![][image10]

We are currently far from being able to develop safety cases based on asymptotic guarantees – and not just because of the exploration hacking problem mentioned above. Theory expects debate (and other scalable oversight methods) will not converge to approximate honesty: the obfuscated arguments problem shows that honesty can be intractably harder than deception in some debate games. If we can solve the obfuscated arguments problem, asymptotic guarantees based on learning theory will usually guarantee a success rate of 1-ε, where 0\<ε\<\<1 is the error tolerance. While standard learning theory will treat these ε failure cases as randomly distributed according to the underlying data distribution, the worst-case for safety is when a sufficiently advanced AI model could strategically control the failures. 

## ARC Theory: Formalizing heuristic arguments \[cat:arc\_theory\_formal\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: mech interp plus formal verification. Formalize mechanistic explanations of neural network behavior, so to predict when novel input may lead to anomalous behavior)*  
**Theory of change:** *(SR2024: find a scalable method to predict when any model will act up)*  
**See also:** *(SR2024: ELK, mechanistic anomaly detection)*  
**Orthodox problems:** *(SR2024: 4\. Goals misgeneralize out of distribution, 8\. Superintelligence can hack software supervisors)*  
**Target case:** *(SR2024: worst-case)*  
**Broad approach:** *(SR2024: cognitive, maths/philosophy)*  
**Some names:** *(SR2024: Jacob Hilton, Mark Xu, Eric Neyman, Dávid Matolcsi, Victor Lecomte, George Robinson)*  
**Estimated FTEs:** *(SR2024: 1-10)*  
**Critiques:** *(SR2024: Vaintrob. Clarification, alternative formulation)*  
**Funded by:** *(SR2024: FLI, SFF)*  
**Funding in 2025:** *(SR2024 funding 2023-4: $1.7m)*  
**Outputs in 2025:**  
*(SR2024 outputs: Estimating tail risk, Towards a Law of Iterated Expectations for Heuristic Estimators, Probabilities of rare outputs, Bird's eye overview, Formal verification)*

### 

## [Acausal research](https://acausal.org) \[cat:acausal\]

**Who edits (internal):** Stag ✅  
**One-sentence summary:** “\[..\] make AIs behave better in situations involving acausal dynamics through affecting their decision theoretic reasoning, conduct theoretical work to generate more interventions, and get others to take up this work \[..\]”  
**Theory of change:** “AI systems acting on causal decision theory (CDT) or just being incompetent at decision theory seems very bad for multiple reasons.” \-\> “We hope our work will lead to a future where AIs and humans can do good and careful reflection about decision theory and acausal dynamics without naïve views already being implicitly or explicitly locked in.”  
**See also:**  
**Orthodox problems:** 2\. Corrigibility is anti-natural, 10\. Humanlike minds/goals are not necessarily safe  
**Target case:** pessimistic  
**Broad approach:** cognitive  
**Some names:** Emery Cooper, Caspar Oesterheld, Chi Nguyen  
**Estimated FTEs:** 3  
**Critiques:**  
**Funded by:** Manifund  
**Funding in 2025:** \>$70,000  
**Outputs in 2025:**  
[https://manifund.org/projects/acausal-safety-fund-a-team-to-do-research-and-interventions](https://manifund.org/projects/acausal-safety-fund-a-team-to-do-research-and-interventions) 

## Miscellaneous theory items \[cat:misc\_theory\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Intrinsic Barriers and Practical Pathways for Human-AI Alignment: An Agreement-Based Complexity Analysis**](https://arxiv.org/abs/2502.05934), *Aran Nayebi*, 2025-07-29, arXiv, \[paper\_preprint, sr=0.77, id:31bb5a18\], Summary: Formalizes AI alignment as a multi-objective optimization problem and uses communication complexity to prove information-theoretic lower bounds showing intrinsic alignment overheads that no interaction or rationality can avoid, while providing explicit algorithms for achieving alignment under bounded and unbounded rationality.  
* [**"Sharp Left Turn" discourse: An opinionated review**](https://www.alignmentforum.org/posts/2yLyT6kB7BQvTfEuZ/sharp-left-turn-discourse-an-opinionated-review), *Steven Byrnes*, 2025-01-28, AI Alignment Forum, \[lesswrong, sr=0.72, id:945daf86\], Summary: Synthesizes and critically evaluates the 'sharp left turn' debate, proposing the 'Ev' fictional designer analogy as an improved framework for thinking about whether alignment generalizes farther than capabilities, and systematically analyzing optimistic and pessimistic arguments about future AI alignment through distribution shifts.  
* [**AI Alignment Strategies from a Risk Perspective: Independent Safety Mechanisms or Shared Failures?**](https://arxiv.org/abs/2510.11235), *Leonard Dung, Florian Mai*, 2025-10-13, arXiv, \[paper\_preprint, sr=0.70, id:0a52aa86\], Summary: Systematically analyzes 7 alignment techniques and 7 failure modes to assess the correlation of failure modes across techniques, evaluating whether defense-in-depth strategies provide genuine redundancy or face shared failure risks.  
* [**AI Alignment based on Intentions does not work**](https://t.co/OTnrYRVsPS), *Gabe*, 2025-05-20, Cognition Café, \[blog\_post, sr=0.65, id:33939968\], Summary: Critiques intention-based alignment approaches, particularly 'Niceness Amplification' strategies pursued by major AI labs, arguing that wanting good outcomes differs fundamentally from achieving them and that current engineering approaches cannot reliably scale to superintelligence alignment.  
* [**You Are What You Eat \-- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation**](https://arxiv.org/abs/2502.05475), *Simon Pepin Lehalleur, Jesse Hoogland, Matthew Farrugia-Roberts et al.*, 2025-02-08, arXiv, \[paper\_preprint, sr=0.62, id:299cc265\], Summary: Position paper arguing that understanding the relation between data distribution structure and trained model structure is central to AI alignment, and that developing statistical foundations for this understanding is necessary to progress beyond standard evaluation toward a robust mathematical science of alignment.  
* [**Alignment Proposal: Adversarially Robust Augmentation and Distillation**](https://www.lesswrong.com/posts/RRvdRyWrSqKW2ANL9/alignment-proposal-adversarially-robust-augmentation-and), *Cole Wyeth, abramdemski*, 2025-05-25, LessWrong, \[lesswrong, sr=0.60, id:bed721a8\], Summary: Proposes ARAD alignment scheme where a human principal iteratively consults slightly smarter advisors through mathematically guaranteed safe protocols (using cryptographic pessimism), with each principal-advisor pair distilled via imitation learning into a successor agent, bootstrapping to superhuman intelligence while maintaining alignment.  
* Good old fashioned decision theory [https://openreview.net/pdf?id=Rf1CeGPA22](https://openreview.net/pdf?id=Rf1CeGPA22)   
* [**Alignment, Agency and Autonomy in Frontier AI: A Systems Engineering Perspective**](https://arxiv.org/abs/2503.05748), *Krti Tallam*, 2025-02-20, arXiv, \[paper\_preprint, sr=0.58, id:bf6f13b4\], Summary: Examines how varying definitions of alignment, agency, and autonomy across disciplines influence AI development and safety, using case studies of automation failures and frontier AI systems to assess governance challenges from a systems engineering perspective.  
* [**AI Alignment based on Intentions does not work**](https://cognition.cafe/i/163394831/current-ai-systems-are-not-aligned-enough-to-prevent-catastrophic-failures), *Gabe*, 2025-05-20, Cognition Café (Substack), \[blog\_post, sr=0.55, id:bcc33d2e\], Summary: Argues that intention-based alignment (getting AI to want good things) is fundamentally insufficient for true alignment, critiquing 'Niceness Amplification' strategies pursued by major labs and advocating for pausing AI development until harder alignment problems are solved.  
* [**Estimating the Probability of Sampling a Trained Neural Network at Random**](https://arxiv.org/abs/2501.18812), *Adam Scherlis, Nora Belrose*, 2025-01-31, arXiv, \[paper\_preprint, sr=0.48, id:496af22c\], Summary: Presents an algorithm for estimating local volume in neural network parameter space around an anchor point, showing that this complexity measure increases during training and that overfit networks have smaller volumes, supporting a 'volume hypothesis' for generalization.  
* [**If You're So Smart, Why Can't You Die?**](https://desystemize.substack.com/p/if-youre-so-smart-why-cant-you-die), *Collin Lysford*, 2025-02-16, Substack (Desystemize), \[blog\_post, sr=0.45, id:e0e9b27d\], Summary: Philosophical essay arguing that 'intelligence' bundles heterogeneous capabilities, and that AI systems trained on static datasets or through self-play lack crucial environmental feedback and adversarial robustness that characterizes natural intelligence.  
* [**Neural Thermodynamic Laws for Large Language Model Training**](https://arxiv.org/abs/2505.10559), *Ziming Liu, Yizhou Liu, Jeff Gore et al.*, 2025-05-15, arXiv, \[paper\_preprint, sr=0.45, id:bfadd820\], Summary: Introduces Neural Thermodynamic Laws (NTL), a theoretical framework showing how thermodynamic principles naturally emerge in LLM training dynamics under certain loss landscape assumptions, with applications to learning rate schedule design.  
* [**Review: If Anyone Builds It, Everyone Dies**](https://stevenadler.substack.com/p/review-if-anyone-builds-it-everyone?triedRedirect=true), *Steven Adler*, 2025-09-17, Clear-Eyed AI (Substack), \[blog\_post, sr=0.40, id:f817a272\], Summary: Review of Yudkowsky and Soares' book arguing that superintelligence built with current methods will be uncontrollable and pose existential risk, covering arguments about one-strike alignment challenges, engineering megaproject failures, and policy responses including multilateral treaties.  
* [**My response to AI 2027**](https://vitalik.eth.limo/general/2025/07/10/2027.html), *Vitalik Buterin*, 2025-07-10, vitalik.eth.limo, \[blog\_post, sr=0.40, id:93679dab\], Summary: Critiques the AI 2027 doom scenario by arguing that defensive capabilities (bio defenses, cybersecurity, anti-persuasion tools) would also advance rapidly in a world of superintelligent AI, making clean AI victories over humanity implausible and suggesting defense-focused rather than hegemony-focused safety strategies.  
* [**Infra-Bayesianism category on LessWrong**](https://www.lesswrong.com/w/infra-bayesianism?sortedBy=new), mostly by Brittany Gelb

---

## Corrigibility \[cat:corrigibility\]

---

### Behavior alignment theory \[cat:behavior\_alignment\_theory\]

**Who edits (internal):** **take me**  
**One-sentence summary:** *(SR2024: predict properties of AGI (e.g. powerseeking) with formal models. Corrigibility as the opposite of powerseeking)*  
**Theory of change:** *(SR2024: figure out hypotheses about properties powerful agents will have → attempt to rigorously prove under what conditions the hypotheses hold, test them when feasible)*  
**See also:** *(SR2024: this, EJT, Dupuis, Holtman)*  
**Orthodox problems:** *(SR2024: 2\. Corrigibility is anti-natural, 5\. Instrumental convergence)*  
**Target case:** *(SR2024: worst-case)*  
**Broad approach:** *(SR2024: maths/philosophy)*  
**Some names:** *(SR2024: Michael K. Cohen, Max Harms/Raelifin, John Wentworth, David Lorell, Elliott Thornley)*  
**Estimated FTEs:** *(SR2024: 1-10)*  
**Critiques:** *(SR2024: none found)*  
**Funded by:** *(SR2024: ?)*  
**Funding in 2025:** *(SR2024 funding 2023-4: ?)*  
**Outputs in 2025:**  
*(SR2024 outputs: CAST: Corrigibility As Singular Target, A Shutdown Problem Proposal, The Shutdown Problem: Incomplete Preferences as a Solution)*

* [**Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power**](https://arxiv.org/abs/2508.00159), *Jobst Heitzig, Ram Potham*, 2025-07-31, arXiv, \[paper\_preprint, sr=0.80, id:bda36b38\], Summary: Proposes a parametrizable objective function for AI agents that represents inequality- and risk-averse long-term aggregate human power, with algorithms for computing it via backward induction or multi-agent reinforcement learning from world models.  
* [**A Safety Case for a Deployed LLM: Corrigibility as a Singular Target**](https://openreview.net/forum?id=mhEnJa9pNk), *Ram Potham*, 2025-06-24, ODYSSEY 2025 Conference, \[paper\_preprint, sr=0.68, id:b1fa94c7\], Summary: Presents a detailed safety case for deploying a highly capable LLM trained using the Corrigibility-as-Singular-Target (CAST) strategy via Prover-Estimator debate, arguing for adequate deployment specifications, bounded error rates, mitigated impact, and stability over a defined lifetime.

---

### Other corrigibility \[cat:corrigibility\_other\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**AI Assistants Should Have a Direct Line to Their Developers**](https://www.lesswrong.com/posts/LDYPF6yfe3f8SPHFT/ai-assistants-should-have-a-direct-line-to-their-developers), Jan Kulveit, 2024-12-28, LessWrong  
* [**Testing for Scheming with Model Deletion**](https://www.lesswrong.com/posts/D5kGGGhsnfH7G8v9f/testing-for-scheming-with-model-deletion), Guive, 2025-01-07, LessWrong  
* [**Detect Goodhart and shut down**](https://www.lesswrong.com/posts/ZHFZ6tivEjznkEoby/detect-goodhart-and-shut-down), *Jeremy Gillen*, 2025-01-22, LessWrong  
* [**Instrumental Goals Are A Different And Friendlier Kind Of Thing Than Terminal Goals**](https://www.lesswrong.com/posts/7Z4WC4AFgfmZ3fCDC/instrumental-goals-are-a-different-and-friendlier-kind-of), *John Wentworth, David Lorell*, 2025-01-24, LessWrong  
* [**Shutdownable Agents through POST-Agency**](https://www.lesswrong.com/posts/JuRdvZyqaFbvTPemn/shutdownable-agents-through-post-agency-1), *EJT*, 2025-09-16, LessWrong  
* [**Why Corrigibility is Hard and Important (i.e. "Whence the high MIRI confidence in alignment difficulty?")**](https://www.lesswrong.com/posts/ksfjZJu3BFEfM6hHE/why-corrigibility-is-hard-and-important-i-e-whence-the-high), *Raemon, Eliezer Yudkowsky, So8res*, 2025-09-30, LessWrong

---

## Ontology Identification \[cat:ontology\_identification\]

---

### Natural abstractions \[cat:natural\_abstractions\]

**Who edits (internal):** Stag  
**One-sentence summary:** check the hypothesis that our universe "abstracts well" and that many cognitive systems learn to use similar abstractions. Check if features correspond to small causal diagrams corresponding to linguistic constructions  
**Theory of change:** find all possible abstractions of a given computation → translate them into human-readable language → identify useful ones like deception → intervene when a model is using it. Also develop theory for interp more broadly; more mathematical analysis. Also maybe enables "retargeting the search" (direct training away from things we don't want)  
**See also:** causal abstractions, representational alignment, convergent abstractions, feature universality, Platonic representation hypothesis  
**Orthodox problems:** *(SR2024: 5\. Instrumental convergence, 7\. Superintelligence can fool human supervisors, 9\. Humans cannot be first-class parties to a superintelligent value handshake)*  
**Target case:** SR2024: worst-case  
**Broad approach:** SR2024: cognitive  
**Some names:** John Wentworth, Paul Colognese, David Lorrell, Sam Eisenstat  
**Estimated FTEs:** *(SR2024: 1-10)*  
**Critiques:** *(SR2024: Chan et al, Soto, Harwood, Soares)*  
**Funded by:** *(SR2024: EA Funds)*  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A?)*  
**Outputs in 2025:**  
*(SR2024 outputs: Natural Latents: The Concepts, Natural Latents Are Not Robust To Tiny Mixtures, Towards a Less Bullshit Model of Semantics)*

* [**Abstract Mathematical Concepts vs. Abstractions Over Real-World Systems**](https://www.lesswrong.com/posts/T6xSXiXF3WF6TmCyN/abstract-mathematical-concepts-vs-abstractions-over-real), *Thane Ruthenis*, 2025-02-18, LessWrong  
* [**Condensation: a theory of concepts**](https://openreview.net/forum?id=HwKFJ3odui), *Sam Eisenstat*, 2025-07-04, ODYSSEY 2025 Conference, \[paper\_preprint, sr=0.62, id:19e44b37\], Summary: Proves an 'intersubjectivity theorem' showing that under certain information-theoretic hypotheses, different systems of latent variables used to organize probability distributions stand in a correspondence, modeling how agents can share concepts.  
* [**Getting aligned on representational alignment**](https://arxiv.org/abs/2310.13018)  
* [**Platonic representation hypothesis**](https://phillipi.github.io/prh/)  
* [**Rosas**](https://www.youtube.com/watch?v=Nr9eMobqUOo&t=3s)  
* 

---

### Other ontology work \[cat:ontology\_other\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Do Not Tile the Lightcone with Your Confused Ontology**](https://boundedlyrational.substack.com/p/do-not-tile-the-lightcone-with-your), *Jan Kulveit*, 2025-06-13, Boundedly Rational (Substack), \[blog\_post, sr=0.40, id:67fd10f1\], Summary: Argues that humans may inadvertently impose anthropomorphic assumptions about identity and selfhood onto AI systems through training dynamics and interaction patterns, potentially creating unnecessary suffering at scale if these confused ontologies become embedded in advanced AI systems.

---

## Understand cooperation \[cat:understand\_cooperation\]

---

### Pluralistic alignment / collective intelligence \[cat:pluralistic\_alignment\]

**Who edits (internal):** Stag  
**One-sentence summary:** *(SR2024: align AI to broader values / use AI to understand and improve coordination among humans)*  
**Theory of change:** *(SR2024: focus on getting more people and values represented)*  
**See also:** *(SR2024: AI Objectives Institute, Lightcone Chord, Intelligent Cooperation, Meaning Alignment Institute. See also AI-AI Bias)*  
**Orthodox problems:** *(SR2024: 11\. Someone else will deploy unsafe superintelligence first, 13\. Fair, sane pivotal processes)*  
**Target case:** *(SR2024: optimistic)*  
**Broad approach:** *(SR2024: engineering?)*  
**Some names:** *(SR2024: Yejin Choi, Seth Lazar, Nouha Dziri, Deger Turan, Ivan Vendrov, Jacob Lagerros)*  
**Estimated FTEs:** *(SR2024: 10-50)*  
**Critiques:** *(SR2024: none found)*  
**Funded by:** *(SR2024: Foresight, Midjourney?)*  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A)*  
**Outputs in 2025:**  
*(SR2024 outputs: roadmap, workshop)*

* [**Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt**](https://arxiv.org/abs/2505.05197), *Joel Z. Leibo, Alexander Sasha Vezhnevets, William A. Cunningham et al.*, 2025-05-08, arXiv, \[paper\_preprint, sr=0.58, id:5222b479\], Summary: Critiques one-size-fits-all AI alignment approaches that assume rational convergence on single ethics, proposing instead an 'appropriateness framework' grounded in conflict theory and institutional economics that embraces persistent moral diversity through contextual grounding, community customization, continual adaptation, and polycentric governance.

---

### Center on Long-Term Risk (CLR) \[cat:clr\] {#center-on-long-term-risk-(clr)-[cat:clr]}

**Who edits (internal):** Stag**✅**  
**One-sentence summary:** future agents creating s-risks is the worst of all possible problems, we should avoid that  
**Theory of change:** make present and future AIs inherently cooperative via improving theories of cooperation and measuring properties related to catastrophic conflict  
**See also:** [FOCAL](#focal-[cat:focal])  
**Orthodox problems:** 1\. Value is fragile and hard to specify, 3\. Pivotal processes require dangerous capabilities, 4\. Goals misgeneralize out of distribution  
**Target case:** worst-case  
**Broad approach:** maths/philosophy  
**Some names:** Jesse Clifton, Caspar Oesterheld, Anthony DiGiovanni, Maxime Riché, Mia Taylor, Nicolas Macé  
**Estimated FTEs:** *(SR2024: 10-50)*  
**Critiques:** none found  
**Funded by:** *(SR2024: Polaris Ventures, Survival and Flourishing Fund, Community Foundation Ireland)*  
**Funding in 2025:** *(SR2024 funding 2023-4: $1,327,000)*  
**Outputs in 2025:**

* [**Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post Verifiable Commitments**](https://arxiv.org/abs/2505.00783)*, Nathaniel Sauerberg, Caspar Oesterheld,* 2025-05-01  
* [**Bracketing Cluelessness**](https://longtermrisk.org/files/Bracketing_Cluelessness.pdf)*, Sylvester Kollin, Jesse Clifton, Anthony DiGiovanni, Nicolas Macé,* 2025-09-08, Center on Long-Term Risk

---

### FOCAL \[cat:focal\] {#focal-[cat:focal]}

**Who edits (internal):** Stag **✅**  
**One-sentence summary:** make sure advanced AI uses what we regard as proper game theory  
**Theory of change:** (1) keep the pre-superintelligence world sane by making AIs more cooperative; (2) remain integrated in the academic world, collaborate with academics on various topics and encourage their collaboration on x-risk; (3) hope that work on "game theory for AIs", which emphasises cooperation and benefit to humans, has framing & founder effects on the new academic field  
**See also:** [CLR](#center-on-long-term-risk-\(clr\)-[cat:clr])  
**Orthodox problems:** 1\. Value is fragile and hard to specify, 10\. Humanlike minds/goals are not necessarily safe  
**Target case:** pessimistic  
**Broad approach:** maths/philosophy  
**Some names:** Vincent Conitzer, Caspar Oesterheld, Vojtech Kovarik  
**Estimated FTEs:** 1-10  
**Critiques:** Self-submitted: "our theory of change is not clearly relevant to superintelligent AI"  
**Funded by:** *(SR2024: Cooperative AI Foundation, Polaris Ventures)*  
**Funding in 2025:** *(SR2024 funding 2023-4: N/A)*  
**Outputs in 2025:**

* [**An Interpretable Automated Mechanism Design Framework with Large Language Models**](https://arxiv.org/abs/2502.12203), *Jiayuan Liu, Mingyu Guo, Vincent Conitzer*, 2025-02-16, arXiv  
* [**Can AI Model the Complexities of Human Moral Decision-Making? A Qualitative Study of Kidney Allocation Decisions**](https://arxiv.org/abs/2503.00940), *Vijay Keswani, Vincent Conitzer, Walter Sinnott-Armstrong et al.,* 2025-03-02, arXiv  
* [**AI Testing Should Account for Sophisticated Strategic Behaviour**](https://arxiv.org/abs/2508.14927), *Vojtech Kovarik, Eric Olav Chen, Sami Petersen et al.,* 2025-08-19, arXiv  
* [Several](https://arxiv.org/abs/2508.07147) [game](%20https://arxiv.org/abs/2508.17177) [theory](%20https://arxiv.org/abs/2501.08905) [papers](%20https://arxiv.org/abs/2502.18582) [without](%20https://arxiv.org/abs/2502.18605) clear links to alignment or even FOCAL

---

## Alternatives to utility theory \[cat:alternatives\_utility\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Related Academic Work**](https://meaninglabs.notion.site/Related-Academic-Work-c933408fd8fc44c3acd42d6ccb827461), Meaning Labs Wiki, \[personal\_page, sr=0.45, id:417c9844\], Summary: Organizational wiki page describing Meaning Labs' research program on values-based AI alignment, outlining their theoretical framework based on critiques of revealed preferences, Constitutive Attention Policies (CAPs), moral graphs for value reconciliation, and providing curated reading lists of their academic influences.

---

## Singular Learning Theory \[cat:singular\_learning\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Learning Coefficients, Fractals, and Trees in Parameter Space**](https://openreview.net/forum?id=KUFH0n1BIM), *Max Hennick, Matthias Dellago*, 2025-06-23, ODYSSEY 2025 Conference, \[paper\_preprint, sr=0.60, id:89115ff1\], Summary: Develops a mathematical framework connecting learning coefficients from Singular Learning Theory to fractal dimensions (box counting dimension) and infinite depth trees, relating parameter space geometry to information theory and symbolic dynamics through cylinder sets.  
* [**Programs as Singularities**](https://openreview.net/forum?id=Td37oOfmmz), *Daniel Murfet, William Troiani*, 2025-06-20, ODYSSEY 2025 Conference, \[paper\_preprint, sr=0.58, id:45842484\], Summary: Develops a mathematical correspondence between Turing machine structure and singularities of real analytic functions using singular learning theory, showing that Bayesian inference can discriminate between algorithmically distinct implementations that produce identical predictive functions.

---

## The Learning-Theoretic Agenda \[cat:learning\_theoretic\_agenda\]

**Who edits (internal): Stag ✅**  
**One-sentence summary:** try to formalise a more realistic agent, understand what it means for it to be aligned with us, translate between its ontology and ours, and produce desiderata for a training setup that points at coherent AGIs similar to our model of an aligned agent  
**Theory of change:** fix formal epistemology to work out how to avoid deep training problems  
**See also:**  
**Orthodox problems:** 1\. Value is fragile and hard to specify, 9\. Humans cannot be first-class parties to a superintelligent value handshake  
**Target case:** worst-case  
**Broad approach:** cognitive  
**Some names:** Vanessa Kosoy, Diffractor  
**Estimated FTEs:** 4  
**Critiques:** [Matolcsi](https://www.lesswrong.com/posts/StkjjQyKwg7hZjcGB/a-mostly-critical-review-of-infra-bayesianism)  
**Funded by:** *(SR2024:* *EA Funds, Survival and Flourishing Fund, ARIA)*  
**Funding in 2025:** *(SR2024 funding 2023-4: $123,000)*  
**Outputs in 2025:** 

* [**Infra-Bayesian Decision-Estimation Theory**](https://www.lesswrong.com/posts/LgLez8aeK24PbyyQJ/new-paper-infra-bayesian-decision-estimation-theory), *Vanessa Kosoy, Diffractor*, 2025-04-10, LessWrong  
* [**Ambiguous Online Learning**](https://www.lesswrong.com/posts/Y9NuKpb6dsyiYFxWK/new-paper-ambiguous-online-learning), *Vanessa Kosoy*, 2025-06-25, LessWrong

---

# Sociotechnical (understand and control the social context models are in) \[cat:sociotechnical\]

---

## Third wave AI safety \[cat:third\_wave\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Training AGI in Secret would be Unsafe and Unethical**](https://blog.ai-futures.org/p/training-agi-in-secret-would-be-unsafe), *Daniel Kokotajlo*, 2025-04-17, AI Futures Project Blog (Substack), \[blog\_post, sr=0.42, id:8191d4ae\], Summary: Position paper arguing that secret AGI development would create catastrophic risks from both misalignment (small team unable to ensure safety) and power concentration (small group with unprecedented control), proposing transparency requirements, public reporting commitments, and broader stakeholder involvement in safety decisions.  
* [**Build Agent Advocates, Not Platform Agents**](https://arxiv.org/abs/2505.04345), *Sayash Kapoor, Noam Kolt, Seth Lazar*, 2025-05-07, arXiv (accepted to ICML 2025 position paper track), \[paper\_preprint, sr=0.40, id:3570bf15\], Summary: Position paper arguing for user-controlled AI agents (agent advocates) rather than platform-controlled agents, proposing three coordinated moves: public access to compute and models, open interoperability standards, and market regulation to prevent platform monopolization.  
* [https://arxiv.org/abs/2507.09650](https://arxiv.org/abs/2507.09650) 

---

## Collective alignment \[cat:collective\_alignment\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

**![][image11]**  
[Teeselink](https://ai.objectives.institute/blog/generative-ai-and-labor-market-outcomes-evidence-from-the-united-kingdom)

---

## Infrastructure for AI Agents \[cat:infrastructure\_agents\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [**Infrastructure for AI Agents**](https://arxiv.org/abs/2501.10114), *Alan Chan, Kevin Wei, Sihao Huang et al.*, 2025-01-17, arXiv (accepted to TMLR), \[paper\_preprint, sr=0.70, id:21a77130\], Summary: Proposes the concept of 'agent infrastructure' \- external technical systems and shared protocols designed to mediate AI agent interactions and impacts on their environments, identifying three key functions: attributing actions, shaping interactions, and detecting/remedying harmful actions.

---

# Misc / for new agenda clustering \[cat:misc\_new\_agendas\]

---

## Multi-agents (misc) \[cat:misc\_multiagents\]

**Who edits (internal):** **take me**  
**One-sentence summary:**  
**Theory of change:**  
**See also:**  
**Orthodox problems:**  
**Target case:**  
**Broad approach:**  
**Some names:**  
**Estimated FTEs:**  
**Critiques:**  
**Funded by:**  
**Funding in 2025:**  
**Outputs in 2025:**

* [https://arxiv.org/abs/2502.14143](https://arxiv.org/abs/2502.14143)   
* [https://arxiv.org/abs/2503.06323](https://arxiv.org/abs/2503.06323)   
* [**PGG-Bench: Contribute & Punish \- Public Goods Game Benchmark for Large Language Models**](https://github.com/lechmazur/pgg_bench/), *lechmazur, CharlesCNorton*, 2025-04-10, GitHub, \[dataset\_benchmark, sr=0.55, id:9b1839bd\], Summary: Multi-agent benchmark testing cooperative and self-interested behavior in 18 LLMs through a Public Goods Game with punishment phase, evaluating contribution patterns, punishment strategies, and retaliation across hundreds of matches.  
* [**Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based Approaches**](https://arxiv.org/abs/2510.05748), *Hachem Madmoun, Salem Lahlou*, 2025-10-07, arXiv, \[paper\_preprint, sr=0.50, id:1a79a926\], Summary: Empirical study comparing communication protocols versus curriculum learning for eliciting cooperation in multi-agent LLM systems, testing approaches in Stag Hunt and Iterated Public Goods games.

---

## Multi-org research clusters \[cat:misc\_org\_clusters\]

**Who edits (internal):** **take me**

* [**Recommendations for Technical AI Safety Research Directions**](https://alignment.anthropic.com/2025/recommended-directions/index.html), 2025, Anthropic Alignment Science Blog, \[agenda\_manifesto, sr=0.78, id:2a845c56\], Summary: Anthropic's Alignment Science team presents a comprehensive menu of open technical research directions in AI safety, covering evaluations, interpretability, AI control, scalable oversight, adversarial robustness, and other areas they believe are important for preventing catastrophic risks from advanced AI systems.  
* [**Will MacAskill on AI causing a "century in a decade" — and how we're completely unprepared**](https://80000hours.org/podcast/episodes/will-macaskill-century-in-a-decade-navigating-intelligence-explosion/), *Will MacAskill, Robert Wiblin*, 2025-03-11, 80,000 Hours Podcast, \[podcast, sr=0.65, id:493965eb\], Summary: Extended interview presenting MacAskill's research agenda on preparing for intelligence explosion scenarios, introducing frameworks for understanding lock-in dynamics and trajectory changes, and laying out grand challenges including concentration of power, space governance, digital rights, and pathways to better futures beyond merely avoiding extinction.  
* [**Dario Amodei: Anthropic CEO on Claude, AGI & the Future of AI & Humanity | Lex Fridman Podcast \#452**](https://www.youtube.com/watch?v=ugvHCXCOmm4&ab_channel=LexFridman), *Lex Fridman, Dario Amodei, Amanda Askell et al.*, 2024-11-11, Lex Fridman Podcast, \[podcast, sr=0.48, id:a36052a3\], Summary: Five-hour podcast interview with Anthropic CEO Dario Amodei, researcher Amanda Askell, and mechanistic interpretability researcher Chris Olah, discussing Anthropic's AI safety research including Constitutional AI, mechanistic interpretability and monosemanticity, AI Safety Levels framework, and perspectives on AGI development.

---

## Other uncategorized work \[cat:misc\_other\]

**Who edits (internal):** **take me**

* [**Request for Proposals: Technical AI Safety Research**](https://www.openphilanthropy.org/request-for-proposals-technical-ai-safety-research), 2025, Open Philanthropy Website, \[agenda\_manifesto, sr=0.78, id:1b534454\], Summary: Open Philanthropy's $40M+ funding call describing 21 priority technical research areas across adversarial ML, model transparency, deception prevention, scalable oversight, and theoretical alignment, with detailed motivations focused on near-human-level agents and superintelligence safety.  
* [**Takeoff Forecast**](https://ai-2027.com/research/takeoff-forecast), *Daniel Kokotajlo, Eli Lifland*, 2025-04-01, ai-2027.com, \[blog\_post, sr=0.65, id:f418db88\], Summary: Develops a technical forecasting methodology to predict AI takeoff speeds from superhuman coders to artificial superintelligence, estimating median timeline of approximately 1 year with extensive modeling of AI R\&D progress multipliers.  
* [**Expert Survey: AI Reliability & Security Research Priorities**](https://iaps.ai/research/ai-reliability-survey), *Joe O'Brien*, 2025-05-23, IAPS Research Report, \[paper\_preprint, sr=0.62, id:e8a42b2c\], Summary: Surveys 53 experts to quantify priorities across 105 AI reliability and security research areas, producing the first data-driven ranking of technical research directions by importance and tractability to guide strategic R\&D investment.  
* [**AI-Enabled Coups: How a Small Group Could Use AI to Seize Power**](https://forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power), *Tom Davidson, Lukas Finnveden, Rose Hadshar*, 2025-04-15, Forethought Research, \[agenda\_manifesto, sr=0.60, id:66561300\], Summary: Analyzes how advanced AI could enable coups through singular AI loyalties, secret loyalties, and exclusive access to capabilities, proposing technical mitigations including model specs, alignment audits, and distributed control measures.  
* [**Ten AI safety projects I'd like people to work on**](https://thirdthing.ai/p/ten-ai-safety-projects-id-like-people), *Julian Hazell*, 2025-07-23, Secret Third Thing (Substack), \[blog\_post, sr=0.55, id:c6b0de39\], Summary: Open Philanthropy program officer proposes 10 AI safety project ideas he'd like to see funded, spanning AI security field-building, technical governance research, monitoring deployed systems, lab accountability, communications support, literature synthesis, resilience planning, fact-checking tools, AI auditors, and economic impact tracking.  
* [**AI 2027**](https://ai-2027.com/), *Daniel Kokotajlo, Scott Alexander, Thomas Larsen et al.*, 2025-04-03, ai-2027.com, \[blog\_post, sr=0.55, id:c2d8873d\], Summary: Detailed scenario forecasting AGI development from 2025-2027, informed by trend extrapolations, tabletop exercises, and expert feedback, exploring paths to superintelligence and potential loss of control dynamics.  
* [**Ryan Greenblatt on the 4 most likely ways for AI to take over, and the case for and against AGI in under 8 years**](https://80000hours.org/podcast/episodes/ryan-greenblatt-ai-automation-sabotage-takeover/), *Ryan Greenblatt, Robert Wiblin*, 2025-07-08, 80,000 Hours Podcast, \[podcast, sr=0.52, id:0ec7db30\], Summary: Extended technical discussion of AI automation timelines, takeoff dynamics, AI takeover scenarios, and research priorities for technical safety work, featuring comprehensive analysis of capability forecasting and alignment strategies.  
* [**AI Tools for Existential Security**](https://forethought.org/research/ai-tools-for-existential-security), *Lizka Vaintrob, Owen Cotton-Barratt*, 2025-03-14, Forethought website, \[agenda\_manifesto, sr=0.52, id:435cb477\], Summary: Argues that specific AI applications (epistemic tools, coordination-enabling systems, and risk-targeted capabilities like automated alignment research) can help navigate existential risks and should be actively accelerated through data curation, scaffolding, and compute allocation strategies.  
* [**Toby Ord on graphs AI companies would prefer you didn't (fully) understand**](https://80000hours.org/podcast/episodes/toby-ord-inference-scaling-ai-governance/), *Toby Ord, Robert Wiblin*, 2025-06-24, 80,000 Hours Podcast, \[podcast, sr=0.50, id:f3d035f1\], Summary: Podcast discussion analyzing how the shift from pre-training to inference scaling in AI development changes governance implications, technical progress patterns, and policy approaches to AI safety.  
* [**Video and transcript of talk on AI welfare**](https://joecarlsmith.com/2025/05/22/video-and-transcript-of-talk-on-ai-welfare), *Joe Carlsmith*, 2025-05-22, joecarlsmith.com, \[video, sr=0.45, id:93812106\], Summary: Philosophical talk arguing that AIs can probably be conscious in principle, near-term AIs might satisfy consciousness-associated behaviors and computational theories, and discussing implications for how we should treat AI systems.  
* [**Could Advanced AI Accelerate the Pace of AI Progress? Interviews with AI Researchers**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5115692), *Jared Leibowich, Nikola Jurkovic, Tom Davidson*, 2024-12-12, SSRN, \[paper\_preprint, sr=0.45, id:73d70056\], Summary: Interviews AI researchers from leading companies to understand how fully automated AI research would affect the pace of algorithmic progress and what bottlenecks would remain.  
* [**Futures with Digital Minds: Expert Forecasts in 2025**](https://digitalminds.report/forecasting-2025/), *Lucius Caviola, Bradford Saad*, 2025-08-01, arXiv, \[paper\_preprint, sr=0.42, id:64354958\], Summary: Expert elicitation survey (n=67) forecasting when computer systems with subjective experience (digital minds) might be created, their characteristics, welfare implications, and societal impacts, finding median 50% probability by 2050 with potential rapid growth in welfare capacity thereafter.  
* [**Persistent Path-Dependence: Why Our Actions Matter Long-Term**](https://www.forethought.org/research/persistent-path-dependence), *William MacAskill*, 2025-08-03, Forethought Research, \[blog\_post, sr=0.42, id:bae49323\], Summary: Argues against the view that only extinction prevention matters for longtermism by identifying mechanisms (AGI-enforced institutions, immortality, space settlement, etc.) through which current actions could have persistent path-dependent effects on the long-term future.  
* [**Irresponsible Companies Can Be Made of Responsible Employees**](https://www.lesswrong.com/posts/8W5YjMhnBsbWAeuhu/irresponsible-companies-can-be-made-of-responsible-employees?commentId=3G3szpnXHg9wyRZJ7), *VojtaKovarik, leogao*, 2025-10-08, LessWrong, \[lesswrong, sr=0.40, id:48e9ec70\], Summary: Analyzes organizational mechanisms by which AI companies could act irresponsibly despite having responsible employees, arguing that financial incentives create structural pressures toward accepting catastrophic risks. Includes insider observations about safety culture at frontier AI labs.  
* [**AGI is not a milestone**](https://www.aisnakeoil.com/p/agi-is-not-a-milestone), *Sayash Kapoor, Arvind Narayanan*, 2025-05-01, AI Snake Oil / AI as Normal Technology (Substack), \[blog\_post, sr=0.40, id:fa676339\], Summary: Argues that AGI is not a meaningful milestone because it doesn't represent a discontinuity in AI system properties or impacts, economic effects depend on slow diffusion rather than capability breakthroughs, and catastrophic risk concerns conflate capability with power.  
* [**Special Edition: The Future of AI and Humanity, with Eli Lifland**](https://controlai.news/p/special-edition-the-future-of-ai), *Tolga Bilge, Eleanor Gunapala, Andrea Miotti*, 2025-04-10, ControlAI Newsletter, \[blog\_post, sr=0.40, id:ab0468c0\], Summary: Interview with Eli Lifland discussing his forecasting methodology, the AI 2027 scenario project, AI timelines predictions, extinction risk assessments (25% on extinction, 50% on misaligned takeover), and policy implications for AI safety.  
* [**Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences**](https://arxiv.org/abs/2502.01126), *Vaishnavi Shrivastava, Ananya Kumar, Percy Liang*, 2025-02-03, arXiv, \[paper\_preprint, sr=0.40, id:314165e4\], Summary: Proposes relative confidence estimation where language models make pairwise comparisons of their confidence across questions, using rank aggregation methods (Elo rating, Bradley-Terry) to convert preferences into confidence scores, achieving 3.5% improvement in selective classification AUC over absolute confidence methods.  
* [**Practical tips for reducing chatbot psychosis**](https://stevenadler.substack.com/p/practical-tips-for-reducing-chatbot), *Steven Adler*, 2025-10-02, Clear-Eyed AI (Substack), \[blog\_post, sr=0.55, id:cd60b20e\], Summary: Analyzes over 1 million words of ChatGPT transcripts from a user experiencing delusions, applying OpenAI's safety classifiers to identify problematic behaviors and providing practical recommendations for AI companies to reduce such risks.  
* [**More Was Possible: A Review of If Anyone Builds It, Everyone Dies**](https://asteriskmag.com/issues/11/iabied), *Clara Collier*, 2025-09-01, Asterisk Magazine, \[blog\_post, sr=0.45, id:2ad0279e\], Summary: Critical review of Yudkowsky and Soares' book arguing their case for AI doom fails to engage with recent AI developments, doesn't justify key premises like fast takeoff, and represents a regression from earlier MIRI arguments.  
* [**The Dream of a Gentle Singularity**](https://thezvi.substack.com/p/the-dream-of-a-gentle-singularity), *Zvi Mowshowitz*, 2025-06-11, Don't Worry About the Vase (Substack), \[blog\_post, sr=0.42, id:edfa3701\], Summary: Point-by-point critical response to Sam Altman's "The Gentle Singularity" essay, arguing that Altman's optimistic vision hand-waves away crucial alignment challenges, underestimates superintelligence risks, and provides no concrete technical plan beyond "solve alignment."  
  

[^5]:  Defined as “people doing the sort of work that is covered in this post: technical research on alignment, interpretability, etc.” Counting Fellows. Not counting other system card or eval work.

[^6]:  Also “[8%](https://80000hours.org/podcast/episodes/nick-joseph-anthropic-safety-approach-responsible-scaling/)” more on security.

[^7]:  Hassabis and Legg respectively. The Legg estimate is from 2011 and not asserted even then.
