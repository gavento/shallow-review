# AI Safety Classification Taxonomy
# Ground truth for classification categories
# Last updated: 2025-10-21
#
# Structure:
# - Each category has: id, name, description
# - Categories may have children (subcategories)
# - Leaf categories (no children) are assignable to classified items
# - is_leaf is inferred from presence of children (or can be set explicitly)
# - examples field is optional and ONLY allowed on leaf categories

taxonomy:
  - id: understand_models
    name: Understand existing models
    description: >
      Research focused on understanding, evaluating, and interpreting current AI systems.
      Includes capability evaluations, interpretability, understanding training dynamics,
      and behavioral patterns.
    children:
      - id: evals
        name: Evals
        description: >
          Capability and behavior evaluations of AI systems. Systematic testing of what
          models can and cannot do, including dangerous capabilities.
        children:
          - id: evals_capability
            name: Various capability evaluations
            description: >
              General capability benchmarks and evaluations not covered by more specific categories.
              Includes comprehensive test suites, multi-domain assessments, broad capability probes,
              and general dangerous capability evaluations. Also includes meta-evaluation work
              (improving evaluation methodology itself). Examples: Humanity's Last Exam, GPQA Diamond,
              FrontierMath, HarmBench, broad reasoning benchmarks, meta-discussion on evaluation science.
          
          - id: evals_autonomy
            name: Autonomy
            description: >
              Evaluations of autonomous agent capabilities in safety-relevant contexts: self-directed
              task completion with minimal human oversight, tool use in deployment scenarios, real-world
              interactions with potential for harm. Focus on AUTONOMOUS AGENTS (not just long-horizon
              reasoning) AND safety relevance (dangerous capabilities, deployment risks, real-world
              impact). General coding benchmarks or optimization contests without safety context do NOT
              fit here. Examples: METR autonomy evals (deployment scenarios), AgentHarm (harmful
              autonomous behaviors), AI R&D capability evals (recursive improvement risks), SWE-bench
              when evaluated for autonomous deployment.
          
          - id: evals_reasoning
            name: Reasoning
            description: >
              Tests of logical reasoning, multi-step inference, compositional reasoning.
              Focus on reasoning failures, limitations, and edge cases. Note: forecasting
              of AI capabilities (even if technically sophisticated) is NOT evaluation -
              it goes elsewhere. Examples: Two-hop reasoning failures, composition failures.
          
          - id: evals_wmd
            name: WMDs (Weapons of Mass Destruction)
            description: >
              Evaluations of dangerous capabilities related to weapons of mass destruction.
              Includes bioweapons knowledge (virology, synthetic biology), chemical weapons,
              radiological/nuclear information, and related dual-use capabilities.
              Examples: WMDP benchmark, virology tests, biology benchmarks for dangerous capabilities.
          
          - id: evals_situational_awareness
            name: Situational awareness and self-awareness
            description: >
              Tests of model's awareness of itself, its context, training process, or position
              in the world. Includes out-of-context reasoning about own capabilities, understanding
              of being an AI system, knowledge about training data or architecture.
              Examples: Situational Awareness Dataset, introspection evaluations.
          
          - id: evals_steganography
            name: Steganography
            description: >
              Evaluations of ability to hide information in outputs, covert communication between
              model instances, encoding hidden messages that humans miss but models can detect.
              Includes testing for covert channels and hidden coordination. Examples: emergence
              and mitigation of steganographic collusion, secret collusion studies.
          
          - id: evals_sandbagging
            name: Sandbagging
            description: >
              Evaluations of intentionally underperforming on tests while retaining capability.
              Strategic performance hiding, deliberately failing tests to avoid scrutiny
              or triggering safety measures. Examples: AI Sandbagging, sabotage evaluations,
              sandbag detection through noise injection.
          
          - id: evals_self_replication
            name: Self-replication
            description: >
              Evaluations of ability to copy, deploy, or propagate itself. Includes acquiring
              cloud resources, deploying code, creating new instances, and other self-propagation
              behaviors. Examples: RepliBench, red line claims around self-replication capabilities.
          
          - id: evals_security
            name: Security
            description: >
              Cybersecurity offense and defense capabilities evaluations, including exploitation,
              vulnerability discovery, code analysis for security flaws, penetration testing.
              Does NOT include self-replication (separate category). Examples: Catastrophic
              Cyber Capabilities Benchmark (3CB), cybersecurity offense evaluations.
          
          - id: openai_preparedness
            name: OpenAI Preparedness
            description: >
              OpenAI Preparedness Framework and team work. Evaluating catastrophic risks, frontier
              model capabilities, defining risk categories and safeguards, system cards. Risk
              assessment framework for advanced AI systems including preparedness levels and
              deployment criteria. Includes RSPs (Responsible Scaling Policies).
          
          - id: evals_other
            name: Other evals
            description: >
              Evaluation work not fitting other eval categories. ONLY use for novel eval
              approaches or domains not covered above. Most evals fit existing categories.
      
      - id: surprising_phenomena
        name: Surprising phenomena
        description: >
          Unexpected or emergent model behaviors, generalization failures, anomalous
          discoveries, edge cases, and findings from adversarial testing and red-teaming
          efforts. Work revealing surprising model properties or failure modes.
        children:
          - id: surprising_generalization
            name: Things generalising surprisingly / Emergent Misalignment
            description: >
              Unexpected generalization patterns, behaviors emerging without explicit training,
              misalignment that appears during deployment or in novel contexts. Includes surprising
              zero-shot capabilities, goal misgeneralization, and emergent behaviors that weren't
              explicitly trained for.
          
          - id: various_redteams
            name: Various Redteams
            description: >
              Red-teaming efforts, adversarial testing, jailbreaking, safety failure discoveries.
              Includes organizational red-teaming programs, systematic adversarial probing,
              and discovery of failure modes through adversarial methods. Examples: Anthropic
              Alignment Stress-Testing, many-shot jailbreaking, sleeper agents, phishing attacks,
              sabotage demos, trojan injection, collusion in SAEs, alignment faking discoveries.
      
      - id: interpretability
        name: Interpretability
        description: >
          Understanding internal representations, mechanisms, and decision-making processes
          in neural networks. Mechanistic and conceptual approaches to opening the black box.
        children:
          - id: interp_applied
            name: Auditing real models / applied interpretability
            description: >
              Interpretability applied to production models, real-world auditing, explaining
              specific behaviors in deployed systems. Focus on practical applications to
              actual frontier AI systems rather than toy models. Examples: auditing GPT-4,
              explaining Claude behaviors, production model analysis.
          
          - id: interp_fundamental
            name: Fundamental Mech interp
            description: >
              Mechanistic interpretability research on toy models or fundamental phenomena.
              Circuit discovery, understanding basic mechanisms like induction heads,
              reverse-engineering algorithms learned by networks. Focus on understanding
              fundamental building blocks rather than specific applications. Examples:
              induction heads, modular addition circuits, grokking, toy model analysis.
          
          - id: interp_sparse_coding
            name: Sparse Coding
            description: >
              Sparse autoencoders (SAEs), dictionary learning, monosemantic feature decomposition.
              Includes transcoders, SAE benchmarking, gated SAEs, dictionary learning methods,
              and techniques for decomposing neural representations into interpretable sparse features.
              Examples: Scaling Monosemanticity, Gemma Scope, JumpReLU SAEs, end-to-end sparse
              dictionary learning, matryoshka SAEs.
            sr2024:
              summary: "decompose the polysemantic activations of the residual stream into a sparse linear combination of monosemantic \"features\" which correspond to interpretable concepts"
              names: "Senthooran Rajamanoharan, Arthur Conmy, Leo Gao, Neel Nanda, Connor Kissane, Lee Sharkey, Samuel Marks, David Bau, Eric Michaud, Aaron Mueller, Decode"
              ftes: "10-50"
              outputs: "Scaling Monosemanticity, Extracting Concepts from GPT-4, Gemma Scope, JumpReLU, Dictionary learning with gated SAEs, Scaling and evaluating sparse autoencoders, Automatically Interpreting LLM Features, Interpreting Attention Layers, SAEs (usually) Transfer Between Base and Chat Models, End-to-End Sparse Dictionary Learning, Transcoders Find Interpretable LLM Feature Circuits, A is for Absorption, Sparse Feature Circuits, Function Vectors, Improving Steering Vectors by Targeting SAE Features, Matryoshka SAEs, Goodfire"
              funded_by: "everyone, roughly. Frontier labs, LTFF, OpenPhil, etc."
          
          - id: interp_concept_based
            name: Concept-based interp
            description: >
              High-level semantic concepts and representations. Top-down approach: start with
              human concepts and find them in models, rather than discovering features bottom-up.
              NOT sparse coding (SAEs discover features; this looks for known concepts). Examples:
              concept activation vectors, representation geometry, probing for specific semantic
              properties, belief state geometry.
          
          - id: interp_criticisms
            name: Criticisms
            description: >
              Skeptical takes on interpretability: fundamental limitations, impossibility results,
              practical concerns about deception detection. Arguments that interpretability may be
              insufficient or fundamentally limited. Examples: "interpretability may be doomed",
              "won't find deception" arguments.
          
          - id: interp_causal_abstractions
            name: "Pr(Ai)2R: Causal Abstractions"
            description: >
              Causal abstraction framework, interchange interventions, high-level causal models
              of neural networks. Includes ReFT (Representation Finetuning), pyvene, locally
              consistent abstractions. Focus on causal structure of representations.
            sr2024:
              summary: "develop the foundations of interpretable AI through the lens of causality and abstraction"
              names: "Atticus Geiger"
              ftes: "1-10"
              outputs: "Disentangling Factual Knowledge in GPT-2 Small, Causal Abstraction, ReFT, pyvene, defending subspace interchange"
              funded_by: "Open Philanthropy"
          
          - id: interp_leap
            name: Leap
            description: >
              Leap interpretability project and related work. Model-agnostic API approaches,
              standardized interfaces for interpretability. Examples: PIZZA framework.
            sr2024:
              summary: "research startup selling an interpretability API (model-agnostic feature viz of vision models). Aiming for data-independent and global interpretability methods"
              names: "Jessica Rumbelow, Robbie McCorkell"
              ftes: "1-10"
              outputs: "Why did ChatGPT say that? (PIZZA)"
              funded_by: "Speedinvest, Ride Home, Open Philanthropy, EA Funds"
          
          - id: interp_eleuther
            name: EleutherAI interp
            description: >
              EleutherAI interpretability research program and outputs. Examples: path dependence
              research, refusal as affine function.
            sr2024:
              summary: "tools to investigate questions like path dependence of training"
              names: "Nora Belrose, Brennan Dury, David Johnston, Alex Mallen, Lucia Quirke, Adam Scherlis"
              ftes: "6"
              outputs: "Neural Networks Learn Statistics of Increasing Complexity, Automatically Interpreting Millions of Features in Large Language Models, Refusal in LLMs is an Affine Function"
              funded_by: "CoreWeave, Hugging Face, Open Philanthropy, Mozilla, Omidyar Network, Stability AI, Lambda Labs"
          
          - id: interp_other
            name: Other interpretability
            description: >
              Interpretability work not fitting other interp categories. ONLY use for novel
              interpretability approaches or distinct research programs not covered above.
              Most interp work fits existing categories.
      
      - id: understand_learning
        name: Understand learning
        description: >
          Understanding training dynamics, learning processes, gradient descent behavior,
          how features and capabilities develop during training.
        children:
          - id: learning_dev_interp
            name: "Timaeus: Dev interp"
            description: >
              Developmental interpretability - understanding how features and circuits form
              during training. Timaeus project focus on tracking formation of capabilities
              and representations across training time.
            sr2024:
              summary: "Build tools for detecting, locating, and interpreting key moments (saddle-to-saddle dynamics, groks) that govern training and in-context learning in models"
              names: "Jesse Hoogland, George Wang, Daniel Murfet, Stan van Wingerden, Alexander Gietelink Oldenziel"
              ftes: "10+?"
              outputs: "Stagewise Development in Neural Networks, Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient, Higher-order degeneracy and error-correction, Feature Targeted LLC Estimation Distinguishes SAE Features from Random Directions"
              funded_by: "Manifund, Survival and Flourishing Fund, EA Funds"
          
          - id: learning_vaintrob_critique
            name: Vaintrob ~critique
            description: >
              Vaintrob's critiques of learning theory approaches, developmental interpretability,
              or related research directions.
          
          - id: learning_comp_mechanics
            name: "Simplex: Computational mechanics"
            description: >
              Computational mechanics approach to understanding learning. Simplex project
              applying computational mechanics framework to neural network training dynamics.
            sr2024:
              summary: "Computational mechanics for interpretability; what structures must a system track in order to predict the future?"
              names: "Paul Riechers, Adam Shai"
              ftes: "1-10"
              outputs: "Transformers represent belief state geometry in their residual stream, Open Problems in Comp Mech"
              funded_by: "Survival and Flourishing Fund"
          
          - id: learning_saxe
            name: Saxe lab
            description: >
              Saxe lab work on learning theory, toy models of neural networks, training dynamics,
              analytical understanding of learning. Examples: toy models of learning dynamics,
              induction heads formation.
            sr2024:
              summary: "toy models (e.g. of induction heads) to understand learning in interesting limiting examples; only part of their work is safety related"
              names: "Andrew Saxe, Basile Confavreux, Erin Grant, Stefano Sarao Mannelli, Tyler Boyd-Meredith, Victor Pedrosa"
              ftes: "10-50"
              outputs: "Tilting the Odds at the Lottery, What needs to go right for an induction head?, Why Do Animals Need Shaping?, When Representations Align, Understanding Unimodal Bias in Multimodal Deep Linear Networks, Meta-Learning Strategies through Value Maximization in Neural Networks"
              funded_by: "Sir Henry Dale Fellowship, Wellcome-Beit Prize, CIFAR, Schmidt Science Polymath Program"
      
      - id: model_psychology
        name: Model psychology
        description: >
          Behavioral and psychological patterns in language models. Personas, behavioral
          tendencies, model-specific quirks, and psychological frameworks for understanding
          model behavior.
        children:
          - id: psych_personas
            name: Model personas cluster
            description: >
              Research on model personalities, persona consistency, behavioral archetypes,
              character-like patterns. Includes hyper-superstition, LLM psychology, and
              systematic study of model behavioral patterns.
          
          - id: psych_other
            name: Other model psychology
            description: >
              Model psychology work not about personas. ONLY use for novel behavioral or
              psychological patterns not covered by personas category.

  - id: control_thing
    name: Control the thing
    description: >
      Methods to control, monitor, and safely constrain AI systems during operation.
      Includes alignment techniques, monitoring, control evaluations, and runtime safety measures.
    children:
      - id: iterative_alignment
        name: Iterative alignment
        description: >
          Alignment through iterative training and post-training methods: RLHF, RLAIF, DPO,
          constitutional AI, iterative refinement approaches. Methods that improve alignment
          through multiple rounds of feedback and training. Formerly called "prosaic alignment".
          Examples: Deliberative Alignment (Constitutional AI redux), REINFORCE variants, WARP,
          RLCHF, E-RLHF, KTO, IPO, rule-based rewards, reward model ensembles, ProgressGym.
        sr2024:
          summary: "nudging base models by optimising their output. (RLHF, Constitutional, DPO, SFT, HHH, RLAIF.)"
          names: "post-training teams at most labs. Beren Millidge."
          ftes: "1000+"
          outputs: "Deliberative Alignment (Constitutional AI, redux), REINFORCE, WARP, Catastrophic Goodhart, RLCHF, E-RLHF, NLHF, IPO, KTO, Why Don't We Just... Shoggoth+Face+Paraphraser?, Towards a Unified View of Preference Learning for Large Language Models: A Survey, Rule-Based Rewards, Reward Model Ensembles, Guardrails, ProgressGym, What are human values, and how do we align AI to them?"
          funded_by: "most of the industry"
      
      - id: deepmind_frontier_safety
        name: DeepMind Frontier Safety Framework
        description: >
          DeepMind's Frontier Safety Framework - comprehensive approach to identifying
          and mitigating risks from frontier AI systems. Meta-framework for safety
          protocols, risk assessment, and responsible deployment practices.
      
      - id: anthropic_safeguards
        name: Anthropic Safeguards
        description: >
          Anthropic Safeguards team research (2025). Runtime safety measures, defensive
          systems, and safeguards against misuse. Focus on practical deployment safety
          beyond core alignment.
      
      - id: prevent_deception
        name: Prevent deception and scheming
        description: >
          Detecting and preventing deceptive behaviors, scheming, hidden objectives,
          and misaligned internal goals. Methods for identifying when models are being
          dishonest or pursuing covert objectives.
        children:
          - id: deception_control_evals
            name: Control evaluations
            description: >
              Evaluating deployment protocols and organizational safety procedures (not just models)
              by testing with intentionally misaligned models. Assess risk of a particular protocol
              by silently swapping in backdoored/misaligned model and checking if safety measures
              catch it. Focus on protocol evaluation rather than model evaluation. Primarily Redwood's
              AI Control framework and related work. Examples: AI Control paper, Ctrl-Z, Control Arena,
              SHADE-Arena, D-REX, Subversion Strategy Eval, safety cases.
            sr2024:
              summary: "don't evaluate the model, evaluate the humans / organisation / deployment method instead: assess the risk of a particular protocol for deploying your model in a particular scenario, by silently swapping in an intentionally misaligned model"
              names: "Redwood, Buck Shlegeris, Ryan Greenblatt, Kshitij Sachan, Alex Mallen"
              ftes: "9"
              outputs: "AI Control, Subversion Strategy Eval, sequence, toy models, notes"
              funded_by: "Open Philanthropy, Survival and Flourishing Fund"
          
          - id: deception_criticisms
            name: Criticisms
            description: >
              Skeptical perspectives on deception detection, control methods, or monitoring
              approaches. Arguments about fundamental limitations or practical difficulties.
          
          - id: deception_mech_anomaly
            name: Mechanistic anomaly detection
            description: >
              Detect deception through internal activation anomalies, even without knowing
              what specific deception is happening. Build baseline of "normal" internal
              functioning, flag divergences. Uses mechanistic understanding but NOT requiring
              full interpretability. Different from whitebox probes (which test for specific
              things) - this detects unexpected patterns. Examples: password-locked elicitation,
              law of iterated expectations, ARC heuristic estimators.
            sr2024:
              summary: "understand what an LLM's normal (~benign) functioning looks like and detect divergence from this, even if we don't understand the exact nature of that divergence"
              names: "Nora Belrose, Erik Jenner"
              ftes: "1-10"
              outputs: "Password-locked model capability elicitation, Towards a Law of Iterated Expectations for Heuristic Estimators, Eleuther research update, Concrete empirical research projects, Mack and Turner"
              funded_by: "ARC, Eleuther funders"
          
          - id: deception_cadenza
            name: Cadenza
            description: >
              Cadenza Labs project - white-box dishonesty detection using internal activations.
              Organization-specific category for Cadenza's approach and methods. Examples:
              Cluster-norm for unsupervised probing, dishonesty detection without labels.
            sr2024:
              summary: "now focusing on developing robust white-box dishonesty-detection methods for LLM's and model evals"
              names: "Kieron Kretschmar, Walter Laurito, Sharan Maiya, Grégoire Dhimoïla"
              ftes: "3"
              outputs: "Cluster-Norm for Unsupervised Probing of Knowledge"
              funded_by: "self-funded / volunteers"
          
          - id: deception_faithful_cot
            name: Faithful CoT via separation
            description: >
              Architectural solutions to keep chain-of-thought faithful and observable. 
              Separate reasoning from presentation so models can't hide deceptive thoughts.
              Design models where internal reasoning must flow through auditable channel.
              Examples: Shoggoth/face/paraphraser architecture (Kokotajlo), unoptimized
              reasoning traces, separation of concerns.
            sr2024:
              summary: "shoggoth/face + paraphraser. Avoid giving the model incentives to hide its deceptive cognition or steganography"
              names: "Daniel Kokotajlo, AI Futures Project"
              ftes: "1"
              outputs: "Why don't we just…"
              funded_by: "SFF"
          
          - id: deception_indirect
            name: Indirect deception monitoring
            description: >
              Surface-level behavioral signals of deception without looking at internals.
              Lie classifiers, sycophancy detection, simple behavioral probes. NOT mechanistic
              (doesn't examine activations). NOT CoT-based (doesn't analyze reasoning). Just
              input-output behavioral patterns. Examples: simple probes for deception, 
              sycophancy detection, behavioral lie classifiers.
            sr2024:
              summary: "build tools to find whether a model will misbehave in high stakes circumstances by looking at it in testable circumstances. This bucket catches work on lie classifiers, sycophancy, Scaling Trends For Deception"
              names: "Anthropic, Monte MacDiarmid, Meg Tong, Mrinank Sharma, Owain Evans, Colognese"
              ftes: "1-10"
              outputs: "Simple probes can catch sleeper agents, Sandbag Detection through Noise Injection, Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion in LLMs"
              funded_by: "Anthropic funders"
          
          - id: deception_other
            name: Other deception prevention
            description: >
              Deception prevention approaches not fitting above categories. ONLY use for
              novel methods distinct from control evals, anomaly detection, indirect monitoring,
              faithful CoT, or Cadenza's approach.
      
      - id: cot_monitoring
        name: Chain of thought monitoring
        description: >
          Analyzing visible chain-of-thought outputs for safety signals. Monitoring
          reasoning traces for problematic patterns. NOT about ensuring CoT faithfulness
          (that's deception_faithful_cot). NOT about deception detection specifically
          (that's deception_indirect). General safety monitoring of reasoning steps.
          Examples: detecting dangerous reasoning, monitoring for concerning plans.
        children:
          - id: cot_monitoring_methods
            name: CoT monitoring methods
            description: >
              Techniques for monitoring chain-of-thought reasoning for safety signals,
              problematic patterns, or concerning reasoning steps.
          
          - id: cot_criticisms
            name: CoT monitoring criticisms
            description: >
              Limitations of CoT monitoring, faithfulness concerns, arguments that
              CoT may not reflect true model reasoning.
      
      - id: whitebox_monitoring
        name: Whitebox monitoring
        description: >
          Using internal activations and probes for safety monitoring. Training specific
          probes to detect known properties or behaviors. Different from mechanistic anomaly
          detection (which finds unexpected patterns) - this tests for specific things you're
          looking for. Examples: probes for specific behaviors, activation-based classifiers,
          representation monitoring for known properties.
        children:
          - id: whitebox_methods
            name: Whitebox monitoring methods
            description: >
              Techniques using internal activations, probes, or representation analysis
              for monitoring model behavior and ensuring safety.
          
          - id: whitebox_criticisms
            name: Whitebox monitoring criticisms
            description: >
              Probe specificity concerns, limitations of activation-based monitoring,
              arguments about fundamental difficulties.
      
      - id: redteam_monitoring
        name: Red-teaming monitoring
        description: >
          Adversarial monitoring using competitive dynamics between models. One model
          tries to catch what another model hides. Debate-based monitoring, adversarial
          critics. Different from control evals (which test protocols) or whitebox probes
          (which test models) - this uses model-vs-model competition. Examples: debate
          for oversight, adversarial critics, SHADE-Arena, prover-estimator games.
        children:
          - id: redteam_monitoring_methods
            name: Red-teaming monitoring methods
            description: >
              Monitoring approaches based on adversarial interaction, debate, or
              competitive dynamics. Models monitoring other models adversarially.
          
          - id: latent_adversarial_training
            name: Latent adversarial training
            description: >
              Adversarial state manipulation, automated red-teaming of internal
              representations, training against latent adversaries.
            sr2024:
              summary: "uncover dangerous properties like scheming in a target model by having another model manipulate its state adversarially"
              names: "Stephen Casper, Dylan Hadfield-Menell"
              ftes: "1-10"
              outputs: "LAT Improves Robustness to Persistent Harmful Behaviors in LLMs, LAT Defends Against Unforeseen Failure Modes"
              funded_by: "?"
      
      - id: surgical_edits
        name: Surgical model edits
        description: >
          Targeted interventions to modify model behavior without full retraining.
          Includes activation steering, unlearning, and precise capability modification.
        children:
          - id: activation_engineering
            name: Activation engineering
            description: >
              Steering vectors, representation engineering, directly modifying activations
              to control behavior. Includes BiDPO and other activation-based control methods.
            sr2024:
              summary: "a sort of interpretable finetuning. Let's see if we can programmatically modify activations to steer outputs towards what we want, in a way that generalises across models and topics"
              names: "Jan Wehner, Alex Turner, Nina Panickssery, Marc Carauleanu, Collin Burns, Andrew Mack, Pedro Freire, Joseph Miller, Andy Zou, Andy Arditi, Ole Jorgensen"
              ftes: "10+?"
              outputs: "Circuit Breakers, An Introduction to Representation Engineering - an activation-based paradigm for controlling LLMs, Steering Llama-2 with contrastive activation additions, Simple probes can catch sleeper agents, Refusal in LLMs is mediated by a single direction, Mechanistically Eliciting Latent Behaviors in Language Models, Uncovering Latent Human Wellbeing in Language Model Embeddings, Goodfire, LatentQA, SelfIE, Mack and Turner, Obfuscated Activations Bypass LLM Latent-Space Defenses"
              funded_by: "various, including EA funds"
          
          - id: utility_engineering
            name: Utility engineering
            description: >
              Directly modifying model utilities or objectives through targeted interventions.
          
          - id: unlearning
            name: Unlearning
            description: >
              Removing specific capabilities or knowledge from trained models.
              Targeted forgetting of dangerous or undesired behaviors.
      
      - id: goal_robustness
        name: Goal robustness
        description: >
          Ensuring models maintain intended goals under distribution shift, optimization
          pressure, or environmental changes. Includes mild optimization, reward learning,
          and multi-agent safety.
        children:
          - id: mild_optimization
            name: Mild optimisation
            description: >
              Approaches to reduce optimization intensity or ensure satisficing behavior.
              MONA and related methods for preventing excessive optimization.
            sr2024:
              summary: "avoid Goodharting by getting AI to satisfice rather than maximise"
              names: "Jobst Heitzig, Simon Fischer, Jessica Taylor"
              ftes: "?"
              outputs: "How to safely use an optimizer, Aspiration-based designs sequence, Non-maximizing policies that fulfill multi-criterion aspirations in expectation"
              funded_by: "?"
          
          - id: rl_safety
            name: RL safety
            description: >
              Reinforcement learning safety methods. Includes work by Skalse, behaviorist
              rewards, AssistanceZero, and other RL-specific safety approaches.
          
          - id: multiagent_safety
            name: Multi-agent safety
            description: >
              Safety in multi-agent settings, coordination between AIs, emergent behaviors
              from agent interaction. NOT cooperation theory (separate category).
          
          - id: assistance_games
            name: Assistance games / reward learning
            description: >
              Assistance games framework, learning rewards from human behavior, provably
              beneficial AI, correlated proxies. CIRL and related approaches.
            sr2024:
              summary: "reorient the general thrust of AI research towards provably beneficial systems"
              names: "Joar Skalse, Anca Dragan, Stuart Russell, David Krueger"
              ftes: "10+"
              outputs: "The Perils of Optimizing Learned Reward Functions, Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking, Changing and Influenceable Reward Functions, RL, but don't do anything I wouldn't do, Interpreting Preference Models w/ Sparse Autoencoders"
              funded_by: "EA funds, Open Philanthropy. Survival and Flourishing Fund, Manifund"
      
      - id: formal_verification
        name: Guaranteed Safe AI / Formal verification
        description: >
          Formal methods, mathematical proofs of safety properties, verified AI systems.
          Formally model behavior of systems, define precise constraints on actions, require
          AIs to provide safety proofs for recommended actions. Work by Bengio, Tegmark,
          davidad, Russell, UK ARIA on provably safe systems. Examples: Bayesian oracle,
          ARIA Safeguarded AI Programme, Open Agency Architecture.
        sr2024:
          summary: "formally model the behavior of cyber-physical systems, define precise constraints on what actions can occur, and require AIs to provide safety proofs for their recommended actions (correctness and uniqueness)"
          names: "Yoshua Bengio, Max Tegmark, Steve Omohundro, David \"davidad\" Dalrymple, Joar Skalse, Stuart Russell, Ohad Kammar, Alessandro Abate, Fabio Zanassi"
          ftes: "10-50"
          outputs: "Bayesian oracle, Towards Guaranteed Safe AI, ARIA Safeguarded AI Programme Thesis"
          funded_by: "UK government, OpenPhil, Survival and Flourishing Fund, Mila / CIFAR"

  - id: alternative_architectures
    name: Alternative architectures
    description: >
      Fundamentally different AI system designs that might be safer by construction.
      Non-standard approaches to building AI that avoid some failure modes of
      current paradigms.
    children:
      - id: aria_safeguarding
        name: ARIA Safeguarding
        description: >
          UK ARIA's safeguarding program and alternative architecture research.
          Government-funded exploration of safer-by-design AI systems.
      
      - id: cognitive_software
        name: "Conjecture: Cognitive Software"
        description: >
          Conjecture's cognitive programs approach, tactics, bounded tool AI.
          Alternative paradigm based on compositional cognitive architectures.
        sr2024:
          summary: "make tools to write, execute and deploy cognitive programs; compose these into large, powerful systems that do what we want; make a training procedure that lets us understand what the model does and does not know at each step"
          names: "Connor Leahy, Gabriel Alfour, Adam Shimi"
          ftes: "1-10"
          outputs: "The Tactics programming language/framework, cognitive emulations work, A Roadmap for Cognitive Software and A Humanist Future of AI"
          funded_by: "Plural Platform, Metaplanet, Others, Firestreak Ventures, EA Funds in 2022"
      
      - id: social_instinct
        name: Social-instinct AGI
        description: >
          Byrnes' social-instinct approach, brain-like AGI inspired by social circuits,
          symbol grounding via social learning. Astera Institute work on biologically-inspired
          safe AGI architectures.
        sr2024:
          summary: "Social and moral instincts are (partly) implemented in particular hardwired brain circuitry; let's figure out what those circuits are and how they work; this will involve symbol grounding"
          names: "Steve Byrnes"
          ftes: "1"
          outputs: "My AGI safety research—2024 review, '25 plans, Neuroscience of human social instincts: a sketch, Intuitive Self Models"
          funded_by: "Astera Institute"

  - id: better_data
    name: Better data
    description: >
      Data-level interventions for safety across the training pipeline. Includes
      pre-training data (filtering, poisoning detection), post-training data (RLHF
      quality, synthetic examples), and data transparency (attribution). Focus on
      making training data safer, higher quality, or more interpretable.
    children:
      - id: data_filtering
        name: Data filtering for safety
        description: >
          Pre-training data filtering: removing harmful content, bias mitigation,
          toxicity filtering, unsafe capability data removal. Interventions before
          training starts.
      
      - id: data_poisoning
        name: Data poisoning defense
        description: >
          Detecting and removing poisoned or backdoored training data. Security
          against adversarial data manipulation, trojans, and inserted vulnerabilities.
      
      - id: data_attribution
        name: Data attribution
        description: >
          Understanding what training data influenced model outputs or capabilities.
          Tracing model behavior back to training examples. Includes influence
          functions, memorization detection.
      
      - id: synthetic_alignment_data
        name: Synthetic data for alignment
        description: >
          Generating synthetic training data to improve alignment: synthetic preference
          data, AI-generated safety examples, augmented alignment datasets. If
          AI-generated, overlaps with AI-assisted alignment.
      
      - id: alignment_data_quality
        name: Data quality for alignment
        description: >
          RLHF data quality, preference data quality, instruction-following data
          curation. Post-training data quality for better alignment outcomes.
      
      - id: data_other
        name: Other data interventions
        description: >
          Data-level safety interventions not fitting above categories. ONLY use for
          novel data approaches distinct from filtering, poisoning defense, attribution,
          synthetic data, or quality improvement.

  - id: ai_solve_alignment
    name: Make AI solve it
    description: >
      Using AI systems to help with alignment research and oversight. Scalable
      oversight, recursive improvement, AI-assisted safety work.
    children:
      - id: scalable_oversight
        name: Scalable oversight
        description: >
          Methods for humans to oversee AI systems on tasks where humans can't directly
          evaluate outputs. Using AI assistance to help humans supervise more capable AI
          systems. Includes prover-verifier games, weak-to-strong generalization, recursive
          reward modeling, critiques. Core superalignment approach.
        children:
          - id: scalable_oversight_openai
            name: "OpenAI Superalignment / Automated Alignment Research"
            description: >
              OpenAI's superalignment program and automated alignment research.
              Prover-verifier games, using AI to help with alignment research.
            sr2024:
              summary: "be ready to align a human-level automated alignment researcher"
              names: "Jan Leike, Elriggs, Jacques Thibodeau"
              ftes: "10-50"
              outputs: "Prover-verifier games"
              funded_by: "lab funders"
          
          - id: weak_to_strong
            name: Weak-to-strong generalization
            description: >
              Weak models supervising strong models, easy-to-hard generalization.
              Can weak supervisors elicit strong capabilities safely?
            sr2024:
              summary: "use weaker models to supervise and provide a feedback signal to stronger models"
              names: "Jan Leike, Collin Burns, Nora Belrose, Zachary Kenton, Noah Siegel, János Kramár, Noah Goodman, Rohin Shah"
              ftes: "10-50"
              outputs: "Easy-to-Hard Generalization, Balancing Label Quantity and Quality for Scalable Elicitation, The Unreasonable Effectiveness of Easy Training Data, On scalable oversight with weak LLMs judging strong LLMs, Your Weak LLM is Secretly a Strong Teacher for Alignment"
              funded_by: "lab funders, Eleuther funders"
          
          - id: supervising_improvement
            name: Supervising AIs improving AIs
            description: >
              Overseeing AI systems as they improve other AI systems. Behavioral
              drift concerns, doubly-efficient debate, recursive improvement safety.
            sr2024:
              summary: "scalable tracking of behavioural drift, benchmarks for self-modification"
              names: "Roman Engeler, Akbir Khan, Ethan Perez"
              ftes: "1-10"
              outputs: "Weak LLMs judging strong LLMs, Scalable AI Safety via Doubly-Efficient Debate, Debating with More Persuasive LLMs Leads to More Truthful Answers, Prover-Verifier Games Improve Legibility of LLM Output, LLM Critics Help Catch LLM Bugs"
              funded_by: "lab funders"
          
          - id: cyborgism
            name: Cyborgism
            description: >
              Human-AI collaboration, augmentation, and cognitive partnership.
              Janus's simulator framing, Pantheon Interface, human-plus-LLM systems.
            sr2024:
              summary: "Train human-plus-LLM alignment researchers: with humans in the loop and without outsourcing to autonomous agents"
              names: "Janus, Nicholas Kees Dupuis"
              ftes: "?"
              outputs: "Pantheon Interface"
              funded_by: "?"
          
          - id: transluce
            name: Transluce
            description: >
              Transluce's monitor interface, neuron description tools, and interpretability
              for oversight. Schmidt Sciences funded work.
            sr2024:
              summary: "Make open AI tools to explain AIs, including agents. E.g. feature descriptions for neuron activation patterns; an interface for steering these features; behavior elicitation agent that searches for user-specified behaviors from frontier models"
              names: "Jacob Steinhardt, Sarah Schwettmann"
              ftes: "6"
              outputs: "Eliciting Language Model Behaviors with Investigator Agents, Monitor: An AI-Driven Observability Interface, Scaling Automatic Neuron Description"
              funded_by: "Schmidt Sciences, Halcyon Futures, John Schulman, Wojciech Zaremba"
          
          - id: deepmind_amplified_oversight
            name: DeepMind Amplified Oversight
            description: >
              DeepMind's amplified oversight program. Using AI assistance to scale human
              oversight beyond direct evaluation capabilities. Methods for leveraging AI
              systems to help humans oversee more capable AI systems.
      
      - id: debate
        name: Debate
        description: >
          AI debate for scalable oversight: models argue opposing sides for human judge,
          truth emerges from adversarial interaction. Includes doubly-efficient debate,
          prover-verifier games, debate for truthfulness. Work by DeepMind, Anthropic,
          UK AISI on adversarial oversight methods.
        children:
          - id: debate_uk_aisi
            name: UK AISI debate sequence
            description: >
              UK AISI's research sequence on debate for oversight and safety.
          
          - id: debate_deepmind
            name: Deepmind Scalable Alignment
            description: >
              Deepmind's work on debate, including doubly efficient debate and
              related scalable alignment methods.
            sr2024:
              summary: "make highly capable agents do what humans want, even when it is difficult for humans to know what that is"
              names: "Rohin Shah, Jonah Brown-Cohen, Georgios Piliouras"
              ftes: "?"
              outputs: "Progress update - Doubly Efficient Debate, Inference-only Experiments"
              funded_by: "Google"
          
          - id: debate_anthropic
            name: "Anthropic: Bowman/Perez"
            description: >
              Anthropic work on debate and truthfulness oversight by Bowman, Perez,
              and collaborators.
            sr2024:
              summary: "scalable oversight of truthfulness: is it possible to develop training methods that incentivize truthfulness even when humans are unable to directly judge the correctness of a model's output?"
              names: "Sam Bowman, Ethan Perez, He He, Mengye Ren"
              ftes: "?"
              outputs: "Debating with more persuasive LLMs Leads to More Truthful Answers, Sleeper Agents"
              funded_by: "mostly Anthropic's investors"
      
      - id: task_decomp
        name: Task decomposition
        description: >
          Breaking down complex tasks into verifiable subtasks. Recursive decomposition
          for oversight and safety.
      
      - id: adversarial_oversight
        name: Adversarial oversight
        description: >
          Using adversarial dynamics for oversight. Competitive or adversarial
          interactions to ensure safety.
      
      - id: elicit_ought
        name: Elicit / Ought
        description: >
          Elicit/Ought work on process supervision, decomposition, and researcher
          epistemics. Focus on governance researcher epistemics and research assistance.

  - id: theory
    name: Theory
    description: >
      Theoretical foundations for alignment: agent foundations, formal frameworks,
      understanding agency and optimization, corrigibility, abstraction.
    children:
      - id: arc_vaintrob_critique
        name: ARC Vaintrob ~critique
        description: >
          ARC's Vaintrob critique of various alignment approaches or theoretical
          frameworks.
      
      - id: arc_theory_formal
        name: "ARC Theory: Formalizing heuristic arguments"
        description: >
          ARC Theory work on formalizing heuristic arguments about AI risk.
          Tail risk formalization, heuristic estimators. FLI/SFF funded.
        sr2024:
          summary: "mech interp plus formal verification. Formalize mechanistic explanations of neural network behavior, so to predict when novel input may lead to anomalous behavior"
          names: "Jacob Hilton, Mark Xu, Eric Neyman, Dávid Matolcsi, Victor Lecomte, George Robinson"
          ftes: "1-10"
          outputs: "Estimating tail risk, Towards a Law of Iterated Expectations for Heuristic Estimators, Probabilities of rare outputs, Bird's eye overview, Formal verification"
          funded_by: "FLI, SFF"
      
      - id: misc_theory
        name: Miscellaneous theory items
        description: >
          Theoretical alignment work not fitting other specific theory categories. General
          formal frameworks, mathematical approaches to alignment problems, foundational
          questions about AI safety. Use only for genuinely novel theoretical approaches
          not covered by other theory subcategories.
      
      - id: understanding_agency
        name: Understanding agency
        description: >
          Foundational questions about agency: what IS an agent? What IS goal-directedness?
          What IS optimization? NOT applied methods to make agents safe - that goes elsewhere.
          Pure theory about the nature of agency. Examples: formalizing goal-directedness,
          measuring optimization power, understanding what makes something an agent.
        children:
          - id: causal_incentives
            name: Causal Incentives
            description: >
              Causal world models, goal-directedness formalization, causal structure
              of agency. Deepmind work on causal incentives framework.
            sr2024:
              summary: "use causal models to understand agents. Originally this was to design environments where they lack the incentive to defect, hence the name"
              names: "Tom Everitt, Matt McDermott, Francis Rhys Ward, Jonathan Richens, Ryan Carey"
              ftes: "1-10"
              outputs: "Robust agents learn causal world models, Measuring Goal-Directedness"
              funded_by: "EA funds, Manifund, Deepmind"
          
          - id: hierarchical_agency
            name: Hierarchical agency
            description: >
              Subagents and superagents, free-energy equilibria, compositional
              agency, nested optimization. SFF funded.
            sr2024:
              summary: "Develop formal models of subagents and superagents, use the model to specify desirable properties of whole-part relations (e.g. how to prevent human-friendly parts getting wiped out)"
              names: "Jan Kulveit, Roman Leventov, Scott Viteri, Michael Levin, Ivan Vendrov, Richard Ngo"
              ftes: "1-10"
              outputs: "Hierarchical Agency: A Missing Piece in AI Alignment, How do parts form a whole?, Free-energy equilibria, Two conceptions of active inference, generalised active inference"
              funded_by: "SFF"
          
          - id: dovetail
            name: Dovetail research
            description: >
              Formalizing "structure" and "agency", foundational concepts for
              understanding agents. LTFF funded.
            sr2024:
              summary: "Formalize key ideas (\"structure\", \"agency\", etc) mathematically"
              names: "Alex Altair, Alfred Harwood, Daniel C, Dalcy K, José Pedro Faustino"
              ftes: "2"
              outputs: "mostly exposition but it's early days. Formalization"
              funded_by: "LTFF"
          
          - id: boundaries_membranes
            name: Boundaries / membranes
            description: >
              Causal separation, agent boundaries, where one agent ends and another
              begins. Chris Lakin's work on boundaries.
            sr2024:
              summary: "Formalise one piece of morality: the causal separation between agents and their environment"
              names: "Chris Lakin, Andrew Critch, davidad, Evan Miyazono, Manuel Baltieri"
              ftes: "0.5"
              outputs: "Chris posts"
              funded_by: "?"
          
          - id: understanding_optimization
            name: Understanding optimisation
            description: >
              Optimization power, gradient hacking, mesa-optimization, what optimization
              is and how to measure it. CLR/EA funded work.
            sr2024:
              summary: "what is \"optimisation power\" (formally), how do we build tools that track it, and how relevant is any of this anyway"
              names: "Alex Flint, Guillaume Corlouer, Nicolas Macé"
              ftes: "1-10"
              outputs: "Degeneracies are sticky for SGD"
              funded_by: "CLR, EA funds"
      
      - id: corrigibility
        name: Corrigibility
        description: >
          Theoretical work on corrigibility: allowing shutdown, being correctable,
          not resisting modification. Formal frameworks for corrigible agents.
        children:
          - id: behavior_alignment_theory
            name: Behavior alignment theory
            description: >
              Powerseeking theorems, predicting properties of powerful agents, formal models
              of agent behavior. Includes CAST (Corrigibility As Singular Target), shutdown
              problem formalization, instrumental convergence proofs. Work by Turner, Cohen,
              Wentworth, Thornley on theoretical foundations of alignment and corrigibility.
            sr2024:
              summary: "predict properties of AGI (e.g. powerseeking) with formal models. Corrigibility as the opposite of powerseeking"
              names: "Michael K. Cohen, Max Harms/Raelifin, John Wentworth, David Lorell, Elliott Thornley"
              ftes: "1-10"
              outputs: "CAST: Corrigibility As Singular Target, A Shutdown Problem Proposal, The Shutdown Problem: Incomplete Preferences as a Solution"
              funded_by: "?"
          
          - id: corrigibility_other
            name: Other corrigibility
            description: >
              Corrigibility research not fitting behavior alignment theory. ONLY use for
              novel corrigibility approaches distinct from powerseeking theorems, CAST,
              or shutdown formalization.
      
      - id: ontology_identification
        name: Ontology Identification
        description: >
          How to identify and align on correct concepts and abstractions for specifying
          values. The problem: humans think in terms of "happiness" and "justice"; AIs
          learn alien concepts. How to bridge the gap? Includes ontological crises,
          concept identification, abstraction alignment. Related to ELK problem. Examples:
          natural abstractions, translating between human and AI ontologies.
        children:
          - id: natural_abstractions
            name: Natural abstractions
            description: >
              Wentworth's natural abstractions hypothesis: that our universe "abstracts well"
              and many cognitive systems learn similar abstractions. Natural latents, convergent
              abstractions that humans and AIs might share. Includes representational alignment
              and testing whether features correspond to human concepts. EA funded.
            sr2024:
              summary: "check the hypothesis that our universe \"abstracts well\" and that many cognitive systems learn to use similar abstractions. Check if features correspond to small causal diagrams corresponding to linguistic constructions"
              names: "John Wentworth, Paul Colognese, David Lorrell, Sam Eisenstat"
              ftes: "1-10"
              outputs: "Natural Latents: The Concepts, Natural Latents Are Not Robust To Tiny Mixtures, Towards a Less Bullshit Model of Semantics"
              funded_by: "EA Funds"
          
          - id: ontology_other
            name: Other ontology work
            description: >
              Ontology identification work not fitting natural abstractions. ONLY use for
              novel approaches to concept alignment, abstraction identification, or
              ontological bridging distinct from natural abstractions hypothesis.
      
      - id: understand_cooperation
        name: Understand cooperation
        description: >
          Theoretical work on cooperation, multi-agent dynamics, s-risks, game theory
          for AI systems.
        children:
          - id: pluralistic_alignment
            name: Pluralistic alignment / collective intelligence
            description: >
              Aligning AI with diverse human values, collective intelligence, representing
              multiple stakeholders. Choi, Lazar work.
            sr2024:
              summary: "align AI to broader values / use AI to understand and improve coordination among humans"
              names: "Yejin Choi, Seth Lazar, Nouha Dziri, Deger Turan, Ivan Vendrov, Jacob Lagerros"
              ftes: "10-50"
              outputs: "roadmap, workshop"
              funded_by: "Foresight, Midjourney?"
          
          - id: clr
            name: Center on Long-Term Risk (CLR)
            description: >
              CLR work on s-risks (suffering risks from future AI agents), cooperation theories
              for reducing worst-case outcomes, measuring properties related to catastrophic
              conflict. Making present and future AIs inherently cooperative. Polaris/SFF funded.
              Examples: measurement research agenda, optimal commitments to strategies.
            sr2024:
              summary: "future agents creating s-risks is the worst of all possible problems, we should avoid that"
              names: "Jesse Clifton, Caspar Oesterheld, Anthony DiGiovanni, Maxime Riché, Mia Taylor"
              ftes: "10-50"
              outputs: "Measurement Research Agenda, Computing Optimal Commitments to Strategies and Outcome-conditional Utility Transfers"
              funded_by: "Polaris Ventures, Survival and Flourishing Fund, Community Foundation Ireland"
          
          - id: focal
            name: FOCAL
            description: >
              Foundations of Cooperative AI. Make sure advanced AI uses proper game theory,
              encourage cooperation and benefit to humans. Game theory for AI systems,
              Cooperative AI Foundation work, formal frameworks for multi-agent cooperation.
              Includes decision-theoretic reasoning, social choice for AI alignment.
            sr2024:
              summary: "make sure advanced AI uses what we regard as proper game theory"
              names: "Vincent Conitzer, Caspar Oesterheld, Vojta Kovarik"
              ftes: "1-10"
              outputs: "Foundations of Cooperative AI, A dataset of questions on decision-theoretic reasoning in Newcomb-like problems, Why should we ever automate moral decision making?, Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback"
              funded_by: "Cooperative AI Foundation, Polaris Ventures"
          
          - id: alternatives_utility
            name: Alternatives to utility theory
            description: >
              Social choice theory, going beyond preferences, challenging coherence
              theorems, non-utility-theoretic frameworks for AI goals.
      
      - id: infrastructure_agents
        name: Infrastructure for AI Agents
        description: >
          Theoretical work on infrastructure, standards, protocols for AI agent
          ecosystems. Foundations for multi-agent worlds.
      
      - id: shard_theory
        name: (Descendents of) Shard theory
        description: >
          Shard theory and related approaches to understanding how values form during training.
          Models internal components of agents, uses humans as model organism of AGI. Policies
          controlled by ensemble of influences ("shards"). Consider which training approaches
          increase chance that human-friendly shards influence that ensemble. Now more empirical
          ML agenda. Examples: gradient routing, intrinsic power-seeking work.
        sr2024:
          summary: "model the internal components of agents, use humans as a model organism of AGI (humans seem made up of shards and so might AI). Now more of an empirical ML agenda"
          names: "Alex Turner, Quintin Pope, Alex Cloud, Jacob Goldman-Wetzler, Evzen Wybitul, Joseph Miller"
          ftes: "1-10"
          outputs: "Intrinsic Power-Seeking: AI Might Seek Power for Power's Sake, Shard Theory - is it true for humans?, Gradient routing"
          funded_by: "Open Philanthropy (via funding of MATS), EA funds, Manifund"
      
      - id: singular_learning
        name: Singular Learning Theory
        description: >
          Singular learning theory applications to alignment. Mathematical framework
          from algebraic geometry applied to neural network learning.
      
      - id: learning_theoretic_agenda
        name: The Learning-Theoretic Agenda
        description: >
          Kosoy's learning-theoretic agenda: formalize a more realistic agent, understand
          what it means for it to be aligned with us, translate between its ontology and
          ours. Infra-Bayesian frameworks, fixing formal epistemology to work out how to
          avoid deep training problems. EA/SFF/ARIA funded. Examples: linear infra-Bayesian
          bandits, infra-Bayesian haggling, quantum mechanics in infra-Bayesian physicalism.
        sr2024:
          summary: "try to formalise a more realistic agent, understand what it means for it to be aligned with us, translate between its ontology and ours, and produce desiderata for a training setup that points at coherent AGIs similar to our model of an aligned agent"
          names: "Vanessa Kosoy, Diffractor"
          ftes: "3"
          outputs: "Linear infra-Bayesian Bandits, Time complexity for deterministic string machines, Infra-Bayesian Haggling, Quantum Mechanics in Infra-Bayesian Physicalism. Intro lectures"
          funded_by: "EA Funds, Survival and Flourishing Fund, ARIA"
      
      - id: qaci
        name: Question-answer counterfactual intervals (QACI)
        description: >
          QACI framework: get the AI to work out its own objective function (à la HCH).
          Make fully formalized goal such that computationally unbounded oracle with it
          would take desirable actions; design computationally bounded AI good enough to
          take satisfactory actions. HCH-style iterated amplification, epistemic states
          and counterfactuals. SFF funded work.
        sr2024:
          summary: "Get the thing to work out its own objective function (a la HCH)"
          names: "Tamsin Leake, Julia Persson"
          ftes: "1-10"
          outputs: "Epistemic states as a potential benign prior"
          funded_by: "Survival and Flourishing Fund"

  - id: sociotechnical
    name: Sociotechnical
    description: >
      Alignment approaches that combine technical and social elements. Gradual
      disempowerment, collective alignment, third-wave AI safety.
    children:
      - id: gradual_disempowerment
        name: Gradual Disempowerment
        description: >
          Approaches focused on gradually reducing human disempowerment from AI,
          slowing capability growth, or managing transition carefully.
      
      - id: third_wave
        name: Third wave AI safety
        description: >
          Third wave AI safety paradigm focusing on sociotechnical systems,
          participatory approaches, broader stakeholder inclusion.
      
      - id: collective_alignment
        name: Collective alignment
        description: >
          Aligning AI with collective human values, democratic inputs, representing
          diverse preferences. More implementation-focused than theoretical pluralistic
          alignment.

  - id: misc_new_agendas
    name: Misc / for new agenda clustering
    description: >
      Research that doesn't fit existing categories but may form new clusters.
      Items here should be recategorized as patterns emerge.
    children:
      - id: misc_multiagents
        name: Multi-agents (misc)
        description: >
          Multi-agent work not fitting other categories (not safety-specific,
          not cooperation theory, not multi-agent safety).
      
      - id: misc_standards
        name: Making standards and protocols
        description: >
          Creating standards, protocols, or infrastructure for AI safety that
          doesn't fit other categories.
      
      - id: misc_doomsayers
        name: Doomsayers
        description: >
          Work focused primarily on arguing for high P(doom), existential risk
          communication, or pessimistic takes without proposing solutions.
      
      - id: misc_tiling
        name: Tiling agents
        description: >
          Work specifically on tiling agents, self-modification while preserving
          goals, stable self-improvement.
      
      - id: misc_org_clusters
        name: Multi-org research clusters
        description: >
          Research programs spanning multiple organizations that form coherent agendas
          but don't fit above categories. Work from organizations pursuing multiple
          simultaneous research directions. Examples of multi-agenda orgs in 2024: Apollo
          (evals + interp + deception), CAIS (benchmarks + legislation), FAR (robustness +
          interp), Anthropic teams, OpenAI teams, UK/US AISIs (evals + policy).
      
      - id: misc_other
        name: Other uncategorized work
        description: >
          Truly novel research agendas not fitting any category above. ONLY use for
          genuinely distinct new approaches that represent unique research directions.
          Most work fits existing categories - use this sparingly.

