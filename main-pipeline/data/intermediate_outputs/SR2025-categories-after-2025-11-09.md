# “Labs” (giant companies) \[cat:big\_labs\]
## OpenAI Safety \[cat:openai\]
## Deepmind Responsibility & Safety \[cat:deepmind\]
## Anthropic Safety \[cat:anthropic\]
## xAI \[cat:xai\]
## Meta \[cat:meta\]
# Black-box safety (understand and control current model behaviour) \[cat:control\_thing\]
## Iterative alignment \[cat:iterative\_alignment\]
### Surgical model edits \[cat:surgical\_edits\]
#### Utility engineering \[cat:utility\_engineering\]
#### Unlearning \[cat:unlearning\]
## Control \[cat:control\]
## Safeguards (inference-time auxiliary defences) \[cat:anthropic\_safeguards\]
## Evals \[cat:evals\]
### Various capability evals \[cat:evals\_capability\]
#### AGI metrics
### Autonomy evals \[cat:evals\_autonomy\]
### WMD evals (Weapons of Mass Destruction) \[cat:evals\_wmd\]
### Situational awareness and self-awareness evals \[cat:evals\_situational\_awareness\]
### Steganography evals \[cat:evals\_steganography\]
### AI deception evals \[cat:ai\_deception\]
### Sandbagging evals \[cat:evals\_sandbagging\]
### Self-replication evals \[cat:evals\_self\_replication\]
### Security evals \[cat:evals\_security\]
### Various Redteams \[cat:various\_redteams\]
### Other evals and evals science \[cat:evals\_other\]
## Model psychology \[cat:model\_psychology\]
### Emergent misalignment \[cat:surprising\_generalization\]
### Model specs and constitutions (shape model psychology) \[cat:specs\_and\_constitutions\]
### Character training and persona steering \[cat:psych\_personas\]
### Model values / default preferences \[cat:model\_values\]
### Other model psychology \[cat:psych\_other\]
## Better data \[cat:better\_data\]
### Data filtering \[cat:data\_filtering\]
### Hyperstition studies \[cat:hyperstition\]
### Data poisoning defense \[cat:data\_poisoning\]
### Synthetic data for alignment \[cat:synthetic\_alignment\_data\]
### Data quality for alignment \[cat:alignment\_data\_quality\]
## Prevent deception and scheming \[cat:prevent\_deception\]
### Mechanistic anomaly detection \[cat:deception\_mech\_anomaly\]
### Cadenza \[cat:deception\_cadenza\]
### Indirect deception monitoring \[cat:deception\_indirect\]
### Other deception prevention \[cat:deception\_other\]
## Goal robustness \[cat:goal\_robustness\]
### Mild optimisation \[cat:mild\_optimization\]
### RL safety \[cat:rl\_safety\]
### Multi-agent safety \[cat:multiagent\_safety\]
### Assistance games / reward learning \[cat:assistance\_games\]
# White-box safety (understand and control current model internals) \[cat:whitebox\]
## Interpretability \[cat:interpretability\]
### Reverse engineering \[cat:interp\_fundamental\]
### Concept-based interp \[cat:interp\_concept\_based\]
### Model diffing \[cat:model\_diff\]
## Data interpretability \[cat:datainterp\]
### Auditing real models \[cat:interp\_applied\]
### Sparse Coding \[cat:interp\_sparse\_coding\]
### Causal Abstractions \[cat:interp\_causal\_abstractions\]
### Data attribution \[cat:data\_attribution\]
## Other interpretability \[cat:interp\_other\]
## Whitebox control / monitoring \[cat:whitebox\_monitoring\]
## Activation engineering \[cat:activation\_engineering\]
## Chain of thought monitoring \[cat:cot\_monitoring\]
## Understand learning \[cat:understand\_learning\]
### Dev interp \[cat:learning\_dev\_interp\]
### Computational mechanics \[cat:learning\_comp\_mechanics\]
# Safety by design (new systems, often without singleton deep learning) \[cat:new\_safety\_by\_design\]
## Guaranteed Safe AI \[cat:formal\_verification\]
## Tegmark \[cat:tegmark\]
## Scientist AI \[cat:scientist\_ai\]
## Other formal verification \[cat:other\_formal\_verification\]
## Conjecture: Cognitive Software \[cat:conjecture\]
## Brainlike-AGI Safety \[cat:brainlike\_agi\]
# Make AI solve it \[cat:ai\_solve\_alignment\]
## Strong-to-Weak Elicitation \[cat:strong\_to\_weak\]
## Scalable oversight \[cat:scalable\_oversight\]
### Automated Alignment Research \[cat:scalable\_oversight\_openai\]
### Weak-to-strong generalization \[cat:weak\_to\_strong\]
### Supervising AIs improving AIs \[cat:supervising\_improvement\]
### Cyborgism \[cat:cyborgism\]
### Transluce \[cat:transluce\]
### DeepMind Amplified Oversight \[cat:deepmind\_amplified\_oversight\]
## Debate \[cat:debate\]
### Deepmind Scalable Alignment \[cat:debate\_deepmind\]
### Anthropic: Bowman/Perez \[cat:debate\_anthropic\]
## Task decomposition \[cat:task\_decomp\]
## Adversarial oversight \[cat:adversarial\_oversight\]
# Theory (how to understand and control current and future models) \[cat:theory\]
## Agent foundations \[cat:agent\_foundations\]
## Tiling agents \[cat:tiling\_agents\]
## Dovetail \[cat:theory\_dovetail\]
## Live Theory / Substrate-Sensitive AI \[cat:live\_theory\]
## Simulators \[cat:simulators\]
## Asymptotic guarantees \[cat:aisi\_guarantees\]
## ARC Theory \[cat:arc\_theory\_formal\]
## Corrigibility \[cat:corrigibility\]
### Behavior alignment theory \[cat:behavior\_alignment\_theory\]
### Other corrigibility \[cat:corrigibility\_other\]
## Ontology Identification \[cat:ontology\_identification\]
### Natural abstractions \[cat:natural\_abstractions\]
### Other ontology work \[cat:ontology\_other\]
## Understand cooperation \[cat:understand\_cooperation\]
### Pluralistic alignment / collective intelligence \[cat:pluralistic\_alignment\]
### Center on Long-Term Risk (CLR) \[cat:clr\] 
### FOCAL \[cat:focal\]
## The Learning-Theoretic Agenda \[cat:learning\_theoretic\_agenda\]
## Other theory \[cat:theory\_other\]
# Multi-agent first 
## New agent theories \[cat:new\_agent\_theories\]
## Align them like you’d align a human \[cat:human\_like\_cooperation\]
