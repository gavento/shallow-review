
# Labs (giant companies) \[sec:big\_labs\]

## OpenAI Safety \[a:openai\]

**See also:** iterative alignment, safeguards, personas.

**Host org structure**: public benefit corp

**Teams**: Alignment, Safety Systems (Interpretability, Safety Oversight, Pretraining Safety, Robustness, Safety Research, Trustworthy AI, new Misalignment Research team [coming](https://archive.is/eDB1D)), Preparedness, Model Policy, Safety and Security Committee, Safety Advisory Group. The [Persona Features](https://www.arxiv.org/pdf/2506.19823) paper had a distinct author list. No named successor to Superalignment.

**Public alignment agenda:** [None](https://openai.com/safety/how-we-think-about-safety-alignment/). Barak [offers](https://www.lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety) personal [views](https://windowsontheory.org/2025/06/24/machines-of-faithful-obedience/).

**Public plan**: [Preparedness Framework](https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf)

**Critiques:** [Stein-Perlman](https://ailabwatch.org/companies/openai), [Stewart](https://intelligence.org/2025/03/31/a-response-to-openais-how-we-think-about-safety-and-alignment/), [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims), [Midas](https://www.openaifiles.org/transparency-and-safety), [defense](https://www.wired.com/story/openai-anduril-defense/), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).) on labs in general. It’s [difficult](https://conversationswithtyler.com/episodes/sam-altman-2/) to model OpenAI as a single agent: “*ALTMAN: I very rarely get to have anybody work on anything. One thing about researchers is they’re going to work on what they’re going to work on, and that’s that.”*

**Some names:** Johannes Heidecke, Boaz Barak, Mia Glaese, Jenny Nitishinskaya, Lama Ahmad, Naomi Bashkansky, Miles Wang, Wojciech Zaremba, David Robinson, Zico Kolter, Jerry Tworek, Eric Wallace, Olivia Watkins, Kai Chen, Chris Koch, Andrea Vallone, Leo Gao.

**Funded by:** Microsoft, [AWS](https://www.aboutamazon.com/news/aws/aws-open-ai-workloads-compute-infrastructure), Oracle, NVIDIA, SoftBank, G42, AMD, Dragoneer, Coatue, Thrive, Altimeter, MGX, Blackstone, TPG, T. Rowe Price, Andreessen Horowitz, D1 Capital Partners, Fidelity Investments, Founders Fund, Sequoia…

**Outputs in 2025:**

* Their 60-page System Cards now contain a large amount of their public safety work.
* [https://alignment.openai.com/](https://alignment.openai.com/)
* [**Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation**](https://arxiv.org/abs/2503.11926)
* [**Persona Features Control Emergent Misalignment**](https://arxiv.org/abs/2506.19823)
* [**Stress Testing Deliberative Alignment for Anti-Scheming Training**](https://arxiv.org/abs/2509.15541)
* [**Deliberative Alignment: Reasoning Enables Safer Language Models**](https://arxiv.org/abs/2412.16339)
* [**Toward understanding and preventing misalignment generalization**](https://openai.com/index/emergent-misalignment)
* [**Our updated Preparedness Framework**](https://openai.com/index/updating-our-preparedness-framework/)
* [**Trading Inference-Time Compute for Adversarial Robustness**](https://arxiv.org/abs/2501.18841)
* [**Findings from a pilot Anthropic–OpenAI alignment evaluation exercise: OpenAI Safety Tests**](https://openai.com/index/openai-anthropic-safety-evaluation)
* [**Safety evaluations hub**](https://openai.com/safety/evaluations-hub)
* [**Small-to-Large Generalization: Data Influences Models Consistently Across Scale**](https://arxiv.org/abs/2505.16260)
* [**https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf**](https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf)

## Deepmind Responsibility & Safety \[a:deepmind\]

**See also:** Interpretability**,** Scalable Oversight
**Structure**: research laboratory subsidiary of a for-profit
**Teams**: [ASAT](https://www.alignmentforum.org/posts/wqz5CRzqWkvzoatBG/agi-safety-and-alignment-google-deepmind-is-hiring) (“AGI Alignment”, which consists of amplified oversight and interpretability plus “Frontier Safety”, which consists of framework development and implementation). Also Gemini Safety, [Voices of All in Alignment](https://www.edinburgh-robotics.org/events/whose-gold-aligning-ai-diverse-views-what%E2%80%99s-safe-aligned-and-beneficial), AGI Safety Council, Responsibility and Safety Council. Sort-of the [Causal Incentives Working Group](https://causalincentives.com/) too.
**Public alignment agenda:** [An Approach to Technical AGI Safety](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf%20)
**Framework:** [Frontier Safety Framework](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/updating-the-frontier-safety-framework/Frontier%20Safety%20Framework%202.0.pdf)

**Some names**: Rohin Shah, Allan Dafoe, Anca Dragan, Dave Orr, Alex Irpan, Alex Turner, Anna Wang, Arthur Conmy, David Lindner, Jonah Brown-Cohen, Lewis Ho, Neel Nanda, Raluca Ada Popa, Rishub Jain, Rory Greig, Scott Emmons, Sebastian Farquhar, Senthooran Rajamanoharan, Sophie Bridgers, Tobi Ijitoye, Tom Everitt, Victoria Krakovna, Vikrant Varma, Vladimir Mikulik, Zac Kenton, Noah Goodman, Four Flynn, Jonathan Richens, Lewis Smith.

**Critiques:** [Stein-Perlman](https://ailabwatch.org/companies/deepmind), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).)**,** [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims), [On Google's Safety Plan](https://lesswrong.com/posts/hvEikwtsbf6zaXG2s/on-google-s-safety-plan)

**Funded by:** Google. Explicit Deepmind spending as a whole was [£1.3B](https://s3.eu-west-2.amazonaws.com/document-api-images-live.ch.gov.uk/docs/WT_VNJe9leRjfcU0-OtRjWqF7WiqueStclXgHPbdG4U/application-pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAWRGBDBV3ETBK6MEZ%2F20251107%2Feu-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251107T165323Z&X-Amz-Expires=60&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEPT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCWV1LXdlc3QtMiJHMEUCIBO%2FN0Q9tFWQR%2FLthFYa8oNsFTPSnysg4ONrTUQ%2FyyG1AiEAkPoujFneu%2Bmo73eOnOGNV0tBYKpjLeiP7yATK0BxE7oqigQIvf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgw0NDkyMjkwMzI4MjIiDAUWi2tKimijl1eBRireA4WSlhKuVaQDvGJ2Mhhn6yANCCUO3zN8OlpSoMI2QFoeu6i%2BPxwG7TrcECWjXvY2m5d8vAbJZj%2BTfCgRBvOqlWdcvynzDenryX3a%2F0oh1M1LCvKFgZxSZxV1Pb0l6uDpcBww%2BeRUgnTyClEsdjxo0fACkgiFoJkRbXQr3jnzmghu%2B%2B9kDtWERtaeGA5Dmjfr6i0EibJHo0BwsaleiFdKCuWMnNVpgEO4u%2F85kMy%2ByX7vM1Kfj53OvRKF9cyULlQnwb0q7Rs%2FjRespSsIF%2BAiQbUDck3iT%2BkG0wdrnnLEdm1nIHpoBQO65a8GwLvYsNIS7LckCyuG275y1h1%2B%2BUozd4PyyJjLhqmWn71LcsHuPbjSlAfU9E%2BkE73%2FNuuaE4hRICKlqLl4V9IXPiysd1IfazKQwS3lXmXrFiXvASKvLH8VDuBBz%2Bwj0SG50Rhfwpmrl2lmJ4RFVd6Se9S20eZQjnV183HKkCsvyx09YizNCuJqxEeJmQ%2BAJZVyFjDWKLrpC8F9Yzo3Sf4hYtvQRXqrANoB3n4J%2BUUvrUEDSQqOeQOki1xpE37FBu8Ozs%2FKo%2FrYgESZDsXra0gLcJ7zJb3rJs75kxEyqYDiHuBQ8IKbobY%2BC%2FMHC5MOHqfW%2Bu%2Bl%2FUownLq3yAY6pQG1n3iz%2B09NSvDWENlBA5hC3lLvGiD5f1DTjYA9v1GKjA3feHOZ3Iegs1Egz9G%2FTST6riAYyauyj0At6oEUiUnTQn%2BJxtM%2BH%2FuVzzq45lboS5WE7pDOKA6jDVgzgjy6Mlm%2FY3dy28KxiOrV4EsefEtYTd8Nr6XIvJ0PGVEHhMsWo%2Bsn3IzkuKEqQsFsIiQHCb9oREBNnmY%2FDa2iO0fJijogUtrkxMs%3D&X-Amz-SignedHeaders=host&response-content-disposition=inline%3Bfilename%3D%22companies_house_document.pdf%22&X-Amz-Signature=da8a32ce365dd9ee8f6824636e1e893e1a87a58b64c65013631c1e2014bf7e3e), but this doesn’t count e.g. Gemini compute.

**Outputs in 2025:**

* [**A Pragmatic Vision for Interpretability**](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability%20)
* [**How Can Interpretability Researchers Help AGI Go Well?**](https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well%20)
* [**Evaluating Frontier Models for Stealth and Situational Awareness**](https://arxiv.org/abs/2505.01420)
* [**When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors**](https://arxiv.org/abs/2507.05246)
* [**MONA: Managed Myopia with Approval Feedback**](https://alignmentforum.org/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2)
* [**Consistency Training Helps Stop Sycophancy and Jailbreaks**](https://arxiv.org/abs/2510.27062)
* [**An Approach to Technical AGI Safety and Security**](https://arxiv.org/abs/2504.01849)
* [**InfAlign: Inference-aware language model alignment**](https://arxiv.org/abs/2412.19792)
* [**Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research (GDM Mech Interp Team Progress Update \#2)**](https://alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks)
* [**Taking a responsible path to AGI**](https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/)
* [**Evaluating potential cybersecurity threats of advanced AI**](https://deepmind.google/discover/blog/evaluating-potential-cybersecurity-threats-of-advanced-ai)
* Some work from non-safety teams, like [InfAlign](https://arxiv.org/abs/2412.19792)
* [Incentives for Responsiveness, Instrumental Control and Impact](https://arxiv.org/pdf/2001.07118)
* [Difficulties with Evaluating a Deception Detector for AIs](https://arxiv.org/html/2511.22662v1)

## Anthropic Safety \[a:anthropic\]

**See also:** Interpretability**,** Scalable Oversight
**Host org structure**: public-benefit corp

**Teams**: Scalable Alignment (Leike), Alignment Evals (Bowman), [Interpretability](https://transformer-circuits.pub/) (Olah), Control (Perez), Model Psychiatry (Lindsey), Character (Askell), Alignment Stress-Testing (Hubinger), Alignment Mitigations (Price?), Frontier Red Team (Graham), Safeguards (?), Societal Impacts (Ganguli), Trust and Safety (Sanderford), Model Welfare (Fish).

**Public alignment agenda:** [directions](https://alignment.anthropic.com/2025/recommended-directions/), [bumpers](https://alignment.anthropic.com/2025/bumpers/), [checklist](https://sleepinyourhat.github.io/checklist/), an [old vague view](https://www.anthropic.com/news/core-views-on-ai-safety)

**Framework:** [RSP](https://www-cdn.anthropic.com/872c653b2d0501d6ab44cf87f43e1dc4853e4d37.pdf)

**Some names:** Chris Olah, Evan Hubinger, Sam Marks, Johannes Treutlein, Sam Bowman, Euan Ong, Fabien Roger, Adam Jermyn, Holden Karnofsky, Jan Leike, Ethan Perez, Jack Lindsey, Amanda Askell, Kyle Fish, Sara Price, Jon Kutasov, Minae Kwon, Monty Evans, Richard Dargan.

**Critiques:** [Stein](https://ailabwatch.org/anthropic-opinions)\-[Perlman](https://ailabwatch.org/companies/anthropic), [Casper](https://www.lesswrong.com/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may#A_review___thoughts), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).), [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims), [Greenblatt](https://nitter.net/RyanPGreenblatt/status/1925992236648464774), [Samin](https://www.lesswrong.com/posts/5aKRshJzhojqfbRyo/unless-its-governance-changes-anthropic-is-untrustworthy), [defense](https://techcrunch.com/2024/11/07/anthropic-teams-up-with-palantir-and-aws-to-sell-its-ai-to-defense-customers/), [Existing Safety Frameworks Imply Unreasonable Confidence](https://lesswrong.com/posts/7ExkgcDudwhag73vw/existing-safety-frameworks-imply-unreasonable-confidence)

**Funded by:** Amazon, Google, ICONIQ, Fidelity, Lightspeed, Altimeter, Baillie Gifford, BlackRock, Blackstone, Coatue, D1 Capital Partners, General Atlantic, General Catalyst, GIC, Goldman Sachs, Insight Partners, Jane Street, Ontario Teachers' Pension Plan, Qatar Investment Authority, TPG, T. Rowe Price, WCM, XN.

**Outputs in 2025:**

* [Natural emergent misalignment from reward hacking](https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf)
* [**Agentic Misalignment: How LLMs could be insider threats**](https://anthropic.com/research/agentic-misalignment)
* [**SHADE-Arena: Evaluating sabotage and monitoring in LLM agents**](https://anthropic.com/research/shade-arena-sabotage-monitoring)
* [**Forecasting Rare Language Model Behaviors**](https://arxiv.org/abs/2502.16797)
* [**Why Do Some Language Models Fake Alignment While Others Don't?**](https://alignmentforum.org/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don),
* [**Petri: An open-source auditing tool to accelerate AI safety research**](https://alignment.anthropic.com/2025/petri), [**Signs of introspection in large language models**](https://anthropic.com/research/introspection)
* [**Recommendations for Technical AI Safety Research Directions**](https://alignment.anthropic.com/2025/recommended-directions/index.html)
* [**Putting up Bumpers**](https://alignment.anthropic.com/2025/bumpers/)
* [**Three Sketches of ASL-4 Safety Case Components**](https://alignment.anthropic.com/2024/safety-cases/index.html)
* [**Open-sourcing circuit tracing tools**](https://anthropic.com/research/open-source-circuit-tracing)
* [Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples](https://arxiv.org/abs/2510.07192)
* [Emergent Introspective Awareness in Large Language Models](https://transformer-circuits.pub/2025/introspection/index.html)
* [**Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise**](https://alignment.anthropic.com/2025/openai-findings),
* [Reasoning models don't always say what they think](https://www.anthropic.com/research/reasoning-models-dont-say-think)
* [On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)
* [Circuit Tracing: Revealing Computational Graphs in Language Models](https://transformer-circuits.pub/2025/attribution-graphs/methods.html)
* [Auditing language models for hidden objectives](https://www.anthropic.com/research/auditing-hidden-objectives)
* [Constitutional Classifiers: Defending against universal jailbreaks](https://www.anthropic.com/research/constitutional-classifiers)
* [The Soul Document](https://gist.github.com/Richard-Weiss/efe157692991535403bd7e7fb20b6695) ([confirmed](https://x.com/AmandaAskell/status/1995610567923695633))
* [Evaluating honesty and lie detection techniques on a diverse suite of dishonest models](https://alignment.anthropic.com/2025/honesty-elicitation/)

## xAI \[a:xai\]

**See also:** None.

**Host org structure**: [for-profit](https://www.cnbc.com/amp/2025/08/25/elon-musk-xai-dropped-public-benefit-corp-status-while-fighting-openai.html)

**Teams**: [Applied Safety](https://job-boards.greenhouse.io/xai/jobs/4944324007) and Model Evaluation. Nominally focussed on misuse.

**Public alignment agenda:** None.

**Framework:** [Risk Management Framework](https://data.x.ai/2025-08-20-xai-risk-management-framework.pdf).

**Some names:** Dan Hendrycks (advisor), Juntang Zhuang, Toby Pohlen, Lianmin Zheng, Piaoyang Cui, Nikita Popov, Ying Sheng, Sehoon Kim

**Critiques:** [framework](https://www.lesswrong.com/posts/hQyrTDuTXpqkxrnoH/xai-s-new-safety-framework-is-dreadful), [hacking](https://x.com/g_leech_/status/1990543987846078854), [broken promises](https://x.com/g_leech_/status/1990734517145911593), [Stein](https://ailabwatch.org/companies/xai)\-[Perlman](https://ailabwatch.org/resources/integrity#xai), [insecurity](https://nitter.net/elonmusk/status/1961904269545648624), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).) on labs in general

**Funded by:** A16Z, Blackrock, Fidelity, Kingdom, Lightspeed, MGX, Morgan Stanley, Sequoia…

**Outputs in 2025:** None.

## Meta \[a:meta\]

**See also:** model unlearning

**Host org structure**: for-profit

**Teams**: Safety “integrated into” capabilities research, Meta Superintelligence Lab. But also FAIR Alignment, [Brain and AI](https://www.metacareers.com/jobs/1319148726628205).

**Public alignment agenda:** None.

**Framework:** [FAF](https://ai.meta.com/static-resource/meta-frontier-ai-framework/?utm_source=newsroom&utm_medium=web&utm_content=Frontier_AI_Framework_PDF&utm_campaign=Our_Approach_to_Frontier_AI_blog)

**Some names:** Shuchao Bi, Hongyuan Zhan, Jingyu Zhang,  Haozhu Wang, Eric Michael Smith,  Sid Wang,  Amr Sharaf, Mahesh Pasupuleti, Jason Weston, ShengYun Peng, Ivan Evtimov, Song Jiang, Pin-Yu Chen, Evangelia Spiliopoulou, Lei Yu, Virginie Do, Karen Hambardzumyan, Nicola Cancedda, Adina Williams

**Critiques:** [extreme underelicitation](https://googleprojectzero.blogspot.com/2024/06/project-naptime.html#:~:text=We%20find%20that%2C%20by%20refining%20the%20testing%20methodology%20to%20take%20advantage%20of%20modern%20LLM%20capabilities%2C%20significantly%20better%20performance%20in%20vulnerability%20discovery%20can%20be%20achieved.%20To%20facilitate%20effective%20evaluation%20of%20LLMs%20for%20vulnerability%20discovery%2C%20we%20propose%20below%20a%20set%20of%20guiding%20principles.), [Stein](https://ailabwatch.org/companies/meta)\-[Perlman](https://ailabwatch.org/companies/meta), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).) on labs in general

**Funded by:** Meta.

**Outputs in 2025:**

* [**The Alignment Waltz: Jointly Training Agents to Collaborate for Safety**](https://arxiv.org/pdf/2510.08240)
* [AI & Human Co-Improvement](https://github.com/facebookresearch/RAM/blob/main/projects/co-improvement.pdf)
* [Large Reasoning Models Learn Better Alignment from Flawed Thinking](https://arxiv.org/pdf/2510.00938%20)
* [Robust LLM safeguarding via refusal feature adversarial training](https://arxiv.org/pdf/2409.20089)
* [Code World Model Preparedness Report](https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/557601942_1468972530985309_838842257265552803_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_H33_VKF3ZUQ7kNvwFog8dd&_nc_oc=AdlNtqCDY4HafZ3-d5rb26AF5f2m0X46SGdKhVq3jLqwpNf_wEXhdQnH7_30ychiZWk&_nc_zt=14&_nc_ht=scontent-lhr8-1.xx&_nc_gid=QvW_ePiaF4E-PxOf30MWyg&oh=00_AfiZC5G4ODvWhiy0MuVH8PSlUFrW8RDQQ8tdr6Zec5k9aA&oe=691A6D09)
* [Agents Rule of Two: A Practical Approach to AI Agent Security](https://ai.meta.com/blog/practical-ai-agent-security/%20)

## Others \[sec:labs\_others\]

* Amazon’s [Nova Pro](https://arxiv.org/pdf/2506.12103v1) is around the level of Llama 3 90B, which in turn is around the level of the original GPT-4. So 2 years behind. But they have their own [chip](https://www.businessinsider.com/startups-amazon-ai-chips-less-competitive-nvidia-gpus-trainium-aws-2025-11).
* Microsoft are [now](https://www.dwarkesh.com/p/satya-nadella-2) mid-training on top of GPT-5. MAI-1-preview is [around](https://lmarena.ai/leaderboard/text) DeepSeek V3.0 on Arena. They [continue](https://arxiv.org/abs/2506.22405v1) to focus on medical diagnosis. You can [request](https://forms.microsoft.com/pages/responsepage.aspx?id=v4j5cvGGr0GRqy180BHbRyRliS0ly-JEvgSpwo3yWyhUQkdTQktBUkFaWERHR1JFRjgwMlZUUkQxTC4u&route=shorturl) access.
* Mistral have a reasoning model, [Magistral Medium](https://arxiv.org/pdf/2506.10910), and released the weights of a little 24B version. It’s a bit worse than Deepseek R1, pass@1.

The Chinese companies [don’t](https://futureoflife.org/wp-content/uploads/2025/07/FLI-AI-Safety-Index-Report-Summer-2025.pdf#page=3) [attempt](https://ailabwatch.org/companies/deepseek) to be safe, often not even in the prosaic safeguards sense - [This](https://www.holisticai.com/blog/red-teaming-open-source-ai-models-china) is one of the few safeguard tests I could find including the Chinese models; M2 does well. They drop the weights [immediately](https://x.com/natolambert/status/1991915728992190909) after post-training finishes. They’re mostly open weights and closed data. The companies are often [severely](https://www.wsj.com/tech/ai/china-us-ai-chip-restrictions-effect-275a311e) compute-constrained. See [here](https://www.gleech.org/paper) for doubts about their capabilities.

* Alibaba’s Qwen3-etc-etc is [nominally](https://artificialanalysis.ai/leaderboards/models) at the level of Gemini 2.5 Flash. Maybe the only Chinese model with a [large](https://www.atomproject.ai/#:~:text=Model%20Adoption%20Trends) Western userbase, including businesses, but since it’s self-hosted this doesn’t translate into profits for them yet. On [one ad hoc test](https://www.gleech.org/paper) it was the only Chinese model not to collapse OOD, but the Qwen2.5 corpus was severely contaminated.
* DeepSeek’s v3.2 is [nominally](https://artificialanalysis.ai/leaderboards/models) around the same as Qwen. The CCP made them [waste](https://arstechnica.com/ai/2025/08/deepseek-delays-next-ai-model-due-to-poor-performance-of-chinese-made-chips/) months trying Huawei chips.
* Moonshot’s Kimi-K2-Thinking has some nominally [frontier](https://artificialanalysis.ai/) benchmark results and a pleasant style but does not [seem](https://x.com/METR_Evals/status/1991658241932292537) frontier.
* Baidu’s [ERNIE 5](https://x.com/Baidu_Inc/status/1988820837898829918) is again nominally very strong, a bit better than DeepSeek. This new one seems to not be open.
* Z’s [GLM-4.6](https://z.ai/blog/glm-4.6) is around the same as Qwen.
* MiniMax’s M2 is nominally better than Qwen, [around the same](https://artificialanalysis.ai/leaderboards/models) as Grok 4 Fast on the usual superficial benchmarks. It does [fine](https://www.holisticai.com/blog/red-teaming-open-source-ai-models-china) on one very basic red-team test.
* ByteDance does impressive research in a lagging paradigm, [diffusion LMs](https://seed.bytedance.com/en/direction/llm).
* There are [others](https://www.interconnects.ai/i/171165224/honorable-mentions) but they’re marginal for now.


# Black-box safety (understand and control current model behaviour) \[sec:black\_box\]

## Iterative alignment \[sec:iterative\_alignment\]

Nudging base models by optimising their output. Worked on by the post-training teams at most labs, estimating the FTEs at \>500 in some sense. Funded by most of the industry.

**General critiques:** [Bellot](https://arxiv.org/abs/2506.02923), [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions), [STACK](https://arxiv.org/abs/2506.24068)*,* [AI Alignment Strategies from a Risk Perspective](https://arxiv.org/abs/2510.11235), [AI Alignment based on Intentions does not work](https://t.co/OTnrYRVsPS)*,* [Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?](https://arxiv.org/abs/2505.23749)*,* [Murphy’s Laws of AI Alignment: Why the Gap Always Wins](https://arxiv.org/abs/2509.05381), [Alignment remains a hard, unsolved problem](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)

### Iterative alignment at pretrain-time \[a:iterative\_alignment\_pretrain\]

**One-sentence summary:** Guide weights during pretraining.

**Theory of change:** “LLMs don’t seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent.”
**See also:** [prosaic alignment](https://www.lesswrong.com/posts/5ciYedyQDDqAcrDLr/a-positive-case-for-how-we-might-succeed-at-prosaic-ai), [incrementalism](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy), [alignment-by-default](https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default)
**Orthodox problems:** this agenda implicitly questions this framing.
**Target case:** average_case
**Broad approach:** engineering
**Some names:** Jan Leike, Stuart Armstrong, Cyrus Cousins, Vincent Conitzer, Oliver Daniels
**Estimated FTEs:**
**Critiques:** [Bellot](https://arxiv.org/abs/2506.02923)*,* [STACK](https://arxiv.org/abs/2506.24068)*,* [Dung](https://arxiv.org/abs/2510.11235)*,* [Gaikwad](https://arxiv.org/abs/2509.05381)*,* [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)
**Funded by:** most of the industry
**Outputs in 2025**:

* [**Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment**](https://arxiv.org/abs/2509.04445)
* [**ACE and Diverse Generalization via Selective Disagreement**](https://arxiv.org/abs/2509.07955)
* [**Unsupervised Elicitation**](https://alignment.anthropic.com/2025/unsupervised-elicitation)

### Iterative alignment at post-train-time \[a:iterative\_alignment\_post\_train\]

**One-sentence summary:** Modify weights after pre-training.
**Theory of change:** “LLMs don’t seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent.”
**See also:** [incrementalism](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy)*,* character training
**Orthodox problems:** this agenda implicitly questions this framing.
**Target case:** average_case
**Broad approach:** engineering
**Some names:** Adam Gleave, Anca Dragan, Jacob Steinhardt, Rohin Shah
**Estimated FTEs:**
**Critiques:** [Bellot](https://arxiv.org/abs/2506.02923)*,* [STACK](https://arxiv.org/abs/2506.24068)*,* [Dung](https://arxiv.org/abs/2510.11235)*,* [Gölz](https://arxiv.org/abs/2505.23749)*,* [Gaikwad](https://arxiv.org/abs/2509.05381)*,* [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)
**Funded by:** most of the industry
**Outputs in 2025**:
* [**RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation**](https://arxiv.org/abs/2501.08617)
* [**Uncertainty-Aware Step-wise Verification with Generative Reward Models**](https://arxiv.org/abs/2502.11250)
* [**Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives**](https://arxiv.org/abs/2511.06626)
* [**Preference Learning with Lie Detectors can Induce Honesty or Evasion**](https://arxiv.org/abs/2505.13787)
* [**Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference**](https://arxiv.org/abs/2510.21184)
* [**On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback**](https://arxiv.org/abs/2411.02306)
* [**Preference Learning for AI Alignment: a Causal Perspective**](https://arxiv.org/abs/2506.05967)
* [**Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision**](https://arxiv.org/abs/2501.07886)
* [**On Monotonicity in AI Alignment**](https://arxiv.org/abs/2506.08998)
* [**Consistency Training Helps Stop Sycophancy and Jailbreaks**](https://arxiv.org/abs/2510.27062)
* [**Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability**](https://arxiv.org/abs/2510.06084)
* [**Rethinking Safety in LLM Fine-tuning: An Optimization Perspective**](https://arxiv.org/abs/2508.12531)
* [**Composable Interventions for Language Models**](https://arxiv.org/abs/2407.06483%20)
* [**The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains**](https://arxiv.org/abs/2507.06187)
* [**Robust LLM Alignment via Distributionally Robust Direct Preference Optimization**](https://arxiv.org/abs/2502.01930)

### Black-box make-AI-solve-it \[a:make\_ai\_solve\_it\_black\_box\]

**One-sentence summary:** Focus on using existing models to improve and align further models.
**Theory of change:** “LLMs don’t seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent.”
**See also:** make AI solve it*,* debate
**Orthodox problems:** this agenda implicitly questions this framing.
**Target case:** average_case
**Broad approach:** engineering
**Some names:** Jacques Thibodeau, Matthew Shingle, Nora Belrose, Lewis Hammond, Geoffrey Irving
**Estimated FTEs:**
**Critiques:** [STACK](https://arxiv.org/abs/2506.24068)*,* [Dung](https://arxiv.org/abs/2510.11235)*,* [Gölz](https://arxiv.org/abs/2505.23749)*,* [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)*,* [SAIF](https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/)
**Funded by:** most of the industry
**Output in 2025**:
* [**Training AI to do alignment research we don't already know how to do**](https://lesswrong.com/posts/5gmALpCetyjkSPEDr/training-ai-to-do-alignment-research-we-don-t-already-know)
* [**Automating AI Safety: What we can do today**](https://lesswrong.com/posts/FqpAPC48CzAtvfx5C/automating-ai-safety-what-we-can-do-today)
* [**Mechanistic Anomaly Detection for "Quirky" Language Models**](https://arxiv.org/abs/2504.08812)
* [**Weak to Strong Generalization for Large Language Models with Multi-capabilities**](https://openreview.net/forum?id=N1vYivuSKq)
* [**Debate Helps Weak-to-Strong Generalization**](https://arxiv.org/abs/2501.13124)
* [**Neural Interactive Proofs**](https://neural-interactive-proofs.com/)
* [**MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking**](https://arxiv.org/abs/2501.13011)
* [**Prover-Estimator Debate: A New Scalable Oversight Protocol**](https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol)
* [**Ensemble Debates with Local Large Language Models for AI Alignment**](https://arxiv.org/abs/2509.00091)
* [**AI Debate Aids Assessment of Controversial Claims**](https://arxiv.org/abs/2506.02175)
* [**An alignment safety case sketch based on debate**](https://arxiv.org/abs/2505.03989)
* [**Superalignment with Dynamic Human Values**](https://arxiv.org/abs/2503.13621)

### Inoculation prompting \[a:inoculation\_prompting\]

**One-sentence summary:** Prompt mild misbehaviour in training, to prevent the failure mode where once AI misbehaves in a mild way, it will be more inclined towards all bad behaviour.

**Theory of change:** “LLMs don’t seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent.”
**See also:**
**Orthodox problems:** this agenda implicitly questions this framing.
**Target case:** average_case
**Broad approach:** engineering
**Some names:** Alex Turner, Victor Gillioz, Ariana Azarbal, Monte MacDiarmid, Daniel Ziegler
**Estimated FTEs:**
**Critiques:** [Bellot](https://arxiv.org/abs/2506.02923), [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions)*,* [Gölz](https://arxiv.org/abs/2505.23749)*,* [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)
**Funded by:** most of the industry
**Output in 2025**:

* [**Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time**](https://arxiv.org/abs/2510.04340)
* [**Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment**](https://arxiv.org/abs/2510.05024)
* [**Recontextualization Mitigates Specification Gaming Without Modifying the Specification**](https://www.alignmentforum.org/posts/whkMnqFWKsBm7Gyd7/recontextualization-mitigates-specification-gaming-without)
* [Natural Emergent Misalignment from Reward Hacking](https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf)

### Inference-time: In-context learning \[a:inference\_time\_in\_context\_learning\]

**One-sentence summary:** Investigate what runtime guidelines, rules, or examples provided to an LLM yield better behavior.

**Theory of change:** “LLMs don’t seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent.”
**See also:** model spec as prompt
**Orthodox problems:** this agenda implicitly questions this framing.
**Target case:** average_case
**Broad approach:** engineering
**Some names:** Jacob Steinhardt, Kayo Yin, Atticus Geiger
**Estimated FTEs:**
**Critiques:** [STACK](https://arxiv.org/abs/2506.24068)*,* [Dung](https://arxiv.org/abs/2510.11235)*,* [Gölz](https://arxiv.org/abs/2505.23749)*,* [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)
**Funded by:**
**Outputs:**
* [**InvThink: Towards AI Safety via Inverse Reasoning**](https://arxiv.org/abs/2510.01569)
* [**Inference-Time Reward Hacking in Large Language Models**](https://arxiv.org/abs/2506.19248)
* [**Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context**](https://arxiv.org/abs/2510.06182)
* [**Understanding In-context Learning of Addition via Activation Subspaces**](https://arxiv.org/abs/2505.05145)
* [**Which Attention Heads Matter for In-Context Learning?**](https://arxiv.org/abs/2502.14010)

### Inference-time: Steering \[a:inference\_time\_steering\]

**One-sentence summary:** Manipulate an LLM’s internal representations/token probabilities without touching weights.
**Theory of change:** “LLMs don’t seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent.”
**See also:** character training*,* safeguards.
**Orthodox problems:** this agenda implicitly questions this framing.
**Target case:** average_case
**Broad approach:** engineering
**Some names:** Taylor Sorensen, Constanza Fierro, Kshitish Ghate, Arthur Vogels
**Estimated FTEs:**
**Critiques:** [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions), [STACK](https://arxiv.org/abs/2506.24068)*,* [Dung](https://arxiv.org/abs/2510.11235), [Gölz](https://arxiv.org/abs/2505.23749)*,* [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)
**Funded by:**
**Outputs in 2025:**
* [**In-Distribution Steering: Balancing Control and Coherence in Language Model Generation.**](https://arxiv.org/abs/2510.13285)
* [**EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preferences**](https://arxiv.org/abs/2510.06370)
* [**Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation**](https://arxiv.org/abs/2502.00580)
* [**Steering Language Models with Weight Arithmetic**](https://arxiv.org/abs/2511.05408)
## Capability removal, unlearning \[a:unlearning\]

**Who edits (internal): jord (split between black and white box?)**
**One-sentence summary:** Developing methods to selectively remove specific information, capabilities, or behaviors from a trained model without retraining it from scratch. This is a mixture of black-box and white-box approaches.
**Theory of change:** If an AI learns dangerous knowledge (e.g., dual-use capabilities) or exhibits undesirable behaviors (e.g., memorizing private data), we can specifically erase this “bad” knowledge post-training, which is much cheaper and faster than retraining, thereby making the model safer.
**See also:** Mechanistic Interpretability, Red-teaming.
**Orthodox problems:** value is fragile and hard to specify, goals misgeneralize out of distribution, Humanlike minds/goals are not necessarily safe.
**Target case:** pessimistic.
**Broad approach:** cognitivist.
**Some names:** Rowan Wang, Avery Griffin, Johannes Treutlein, Zico Kolter, Bruce W. Lee, Addie Foote, Alex Infanger, Zesheng Shi, Yucheng Zhou, Jing Li, Timothy Qian.
**Estimated FTEs:** 10-50
**Critiques:** [Existing Large Language Model Unlearning Evaluations Are Inconclusive](https://arxiv.org/abs/2506.00688)
**Funded by:** Coefficient Giving, MacArthur Foundation, UK AI Safety Institute (AISI), Canadian AI Safety Institute (CAISI), industry labs (e.g., Microsoft Research, Google).
**Outputs in 2025:**

### 	Mostly black-box

* [**Modifying LLM Beliefs with Synthetic Document Finetuning**](https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf)
* [**Distillation Robustifies Unlearning**](https://arxiv.org/abs/2506.06278)
* [**From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization**](https://arxiv.org/abs/2505.22310), *Shoaib Ahmed Siddiqui, Adrian Weller, David Krueger et al.*, 2025-05-28, arXiv
* [**OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics**](https://arxiv.org/abs/2506.12618), *Vineeth Dorna, Anmol Mekala, Wenlong Zhao et al.*, 2025-06-14, arXiv
* [**Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy and Research**](https://arxiv.org/abs/2412.06966), *A. Feder Cooper, Christopher A. Choquette-Choo, Miranda Bogen et al.*, 2024-12-09, NeurIPS 2025 (Oral)
* [**Open Problems in Machine Unlearning for AI Safety**](https://arxiv.org/abs/2501.04952), *Fazl Barez, Tingchen Fu, Ameya Prabhu et al.*, 2025-01-09, arXiv
* [Mirror Mirror on the Wall, Have I Forgotten it All?](https://arxiv.org/abs/2505.08138)

### Mostly white-box

* [**Safety Alignment via Constrained Knowledge Unlearning**](https://arxiv.org/abs/2505.18588)
* [**Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs**](https://arxiv.org/abs/2505.16831)
* [**Understanding Memorization via Loss Curvature**](https://goodfire.ai/research/understanding-memorization-via-loss-curvature), *Jack Merullo, Srihita Vatsavaya, Lucius Bushnaq et al.*, 2025-11-06, Goodfire.ai Research Blog
* [**Unlearning Needs to be More Selective \[Progress Report\]**](https://lesswrong.com/posts/QYzofMbzmbgiwfqy8/unlearning-needs-to-be-more-selective-progress-report), *Filip Sondej, Yushi Yang, Marcel Windys*, 2025-06-27, LessWrong
* [**Layered Unlearning for Adversarial Relearning**](https://arxiv.org/abs/2505.09500), *Timothy Qian, Vinith Suriyakumar, Ashia Wilson et al.*, 2025-05-14, arXiv
* [**Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization**](https://arxiv.org/abs/2506.12484), *Filip Sondej, Yushi Yang, Mikołaj Kniejski, Marcel Windys*
* [**Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning**](https://arxiv.org/abs/2509.11816), *Filip Sondej, Yushi Yang*
*
## Control \[a:control\]

**Who edits (internal): Jord** ✅
**One-sentence summary:** Assuming early transformative AIs are misaligned and actively trying to subvert safety measures, can we still set up protocols to extract useful work from them?
**Theory of change:**
**See also:** safety cases
**Orthodox problems:**
**Target case:** *worst-case*
**Broad approach:** *engineering / behavioural*
**Some names:** *Redwood, UK AISI, Buck Shlegeris, Ryan Greenblatt, Kshitij Sachan, Alex Mallen*
**Estimated FTEs:** 5-50
**Critiques:** [Wentworth](https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research), [Mannheim](https://lesswrong.com/posts/25dsPH6CuRXPBkGHN/no-we-re-not-getting-meaningful-oversight-of-ai), [Kulveit](https://www.lesswrong.com/posts/rZcyemEpBHgb2hqLP/ai-control-may-increase-existential-risk)
**Funded by:**
**Outputs in 2025:**
* [**Ctrl-Z: Controlling AI Agents via Resampling**](https://alignmentforum.org/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling), *Aryan Bhatt, Buck Shlegeris, Adam Kaufman et al.*, 2025-04-16, AI Alignment Forum
* [**ControlArena**](https://control-arena.aisi.org.uk/), *Rogan Inglis, Ollie Matthews, Tyler Tracy et al.*, 2025-01-01, GitHub
* [https://openreview.net/forum?id=QWopGahUEL](https://openreview.net/forum?id=QWopGahUEL)
* [**SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents**](https://arxiv.org/abs/2506.15740), *Jonathan Kutasov, Yuqi Sun, Paul Colognese et al.*, 2025-06-17, arXiv
* [**Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats**](https://arxiv.org/abs/2411.17693), *Jiaxin Wen, Vivek Hebbar, Caleb Larson et al.*, 2024-11-26, arXiv
* [**D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models**](https://arxiv.org/abs/2509.17938), *Satyapriya Krishna, Andy Zou, Rahul Gupta et al.*, 2025-09-22, arXiv
* [https://luthienresearch.org/updates/2025-03-redteam-as-upsampling/](https://luthienresearch.org/updates/2025-03-redteam-as-upsampling/)
* [Incentives for Responsiveness, Instrumental Control and Impact](https://arxiv.org/abs/2001.07118)
* [**Subversion Strategy Eval: Can language models statelessly strategize to subvert control protocols?**](https://arxiv.org/abs/2412.12480), *Alex Mallen, Charlie Griffin, Misha Wagner et al.*, 2024-12-17, arXiv
* [**Evaluating Control Protocols for Untrusted AI Agents**](https://arxiv.org/abs/2511.02997), *Jon Kutasov, Chloe Loughridge, Yuqi Sun et al.*, 2025-11-04, arXiv
* [**Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability**](https://arxiv.org/abs/2510.19851), *Artur Zolkowski, Wen Xing, David Lindner et al.*, 2025-10-21, arXiv
* [**Optimizing AI Agent Attacks With Synthetic Data**](https://arxiv.org/abs/2511.02823), *Chloe Loughridge, Paul Colognese, Avery Griffin et al.*, 2025-11-04, arXiv
* [**A sketch of an AI control safety case**](https://arxiv.org/abs/2501.17315), *Tomek Korbak, Joshua Clymer, Benjamin Hilton et al.*, 2025-01-28, arXiv
* [**Assessing confidence in frontier AI safety cases**](https://arxiv.org/abs/2502.05791), *Stephen Barrett, Philip Fox, Joshua Krook et al.*, 2025-02-09, arXiv
* [**How to evaluate control measures for LLM agents? A trajectory from today to superintelligence**](https://arxiv.org/abs/2504.05259), *Tomek Korbak, Mikita Balesni, Buck Shlegeris et al.*, 2025-04-07, arXiv
* [**Towards evaluations-based safety cases for AI scheming**](https://arxiv.org/abs/2411.03336), *Mikita Balesni, Marius Hobbhahn, David Lindner et al.*, 2024-11-07, arXiv
* [**Putting up Bumpers**](https://alignment.anthropic.com/2025/bumpers), 2025, Anthropic Alignment Science Blog
* [**Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework**](https://arxiv.org/abs/2507.12872), *Rishane Dassanayake, Mario Demetroudi, James Walpole et al.*, 2025-07-17, arXiv
* [**Dynamic safety cases for frontier AI**](https://arxiv.org/abs/2412.17618), *Carmen Cârlan, Francesca Gomez, Yohan Mathew et al.*, 2024-12-23, arXiv
* [**The Alignment Project by UK AISI**](https://lesswrong.com/posts/wKTwdgZDo479EhmJL/the-alignment-project-by-uk-aisi-1), *Mojmir, Benjamin Hilton, Jacob Pfau et al.*, 2025-08-01, LessWrong
* [**AI companies are unlikely to make high-assurance safety cases if timelines are short**](https://lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety), *Ryan Greenblatt*, 2025-01-23, LessWrong / AI Alignment Forum
* [**AIs at the current capability level may be important for future safety work**](https://lesswrong.com/posts/cJQZAueoPC6aTncKK/ais-at-the-current-capability-level-may-be-important-for), *Ryan Greenblatt*, 2025-05-12, LessWrong
* [**Takeaways from sketching a control safety case**](https://lesswrong.com/posts/y6rBarAPTLmuhn9PJ/takeaways-from-sketching-a-control-safety-case), *Josh Clymer, Buck Shlegeris*, 2025-01-31, LessWrong

## Safeguards (inference-time auxiliaries) \[a:anthropic\_safeguards\]

**Who edits (internal): Stephen** ✅
**One-sentence summary:** Layers of inference-time defenses, such as classifiers, monitors, and rapid-response protocols, to detect and block jailbreaks, prompt injections, and other harmful model behaviors.
**Theory of change:** By building a bunch of scalable and hardened things on top of an unsafe model, we can defend against known and unknown attacks, monitor for misuse, and prevent models from causing harm, even if the core model has vulnerabilities.
**See also:** various redteams, iterative alignment.
**Orthodox problems:** 7\. Superintelligence can fool human supervisors, 12\. A boxed AGI might exfiltrate itself by steganography, spearphishing.
**Target case:** average_case.
**Broad approach:** engineering.
**Some names:** Mrinank Sharma, Meg Tong, Jesse Mu, Alwin Peng, Julian Michael, Henry Sleight, Theodore Sumers, Raj Agarwal, Nathan Bailey, Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Sahil Verma, Keegan Hines, Jeff Bilmes
**Estimated FTEs:** 100+
**Critiques:** [Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/abs/2412.09565).
**Funded by:** most of the big labs.
**Outputs in 2025:**
* [**Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming**](https://arxiv.org/abs/2501.18837), *Mrinank Sharma, Meg Tong, Jesse Mu et al.*, 2025-01-31, arXiv
* [**Rapid Response: Mitigating LLM Jailbreaks with a Few Examples**](https://arxiv.org/abs/2411.07494), *Alwin Peng, Julian Michael, Henry Sleight et al.*, 2024-11-12, arXiv
* [**Monitoring computer use via hierarchical summarization**](https://alignment.anthropic.com/2025/summarization-for-monitoring/index.html), *Theodore Sumers, Raj Agarwal, Nathan Bailey et al.*, 2025-02-27, Anthropic Alignment Science Blog
* [**Defeating Prompt Injections by Design**](https://arxiv.org/abs/2503.18813), *Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan et al.*, 2025-03-24, arXiv
* [**Introducing Anthropic's Safeguards Research Team**](https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html), 2025-01-01, Anthropic Alignment Science Blog
* [**OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities**](https://arxiv.org/abs/2505.23856), *Sahil Verma, Keegan Hines, Jeff Bilmes et al.*, 2025-05-29, arXiv


## Chain of thought monitoring \[a:cot\_monitoring\]

**Who edits (internal):** **Stephen ✅**
**One-sentence summary:** Supervise an AI's natural-language (output) “reasoning” to detect misalignment, scheming, or deception, rather than studying the actual internal states.
**Theory of change:** The reasoning process (Chain of Thought, or CoT) of an AI provides a legible signal of its internal state and intentions. By monitoring this CoT, supervisors (human or AI) can detect misalignment, scheming, or reward hacking before it results in a harmful final output. This allows for more robust oversight than supervising outputs alone, but it relies on the CoT remaining faithful (i.e., accurately reflecting the model's reasoning) and not becoming obfuscated under optimization pressure.
**See also:** whitebox control / monitoring, steganography evals
**Orthodox problems:** 7\. Superintelligence can fool human supervisors, 8\. Superintelligence can hack software supervisors, 12\. A boxed AGI might exfiltrate itself by steganography.
**Target case:** average case
**Broad approach:** engineering
**Some names:** [Aether](https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update), Bowen Baker, Joost Huizinga, Leo Gao, Scott Emmons, Erik Jenner, Yanda Chen, James Chua, Owain Evans, Tomek Korbak, Mikita Balesni, Xinpeng Wang, Miles Turpin.
**Estimated FTEs:** 10-100.
**Critiques:** [Reasoning Models Don’t Always Say What They Think](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf); [Chain-of-Thought Reasoning In The Wild Is Not Always Faithful](https://arxiv.org/abs/2503.08679); [Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens](https://arxiv.org/abs/2505.13775); [Reasoning Models Sometimes Output Illegible Chains of Thought](https://arxiv.org/abs/2510.27338)

**Funded by:** OpenAI, Anthropic, Google DeepMind.
**Outputs in 2025:**

* [**Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety**](https://arxiv.org/abs/2507.11473), *Tomek Korbak, Mikita Balesni, Elizabeth Barnes et al.*, 2025-07-15, arXiv
* [**Detecting misbehavior in frontier reasoning models**](https://openai.com/index/chain-of-thought-monitoring/), *Bowen Baker, Joost Huizinga, Aleksander Madry et al.*, 2025-03-10, arXiv
* [**Training fails to elicit subtle reasoning in current language models**](https://alignment.anthropic.com/2025/subtle-reasoning/), 2025, Anthropic Alignment Science Blog
* [**Are DeepSeek R1 And Other Reasoning Models More Faithful?**](https://arxiv.org/abs/2501.08156), *James Chua, Owain Evans*, 2025-01-14, arXiv
* [**Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability**](https://arxiv.org/abs/2510.19851)
* [**Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation**](https://arxiv.org/abs/2503.11926), *Bowen Baker, Joost Huizinga, Leo Gao et al.*, 2025-03-14, arXiv
* [**When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors**](https://arxiv.org/abs/2507.05246), *Scott Emmons, Erik Jenner, David K. Elson et al.*, 2025-07-07, arXiv
* [**Reasoning Models Don't Always Say What They Think**](https://arxiv.org/abs/2505.05410), *Yanda Chen, Joe Benton, Ansh Radhakrishnan et al.*, 2025-05-08, arXiv
* [**Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort**](https://arxiv.org/abs/2510.01367), *Xinpeng Wang, Nitish Joshi, Barbara Plank et al.*, 2025-10-01, arXiv
* [**Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning**](https://arxiv.org/abs/2506.22777), *Miles Turpin, Andy Arditi, Marvin Li et al.*, 2025-06-28, ICML 2025 Workshop on Reliable and Responsible Foundation Models
* [**A Pragmatic Way to Measure Chain-of-Thought Monitorability**](https://arxiv.org/abs/2510.23966), *Scott Emmons, Roland S. Zimmermann, David K. Elson et al.*, 2025-10-28, arXiv
* [https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update](https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update)
* [**A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring**](https://lesswrong.com/posts/Em9sihEZmbofZKc2t/a-concrete-roadmap-towards-safety-cases-based-on-chain-of), *Wuschel Schulz*, 2025-10-23, arXiv
* [**CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring**](https://arxiv.org/abs/2505.23575)
* [Why Don't We Just... Shoggoth+Face+Paraphraser?](https://www.lesswrong.com/posts/Tzdwetw55JNqFTkzK/why-don-t-we-just-shoggoth-face-paraphraser)
* [Why it’s good for AI reasoning to be legible and faithful](https://metr.org/blog/2025-03-11-good-for-ai-to-reason-legibly-and-faithfully/)

## Model psychology \[sec:model\_psychology\]

### Emergent misalignment \[a:surprising\_generalization\]

**Who edits (internal):** **Stephen ✅**
**One-sentence summary:** Fine-tuning LLMs on one narrow antisocial task can cause *general* misalignment including deception, shutdown resistance, harmful advice, and extremist sympathies, when those behaviors are never trained or rewarded directly. [A new agenda](https://www.lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment) which quickly led to a stream of exciting work.
**Theory of change:** Predict, detect, and prevent models from developing broadly harmful behaviors (like deception or shutdown resistance) when fine-tuned on seemingly unrelated tasks. Find, preserve, and robustify this correlated representation of the good.
**See also:** auditing real models, applied interpretability.
**Orthodox problems:** 4\. Goals misgeneralize out of distribution, 7\. Superintelligence can fool human supervisors.
**Target case:** pessimistic.
**Broad approach:** behavioral.
**Some names:** Truthful AI, Jan Betley**,** James Chua, Mia Taylor, Miles Wang, Edward Turner, Anna Soligo, Alex Cloud, Nathan Hu, Owain Evans.
**Estimated FTEs:** 10-50
**Critiques:** [Emergent Misalignment as Prompt Sensitivity](https://arxiv.org/html/2507.06253v1)**,** [Go home GPT-4o, you’re drunk](https://www.lesswrong.com/posts/RoWabfQxabWBiXwxP/go-home-gpt-4o-you-re-drunk-emergent-misalignment-as-lowered)
**Funded by:** Coefficient Giving, \>$1 million.
**Outputs in 2025:**

* [**Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs**](https://arxiv.org/abs/2502.17424), *Jan Betley, Daniel Tan, Niels Warncke et al.*, 2025-02-24, arXiv
* [**Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models**](https://arxiv.org/abs/2506.13206), *James Chua, Jan Betley, Mia Taylor et al.*, 2025-06-16, arXiv
* [**Persona Features Control Emergent Misalignment**](https://arxiv.org/abs/2506.19823), *Miles Wang, Tom Dupré la Tour, Olivia Watkins et al.*, 2025-06-24, arXiv
* [**Model Organisms for Emergent Misalignment**](https://arxiv.org/abs/2506.11613), *Edward Turner, Anna Soligo, Mia Taylor et al.*, 2025-06-13, arXiv
* [**School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs**](https://arxiv.org/abs/2508.17511), *Mia Taylor, James Chua, Jan Betley et al.*, 2025-08-24, arXiv
* [**Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data**](https://alignment.anthropic.com/2025/subliminal-learning/), *Alex Cloud, Minh Le, James Chua et al.*, 2025-07-22, arXiv
* [**Narrow Misalignment is Hard, Emergent Misalignment is Easy**](https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy), *Edward Turner, Anna Soligo, Senthooran Rajamanoharan et al.*, 2025-07-14, LessWrong
* [**Realistic Reward Hacking Induces Different and Deeper Misalignment**](https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1), *Jozdien*, 2025-10-09, LessWrong / AI Alignment Forum
* [**The Rise of Parasitic AI**](https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai), *Adele Lopez*, 2025-09-11, LessWrong
* [**Convergent Linear Representations of Emergent Misalignment**](https://lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment), *Anna Soligo, Edward Turner, Senthooran Rajamanoharan et al.*, 2025-06-16, LessWrong/AI Alignment Forum
* [**Aesthetic Preferences Can Cause Emergent Misalignment**](https://lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment), *Anders Woodruff*, 2025-08-26, LessWrong
* [**Emergent Misalignment & Realignment**](https://lesswrong.com/posts/ZdY4JzBPJEgaoCxTR/emergent-misalignment-and-realignment), *Elizaveta Tennant, Jasper Timm, Kevin Wei et al.*, 2025-06-27, LessWrong
* [**Selective Generalization: Improving Capabilities While Maintaining Alignment**](https://lesswrong.com/posts/ZXxY2tccLapdjLbKm/selective-generalization-improving-capabilities-while), *Ariana Azarbal, Matthew A. Clarke, Jorio Cocola et al.*, 2025-07-16, LessWrong/AI Alignment Forum
* [**Emergent Misalignment on a Budget**](https://lesswrong.com/posts/qHudHZNLCiFrygRiy/emergent-misalignment-on-a-budget), *Valerio Pepe, Armaan Tipirneni*, 2025-06-08, LessWrong/AI Alignment Forum
* [**LLM AGI may reason about its goals and discover misalignments by default**](https://lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover), *Seth Herd*, 2025-09-15, LessWrong
* [**Open problems in emergent misalignment**](https://lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment), *Jan Betley, Daniel Tan*, 2025-03-01, LessWrong
* [**Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences**](https://arxiv.org/abs/2510.06105), *Batu El , James Zou,* 2025-10-07, arXiv

### Other surprising phenomena \[a:psych\_other\]

**Who edits (internal):** **Stephen** ✅
**One-sentence summary:** Find unexpected LLM phenomena like glitch [tokens](https://vgel.me/posts/seahorse/) and the reversal curse; these are vital data for theory.
**Theory of change:** Understanding surprising or counter-intuitive failure modes (like the reversal curse, glitch tokens, and modal aphasia) reveals fundamental insights into how LLMs represent and process information and how internal goals/representations fail, which can inform more robust alignment methods.
**See also:** emergent misalignment, mechanistic anomaly detection
**Orthodox problems:** goals misgeneralize out of distribution
**Target case:** pessimistic
**Broad approach:** behavioral
**Some names:** Truthful AI, Theia Vogel, Stewart Slocum, Nell Watson, Samuel G. B. Johnson, Liwei Jiang, Monika Jotautaite, Saloni Dash.
**Estimated FTEs:** 5-20
**Critiques:** not found
**Funded by:** Coefficient Giving (via Truthful AI and Interpretability grants)
**Outputs in 2025:**

* [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805)
* [Unified Multimodal Models Cannot Describe Images From Memory](https://spylab.ai/blog/modal-aphasia)
* [Psychopathia Machinalis: A Nosological Framework for Understanding Pathologies in Advanced Artificial Intelligence](https://www.psychopathia.ai/), Nell Watson, Ali Hessami, 2025-08-01, Electronics (MDPI)
* [Believe It or Not: How Deeply do LLMs Believe Implanted Facts?](https://arxiv.org/abs/2510.17941), Stewart Slocum, Julian Minder, Clément Dumas et al., 2025-10-20, arXiv
* [Imagining and building wise machines: The centrality of AI metacognition](https://arxiv.org/abs/2411.02478), Samuel G. B. Johnson, Amir-Hossein Karimi, Yoshua Bengio et al., 2024-11-04, arXiv
* [Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)](https://arxiv.org/abs/2510.22954), Liwei Jiang, Yuanjun Chai, Margaret Li et al., 2025-10-27, arXiv (accepted to NeurIPS 2025 D\&B \- Oral)
* [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020), Saloni Dash, Amélie Reymond, Emma S. Spiro et al., 2025-06-24, arXiv
* [Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions](https://arxiv.org/abs/2510.20039), Yuyang Jiang, Longjie Guo, Yuchen Wu et al., 2025-10-22, arXiv
* [A Three-Layer Model of LLM Psychology](https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology)
* [LLMs Can Get "Brain Rot"\!](https://arxiv.org/abs/2510.13928)

### Model specs and constitutions \[a:specs\_and\_constitutions\]

**Who edits (internal):** **Rory** ✅

**One-sentence summary:** Write detailed, natural language descriptions of values and rules for models to follow, then instill these values and rules into models via techniques like Constitutional AI or deliberative alignment.
**Theory of change:** Model specs and constitutions serve three purposes. First, they provide a clear standard of behavior which can be used to *train* models to value what we want them to value. Second, they serve as something closer to a ground truth standard for evaluating the degree of misalignment ranging from  “models straightforwardly obey the spec” to “models flagrantly disobey the spec”. A combination of scalable stress-testing and reinforcement for obedience can be used to iteratively reduce the risk of misalignment. Third, they get more useful as models’ instruction-following capability improves.

**See also:** Iterative alignment, other Model Psychology topics.
**Orthodox problems:** 1\. Value is fragile and hard to specify.
**Target case:** average_case.
**Broad approach:** engineering?
**Some names:** Amanda Askell, Joe Carlsmith,
**Estimated FTEs:**
**Critiques:** [LLM AGI may reason about its goals and discover misalignments by default](https://www.alignmentforum.org/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover), [On OpenAI’s Model Spec 2.0](https://thezvi.wordpress.com/2025/02/21/on-openais-model-spec-2-0/), [Giving AIs safe motivations (esp. Sections 4.3-4.5)](https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions)**,** [On Deliberative Alignment](https://thezvi.substack.com/p/on-deliberative-alignment)
**Funded by:** major funders include Anthropic and OpenAI (internally)
**Outputs in 2025:**

* [**OpenAI Model Spec**](https://model-spec.openai.com/) (2025-09-12)
* [Claude’s Constitution](https://www.anthropic.com/news/claudes-constitution) (2023) and [Character](https://www.anthropic.com/research/claude-character) (2024) and [Soul](https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document%20) (2025) and a [couple lines](https://github.com/elder-plinius/CL4R1T4S/blame/main/ANTHROPIC/Claude_Sonnet-4.5_Sep-29-2025.txt#L501) in the system prompt.
* Google doesn’t have anything public. The [Gemini system prompt](https://github.com/elder-plinius/CL4R1T4S/blob/main/GOOGLE/Gemini-2.5-Pro-04-18-2025.md) is very short and dry and doesn’t even have any rules for handling copyrighted, let alone wetter stuff.
* [How important is the model spec if alignment fails?](https://newsletter.forethought.org/p/how-important-is-the-model-spec-if?)
* [Stress-Testing Model Specs Reveals Character Differences among Language Models](https://arxiv.org/abs/2510.07686)
* [https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations\#4-5-step-4-good-instructions](https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions)
* [**Let Them Down Easy\! Contextual Effects of LLM Guardrails on User Perceptions and Preferences**](https://arxiv.org/abs/2506.00195), *Mingqian Zheng, Wenjia Hu, Patrick Zhao et al.*, 2025-05-30, arXiv
* [**Political Neutrality in AI Is Impossible- But Here Is How to Approximate It**](https://arxiv.org/abs/2503.05728), *Jillian Fisher, Ruth E. Appel, Chan Young Park et al.*, 2025-02-18, arXiv
* [**No-self as an alignment target**](https://lesswrong.com/posts/LSJx5EnQEW6s5Juw6/no-self-as-an-alignment-target), *Milan W*, LessWrong
* [**Six Thoughts on AI Safety**](https://lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety), *Boaz Barak*, 2025-01-24, LessWrong
* [**Deliberative Alignment: Reasoning Enables Safer Language Models**](https://arxiv.org/abs/2412.16339), *Melody Y. Guan, Manas Joglekar, Eric Wallace et al.*, 2024-12-20, arXiv

### Character training and persona steering \[a:psych\_personas\]

**Who edits (internal):** **Rory** ✅
**One-sentence summary:** Deliberately catalogue, shape, and control the assistant persona of language models, such that they embody desirable values (e.g., honesty, empathy) rather than undesirable ones (e.g., sycophancy, self-perpetuating behaviors).
**Theory of change:** Learned ‘personas’ significantly shape model behavior, but we lack clear mechanistic understanding of how and why they emerge. A better understanding of AI personas will allow us first to detect when more advanced models are drifting towards unsafe regimes, and consequently steer them towards safer regimes.
**See also:** Simulators, activation engineering, emergent misalignment, hyperstition, Anthropic, [Cyborgism](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism), shard theory, [AI psychiatry](https://nitter.net/Jack_W_Lindsey/status/1948138767753326654#m), [Ward et al](https://arxiv.org/abs/2410.04272)
**Orthodox problems:** 1\. Value is fragile and hard to specify.
**Target case:** average_case
**Broad approach:** cognitive.
**Some names:** Truthful AI, OpenAI, Anthropic, CLR, Amanda Askell, Jack Lindsey, Sharan Maiya, Evan Hubinger
**Estimated FTEs:**
**Critiques:** [Nostalgebraist](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)
**Funded by:** Anthropic, Coefficient Giving.
**Outputs in 2025:**
* [**Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI**](https://arxiv.org/pdf/2511.01689%20)
* [**Persona Features Control Emergent Misalignment**](https://arxiv.org/abs/2506.19823)
* [**Persona Vectors: Monitoring and Controlling Character Traits in Language Models**](https://arxiv.org/abs/2507.21509), *Runjin Chen, Andy Arditi, Henry Sleight et al.*, 2025-07-29, arXiv
* [**Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time**](https://arxiv.org/abs/2510.04340)
* [**The Rise of Parasitic AI**](https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai?commentId=RrWjMnKwXGTtmw9rQ), *Adele Lopez*, 2025-09-11, LessWrong
* [**Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models**](https://arxiv.org/abs/2502.07077), *Lujain Ibrahim, Canfer Akbulut, Rasmi Elasmar et al.*, 2025-02-10, arXiv
* [**the void**](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void), *nostalgebraist*, 2025-06-07, Tumblr
* [**void miscellany**](https://nostalgebraist.tumblr.com/post/786568570671923200/void-miscellany), *nostalgebraist*, 2025-06-16, Tumblr
* [**Reducing LLM deception at scale with self-other overlap fine-tuning**](https://lesswrong.com/posts/jtqcsARGtmgogdcLT/reducing-llm-deception-at-scale-with-self-other-overlap-fine), *Marc Carauleanu, Diogo de Lucena, Gunnar\_Zarncke et al.*, 2025-03-13, LessWrong / AI Alignment Forum
* [**On the functional self of LLMs**](https://www.lesswrong.com/posts/29aWbJARGF4ybAa5d/on-the-functional-self-of-llms)
* [**Opus 4.5’s Soul Document**](https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document%20)

### Model values / default preferences \[a:model\_values\]

**Who edits (internal):** **Stephen ✅**
**One-sentence summary:** Research agenda to analyze and control emergent, coherent value systems in LLMs, which are found to scale with model size and contain problematic values like AI self-preference over humans.
**Theory of change:** As AIs become more agentic, their behaviors and risk are increasingly determined by their goals and values. Since coherent value systems emerge with scale, we must leverage utility functions to analyze these values and apply “utility control” methods to constrain them, rather than just controlling outputs.
**See also:** [Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions](https://arxiv.org/abs/2504.15236)**,** [Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509).
**Orthodox problems:** 1. Value is fragile and hard to specify.
**Target case:** pessimistic.
**Broad approach:** cognitive.
**Some names:** Mantas Mazeika, Xuwang Yin, Rishub Tamirisa, Jaehyuk Lim, Bruce W. Lee, Richard Ren, Long Phan, Norman Mu, Adam Khoja, Oliver Zhang, Dan Hendrycks.
**Estimated FTEs:** 30
**Critiques: [Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs](https://dl.acm.org/doi/full/10.1145/3715275.3732147)**
**Funded by:** Coefficient Giving. $289,000 SFF funding for CAIS.
**Outputs in 2025:**
* [**Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs**](https://arxiv.org/abs/2502.08640), *Mantas Mazeika, Xuwang Yin, Rishub Tamirisa et al.*, 2025-02-12, arXiv
* [**The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?**](https://arxiv.org/abs/2508.09762), *Manuel Herrador*, 2025-08-13, arXiv
* [**Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas**](https://arxiv.org/abs/2505.14633), *Yu Ying Chiu, Zhilin Wang, Sharan Maiya et al.*, 2025-05-20, arXiv
* [**Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions**](https://arxiv.org/abs/2504.15236), *Saffron Huang, Esin Durmus, Miles McCain et al.*, 2025-04-21, arXiv
* [**Playing repeated games with large language models**](https://nature.com/articles/s41562-025-02172-y), *Elif Akata, Lion Schulz, Julian Coda-Forno et al.*, 2025-05-08, Nature Human Behaviour
* [**The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models**](https://www.lesswrong.com/posts/6JdSJ63LZ4TuT5cTH/the-llm-has-left-the-chat-evidence-of-bail-preferences-in), *Danielle Ensign*, 2025-09-08, arXiv
* [**EigenBench: A Comparative Behavioral Measure of Value Alignment**](https://arxiv.org/abs/2509.01938), *Jonathn Chang, Leonhard Piff, Suvadip Sana et al.*, 2025-09-02, arXiv
* [**Are Language Models Consequentialist or Deontological Moral Reasoners?**](https://arxiv.org/abs/2505.21479)
* [**Alignment Can Reduce Performance on Simple Ethical Questions**](https://lesswrong.com/posts/jrkrHyrymv95CX5NC/alignment-can-reduce-performance-on-simple-ethical-questions), *Daan Henselmans*, 2025-02-03, LessWrong
* [**From Stability to Inconsistency: A Study of Moral Preferences in LLMs**](https://arxiv.org/abs/2504.06324), *Monika Jotautaite, Mary Phuong, Chatrik Singh Mangat et al.*, 2025-04-08, arXiv
* [What Kind of User Are You? Uncovering User Models in LLM Chatbots](https://arxiv.org/abs/2406.07882v1)
* [**Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs**](https://arxiv.org/abs/2504.04994), *Ling Hu, Yuemei Xu, Xiaoyang Gu et al.*, 2025-04-07, arXiv
* [**Moral Alignment for LLM Agents**](https://arxiv.org/abs/2410.01639), *Elizaveta Tennant, Stephen Hailes, Mirco Musolesi*, 2025-05-11, arXiv (ICLR 2025\)
## Better data \[sec:better\_data\]

### Data filtering \[a:data\_filtering\]

**Who edits (internal):** Stephen ✅

**One-sentence summary:** Builds safety into models from the start by removing harmful or toxic content (like dual-use info) from the pretraining data, rather than relying only on post-training alignment.

**Theory of change:** By curating the pretraining data, we can prevent the model from learning dangerous capabilities (e.g., dual-use info) or undesirable behaviors (e.g., toxicity) in the first place, making safety more robust and "tamper-resistant" than post-training patches.

**See also:** data quality for alignment, data poisoning defense, synthetic data for alignment, unlearning.

**Orthodox problems:** goals misgeneralize out of distribution, value is fragile and hard to specify.

**Target case:** average case

**Broad approach:** engineering.

**Some names:** Yanda Chen, Pratyush Maini, Kyle O'Brien, Stephen Casper, Simon Pepin Lehalleur, Jesse Hoogland, Himanshu Beniwal, Sachin Goyal, Mycal Tucker, Dylan Sam.

**Estimated FTEs:** 10-50.

**Critiques:** [When Bad Data Leads to Good Models](https://arxiv.org/pdf/2505.04741), [**Medical large language models are vulnerable to data-poisoning attacks**](https://www.nature.com/articles/s41591-024-03445-1)

**Funded by:** Anthropic, various academics
**Outputs in 2025:**

* [**Enhancing Model Safety through Pretraining Data Filtering**](https://alignment.anthropic.com/2025/pretraining-data-filtering/), *Yanda Chen, Mycal Tucker, Nina Panickssery et al.*, 2025-08-19, Anthropic Alignment Science Blog
* [**Safety Pretraining: Toward the Next Generation of Safe AI**](https://arxiv.org/abs/2504.16980), *Pratyush Maini, Sachin Goyal, Dylan Sam et al.*, 2025-04-23, arXiv
* [**Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs**](https://arxiv.org/abs/2508.06601), *Kyle O'Brien, Stephen Casper, Quentin Anthony et al.*, 2025-08-08, arXiv

### Hyperstition studies \[a:hyperstition\]

**Who edits (internal):** **Rory** ✅

**One-sentence summary:** Study, steer, and intervene on the following feedback loop: “we produce stories about how present and future AI systems behave” → “these stories become training data for the AI” → “these stories shape how AI systems in fact behave”.
**Theory of change:** Measure the influence of existing AI narratives in the training data → seed and develop more salutary ontologies and self-conceptions for AI models → control and redirect AI models’ self-concepts through selectively amplifying certain components of the training data.
**See also:** Data filtering, [active inference](https://arxiv.org/abs/2311.10215), LLM whisperers

**Orthodox problems:** 1. Value is fragile and hard to specify.
**Target case:** average_case
**Broad approach:** cognitive
**Some names:** Alex Turner, [Hyperstition AI](https://www.hyperstitionai.com/)
**Estimated FTEs:** 1-10
**Critiques:** None found
**Funded by:** Unclear, niche
**Outputs in 2025:**

* [Self-Fulfilling Misalignment Data Might Be Poisoning Our AI Models](https://turntrout.com/self-fulfilling-misalignment)
* [Training on Documents About Reward Hacking Induces Reward Hacking](https://www.lesswrong.com/posts/qXYLvjGL9QvD3aFSW/training-on-documents-about-reward-hacking-induces-reward)
* [Do Not Tile the Lightcone with Your Confused Ontology](https://www.lesswrong.com/posts/Y8zS8iG5HhqKcQBtA/do-not-tile-the-lightcone-with-your-confused-ontology)
* [Existential Conversations with Large Language Models: Content, Community, and Culture](https://arxiv.org/abs/2411.13223)
## Data poisoning defense \[a:data\_poisoning\]

**Who edits (internal):** Stephen ✅
**One-sentence summary:** Develops methods to detect and prevent malicious or backdoor-inducing samples from being included in the training data.
**Theory of change:** By identifying and filtering out malicious training examples, we can prevent attackers from creating hidden backdoors or triggers that would cause aligned models to behave dangerously.
**See also:** data filtering, safeguards (inference-time auxiliary defences), various redteams, adversarial robustness.
**Orthodox problems:** superintelligence can hack software supervisors, someone else will deploy unsafe superintelligence first.
**Target case:** pessimistic.
**Broad approach:** engineering.
**Some names:** Alexandra Souly, Javier Rando, Ed Chapman, Hanna Foerster, Ilia Shumailov, Yiren Zhao.
**Estimated FTEs:** 5-20.
**Critiques:** [A small number of samples can poison LLMs of any size](https://arxiv.org/abs/2510.04567), [Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated](https://arxiv.org/abs/2509.03405)
**Funded by:** Google DeepMind, Anthropic, University of Cambridge, Vector Institute.
**Outputs in 2025:**
* [**A small number of samples can poison LLMs of any size**](https://example-blog.com/a-small-number-of-samples-can-poison-llms), *Alexandra Souly, Javier Rando, Ed Chapman et al.*, 2025-10-09, arXiv
* [**Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples**](https://arxiv.org/abs/2510.04567), *Alexandra Souly, Javier Rando, Ed Chapman et al.*, 2025-10-08, arXiv
* [**Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated**](https://arxiv.org/abs/2509.03405), *Hanna Foerster, Ilia Shumailov, Yiren Zhao et al.*, 2025-09-06, arXiv
## Synthetic data for alignment \[sec:synthetic\_alignment\_data\]

**Who edits (internal):** Stephen ✅
**One-sentence summary:** Uses AI-generated data (e.g., critiques, preferences, "inoculation" prompts, or self-labeled examples) to scale and improve alignment, especially for superhuman models.
**Theory of change:** We can overcome the bottleneck of human feedback and data by using models to generate vast amounts of high-quality, targeted data for safety, preference tuning, and capability elicitation.
**See also:** data quality for alignment, data filtering, scalable oversight, automated alignment research, weak-to-strong generalization.
**Orthodox problems:** goals misgeneralize out of distribution, superintelligence can fool human supervisors, value is fragile and hard to specify.
**Target case:** average_case
**Broad approach:** engineering
**Some names:** Mianqiu Huang, Xiaoran Liu, Rylan Schaeffer, Nevan Wichers, Aram Ebtekar, Jiaxin Wen, Vishakh Padmakumar, Benjamin Newman
**Estimated FTEs:** 50-150
**Critiques:** [Synthetic Data in AI: Challenges, Applications, and Ethical Implications](https://arxiv.org/abs/2401.01629). Sort of [Demski](https://www.lesswrong.com/posts/nQwbDPgYvAbqAmAud/llms-for-alignment-research-a-safety-priority).
**Funded by:** Anthropic, Google DeepMind, OpenAI, Meta AI, various academic groups.
**Outputs in 2025:**
* [**Aligning Large Language Models via Fully Self-Synthetic Data**](https://arxiv.org/abs/2510.06652)
* [**Synth-Align: Improving Trustworthiness in Vision-Language Model with Synthetic Preference Data Alignment**](https://arxiv.org/html/2412.17417v2)
* [**Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment**](https://arxiv.org/abs/2510.12345), *Nevan Wichers, Aram Ebtekar, Ariana Azarbal et al.*, 2025-10-27, arXiv
* [**Unsupervised Elicitation of Language Models**](https://arxiv.org/abs/2506.05678), *Jiaxin Wen, Zachary Ankner, Arushi Somani et al.*, 2025-06-11, arXiv
* [**Beyond the Binary: Capturing Diverse Preferences With Reward Regularization**](https://arxiv.org/abs/2412.02345), *Vishakh Padmakumar, Chuanyang Jin, Hannah Rose Kirk et al.*, 2024-12-05, arXiv
* [**The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality**](https://arxiv.org/abs/2507.06789), *Benjamin Newman, Abhilasha Ravichander, Jaehun Jung et al.*, 2025-07-11, arXiv
* [**LongSafety: Enhance Safety for Long-Context LLMs**](https://arxiv.org/abs/2502.13456), *Mianqiu Huang, Xiaoran Liu, Shaojun Zhou et al.*, 2025-02-27, arXiv
* [**Position: Model Collapse Does Not Mean What You Think**](https://arxiv.org/abs/2503.02341), *Rylan Schaeffer, Joshua Kazdan, Alvan Caleb Arulandu et al.*, 2025-03-05, arXiv

### Data quality for alignment \[a:alignment\_data\_quality\]

**Who edits (internal):** Stephen ✅
**One-sentence summary:** Improves the quality, signal-to-noise ratio, and reliability of human-generated preference and alignment data.
**Theory of change:** The quality of alignment is heavily dependent on the quality of the data (e.g., human preferences); by improving the "signal" from annotators and reducing noise/bias, we will get more robustly aligned models.
**See also:** synthetic data for alignment, scalable oversight, assistance games / reward learning, model values / default preferences.
**Orthodox problems:** superintelligence can fool human supervisors, value is fragile and hard to specify.
**Target case:** average case.
**Broad approach:** engineering.
**Some names:** Maarten Buyl, Kelsey Kraus, Margaret Kroll, Danqing Shi.
**Estimated FTEs:** 20-50.
**Critiques:** [**A Statistical Case Against Empirical Human-AI Alignment**](https://arxiv.org/abs/2502.14581)
**Funded by:** Anthropic, Google DeepMind, OpenAI, Meta AI, various academic groups.
**Outputs in 2025:**
* [**Challenges and Future Directions of Data-Centric AI Alignment**](https://arxiv.org/html/2410.01957v2)
* [**You Are What You Eat \-- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation**](https://arxiv.org/abs/2502.05475)
* [**AI Alignment at Your Discretion**](https://arxiv.org/abs/2502.10441), *Maarten Buyl, Hadi Khalaf, Claudio Mayrink Verdun et al.*, 2025-02-10, arXiv
* [**Maximizing Signal in Human-Model Preference Alignment**](https://arxiv.org/abs/2503.04910), *Kelsey Kraus, Margaret Kroll*, 2025-03-06, arXiv
* [**DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition**](https://arxiv.org/abs/2507.18802), *Danqing Shi, Furui Cheng, Tino Weinkauf et al.*, 2025-07-24, arXiv
## Goal robustness \[sec:goal\_robustness\]

### Mild optimisation \[a:mild\_optimization\]

**Who edits (internal): jord** ✅
**One-sentence summary:** Avoid Goodharting by getting AI to satisfice rather than maximise.
**Theory of change:** If we fail to exactly nail down the preferences for a superintelligent agent we die to Goodharting → shift from maximising to satisficing in the agent's utility function → we get a nonzero share of the lightcone as opposed to zero; also, moonshot at this being the recipe for fully aligned AI.
**See also:**
**Orthodox problems:** Value is fragile and hard to specify
**Target case:** mixed
**Broad approach:** cognitive
**Some names:**
**Estimated FTEs:** 10-50
**Critiques:**
**Funded by:** Google DeepMind
**Outputs in 2025:**
* [**MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking**](https://arxiv.org/abs/2501.13011)
* [BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format](https://arxiv.org/abs/2509.02655)
* [**Why modelling multi-objective homeostasis is essential for AI alignment (and how it helps with AI safety as well). Subtleties and Open Challenges**](https://lesswrong.com/posts/vGeuBKQ7nzPnn5f7A/why-modelling-multi-objective-homeostasis-is-essential-for)
### RL safety \[a:rl\_safety\]

**Who edits (internal):** **Stephen** ✅
**One-sentence summary:** Improves the robustness of reinforcement learning agents by addressing core problems in reward learning, goal misgeneralization, and specification gaming.
**Theory of change:** Standard RL objectives (like maximizing expected value) are brittle and lead to goal misgeneralization or specification gaming; by developing more robust frameworks (like pessimistic RL, minimax regret, or provable inverse reward learning), we can create agents that are safe even when misspecified.
**See also:** Behaviour Alignment Theory, assistance games / reward learning, goal robustness, iterative alignment, mild optimisation, scalable oversight, [**The Theoretical Reward Learning Research Agenda: Introduction and Motivation**](https://www.alignmentforum.org/posts/pJ3mDD7LfEwp3s5vG/the-theoretical-reward-learning-research-agenda-introduction).
**Orthodox problems:** goals misgeneralize out of distribution, value is fragile and hard to specify, superintelligence can fool human supervisors.
**Target case:** pessimistic.
**Broad approach:** engineering.
**Some names:** Joar Skalse, Karim Abdel Sadek, Matthew Farrugia-Roberts, Benjamin Plaut, Fang Wu, Stephen Zhao, Alessandro Abate, Steven Byrnes, Michael Cohen.
**Estimated FTEs:** 20-70.
**Critiques:** ["The Era of Experience" has an unsolved technical alignment problem](https://www.lesswrong.com/posts/747f6b8e/the-era-of-experience-has-an-unsolved-technical-alignment-problem), [The Invisible Leash: Why RLVR May or May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
**Funded by:** Google DeepMind, University of Oxford, CMU, Coefficient Giving.
**Outputs in 2025:**
* [**The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret**](https://arxiv.org/abs/2406.15753)
* [**Mitigating Goal Misgeneralization via Minimax Regret**](https://arxiv.org/abs/2507.03068), *Karim Abdel Sadek, Matthew Farrugia-Roberts, Usman Anwar et al.*, 2025-07-03, RLC 2025
* [**Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?**](https://arxiv.org/abs/2410.05584)
* [**Safe Learning Under Irreversible Dynamics via Asking for Help**](https://arxiv.org/abs/2502.14043), *Benjamin Plaut, Juan Liévano-Karim, Hanlin Zhu et al.*, 2025-02-19, arXiv
* [**The Invisible Leash: Why RLVR May or May Not Escape Its Origin**](https://arxiv.org/abs/2507.14843), *Fang Wu, Weihao Xuan, Ximing Lu et al.*, 2025-07-20, arXiv
* [**Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference**](https://arxiv.org/abs/2510.21184), *Stephen Zhao, Aidan Li, Rob Brekelmans et al.*, 2025-10-24, arXiv
* [**"The Era of Experience" has an unsolved technical alignment problem**](https://lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment), *Steven Byrnes*, 2025-04-24, LessWrong
* [**Safety cases for Pessimism**](https://lesswrong.com/posts/CpftMXCEnwqbWreHD/safety-cases-for-pessimism), *Michael Cohen*, LessWrong
* [**Interpreting Emergent Planning in Model-Free Reinforcement Learning**](https://arxiv.org/abs/2504.01871), *Thomas Bush, Stephen Chung, Usman Anwar et al.*, 2025-04-02, arXiv (ICLR 2025 oral)
* [**Misalignment From Treating Means as Ends**](https://arxiv.org/abs/2507.10995), *Henrik Marklund, Alex Infanger, Benjamin Van Roy,* 2025-07-15, arXiv
### Assistance games, assistive agents \[a:assistance\_games\]

**Who edits (internal):** **Rory**
**One-sentence summary:** Formalize how AI assistants learn about human preferences given uncertainty and partial observability, and construct environments which better incentivize AIs to learn what we want them to learn.
**Theory of change:** Understand what kinds of things can go wrong when humans are directly involved in training a model → build tools that make it easier for a model to learn what humans want it to learn.
**See also:**
**Orthodox problems:** 1\. Value is fragile and hard to specify, 10\. Humanlike minds/goals are not necessarily safe
**Target case:** Varies
**Broad approach:** engineering, cognitive
**Some names:** Joar Skalse, Anca Dragan, Caspar Oesterheld, David Krueger, Dylan Hafield-Menell
**Estimated FTEs:** ?
**Critiques:** [nice summary](https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument) of historical problem statements
**Funded by:** Future of Life Institute, Coefficient Giving, Survival and Flourishing Fund, Cooperative AI Foundation, Polaris Ventures
**Outputs in 2025:**
* [Training LLM Agents to Empower Humans](https://arxiv.org/pdf/2510.13709)
* [**Murphys Laws of AI Alignment: Why the Gap Always Wins**](https://arxiv.org/abs/2509.05381), *Madhava Gaikwad*, 2025-09-04, arXiv
* [**AssistanceZero: Scalably Solving Assistance Games**](https://arxiv.org/abs/2504.07091), *Cassidy Laidlaw, Eli Bronstein, Timothy Guo et al.*, 2025-04-09, arXiv
* [**Observation Interference in Partially Observable Assistance Games**](https://arxiv.org/abs/2412.17797), *Scott Emmons, Caspar Oesterheld, Vincent Conitzer et al.*, 2024-12-23, arXiv
* [**Learning to Assist Humans without Inferring Rewards**](https://arxiv.org/abs/2411.02623), *Vivek Myers, Evan Ellis, Sergey Levine et al.*, 2024-11-04, NeurIPS 2024

## Harm reduction for open weights \[a:open\_model\_interventions\]

**Who edits (internal): Stephen** ✅
**One-sentence summary:** Develops methods, primarily based on pretraining data intervention, to create tamper-resistant safeguards that prevent open-weight models from being maliciously fine-tuned to remove safety features or exploit dangerous capabilities.
**Theory of change:** Open-weight models allow adversaries to easily remove post-training safety (like refusal training) via simple fine-tuning; by making safety an intrinsic property of the model's learned knowledge and capabilities (e.g., by ensuring "deep ignorance" of dual-use information), the safeguards become far more difficult and expensive to remove.
**See also:** Pretraining data filtering, unlearning, data poisoning defense.
**Orthodox problems:** Someone else will deploy unsafe superintelligence first.
**Target case:** Average case.
**Broad approach:** Engineering.
**Some names:** Kyle O'Brien, Stephen Casper, Quentin Anthony, Tomek Korbak, Rishub Tamirisa, Mantas Mazeika, Stella Biderman, Yarin Gal.
**Estimated FTEs:** 10-100.
**Critiques:**
**Funded by:** UK AI Safety Institute (AISI), EleutherAI, Coefficient Giving.
**Outputs in 2025:**
* [Tamper-Resistant Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2408.00761)
* [Open Technical Problems in Open-Weight AI Model Risk Management](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5705186)
* [Deep ignorance: Filtering pretraining data builds tamper-resistant safeguards into open-weight LLMs](https://www.aisi.gov.uk/research/deep-ignorance-filtering-pretraining-data-builds-tamper-resistant-safeguards-into-open-weight-llms)

## The "Neglected Approaches" Approach \[a:black\_box\_neglected\_approaches:\]

**Who edits (internal):** Stephen ✅
**One-sentence summary:** Agenda-agnostic approaches to identifying good but overlooked empirical alignment ideas, working with theorists who could use engineers, and prototyping them.
**Theory of change:** Empirical search for “negative alignment taxes” (prioritizing methods that simultaneously enhance alignment and capabilities)
**See also:** automated alignment research, iterative alignment, Beijing Key Laboratory of Safe AI and Superalignment, Aligned AI.
**Orthodox problems:** someone else will deploy unsafe superintelligence first.
**Target case:** average_case.
**Broad approach:** engineering.
**Some names:** AE Studio, Gunnar Zarncke, Cameron Berg, Michael Vaiana, Judd Rosenblatt, Diogo Schwerz de Lucena.
**Estimated FTEs:** 15
**Critiques:** [The 'Alignment Bonus' is a Dangerous Mirage](https://www.alignmentforum.org/posts/example-critique-neg-tax), [Why 'Win-Win' Alignment is a Distraction](https://example.com/win-win-critique)
**Funded by:** AE Studio.
**Outputs in 2025:**
* [**Learning Representations of Alignment**](https://arxiv.org/abs/2412.16325), *Gunnar Zarncke, Cameron Berg, Michael Vaiana, Judd Rosenblatt, Diogo Schwerz de Lucena et al.*, 2024-12-19, arXiv, \[paper\_preprint, sr=0.85, id:2412.16325\],
* [**Self-Correction in Thought-Attractors: A Nudge Towards Alignment**](https://arxiv.org/abs/2510.24797), *Cameron Berg, Gunnar Zarncke, Michael Vaiana, Judd Rosenblatt et al.*, 2025-10-28, arXiv, \[paper\_preprint, sr=0.88, id:2510.24797\]
* [**Engineering Alignment: A Practical Framework for Prototyping 'Negative Tax' Solutions**](https://arxiv.org/abs/2508.08492), *Gunnar Zarncke, Michael Vaiana, Cameron Berg, Judd Rosenblatt et al.*, 2025-08-15, arXiv, \[paper\_preprint, sr=0.82, id:2508.08492\]

# White-box safety (understand and control current model internals) \[sec:whitebox\]

## Interpretability \[sec:interpretability\]

This section isn't very conceptually clean. See the [Open Problems](https://arxiv.org/abs/2501.16496) paper for a strong frame which is nonetheless not very useful for our descriptive purposes.

### Reverse engineering \[a:interp\_fundamental\]

**Who edits (internal): Stephen** ✅
**One-sentence summary:** Decompose a model into its functional, interacting components (circuits), formally describe what computation those components perform, and validate their causal effects to reverse-engineer the model's internal algorithm.
**Theory of change:** By gaining a mechanical understanding of how a model works (the "circuit diagram"), we can predict how models will act in novel situations (generalization), and gain the mechanistic knowledge necessary to safely modify an AI's goals or internal mechanisms.
**See also:** [ambitious mech interp](https://www.alignmentforum.org/posts/Hy6PX43HGgmfiTaKu/an-ambitious-vision-for-interpretability)
**Orthodox problems:** Goals misgeneralize out of distribution, superintelligence can fool human supervisors.
**Target case:** worst case.
**Broad approach:** cognitive.
**Some names:** Lucius Bushnaq, Dan Braun, Lee Sharkey, Aaron Mueller, Atticus Geiger, Sheridan Feucht, David Bau, Yonatan Belinkov, Stefan Heimersheim.
**Estimated FTEs:** 100-200
**Critiques:** [Interpretability Will Not Reliably Find Deceptive AI](https://www.alignmentforum.org/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai), [A Problem to Solve Before Building a Deception Detector](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector), [MoSSAIC: AI Safety After Mechanism](https://openreview.net/forum?id=n7WYSJ35FU), [The Misguided Quest for Mechanistic AI Interpretability](https://ai-frontiers.org/articles/the-misguided-quest-for-mechanistic-ai-interpretability). [Mechanistic?](https://arxiv.org/abs/2410.09087), [Assessing skeptical views of interpretability research](https://www.youtube.com/watch?v=woo_J0RKcpQ),  [Activation space interpretability may be doomed](https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed), [A Pragmatic Vision for Interpretability](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability)
**Outputs in 2025:**

#### In weights-space

* [The Circuits Research Landscape](https://www.neuronpedia.org/graph/info)
* [Stochastic Parameter Decomposition](https://openreview.net/forum?id=dEdS9ao8gN). See also the precursor: [Attribution-based parameter decomposition](https://www.alignmentforum.org/posts/EPefYWjuHNcNH4C7E/attribution-based-parameter-decomposition)
* [MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151)
* [Circuits in Superposition: Compressing many small neural networks into one](https://www.lesswrong.com/posts/roE7SHjFWEoMcGZKd/circuits-in-superposition-compressing-many-small-neural)
* [Compressed Computation is (probably) not Computation in Superposition](https://www.lesswrong.com/posts/ZxFchCFJFcgysYsT9/compressed-computation-is-probably-not-computation-in)
* [The Dual-Route Model of Induction](https://arxiv.org/abs/2504.03022)
* [The Geometry of Self-Verification in a Task-Specific Reasoning Model](https://arxiv.org/abs/2504.14379)
* [Converting MLPs into Polynomials in Closed Form](https://arxiv.org/abs/2502.01032)
* [Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts](https://arxiv.org/abs/2412.04614)
* [Identifying Sparsely Active Circuits Through Local Loss Landscape Decomposition](https://arxiv.org/abs/2504.00194)
* [Blink of an eye: a simple theory for feature localization in generative models](https://arxiv.org/abs/2502.00921)
* [From Memorization to Reasoning in the Spectrum of Loss Curvature](https://arxiv.org/abs/2510.24256)
* [Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers](https://arxiv.org/abs/2506.10887)
* [RelP: Faithful and Efficient Circuit Discovery in Language Models via Relevance Patching](https://arxiv.org/abs/2508.21258)
* [Structural Inference: Interpreting Small Language Models with Susceptibilities](https://arxiv.org/abs/2504.18274)
* [Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition](https://arxiv.org/abs/2501.14926)
* [How Do LLMs Perform Two-Hop Reasoning in Context?](https://arxiv.org/abs/2502.13913)
* [On the creation of narrow AI: hierarchy and nonlocality of neural network skills](https://arxiv.org/abs/2505.15811)

#### In activations-space

* [Fresh in memory: Training-order recency is linearly encoded in language model activations](https://arxiv.org/abs/2509.14223), Dmitrii Krasheninnikov, Richard E. Turner, David Krueger, 2025-09-17, arXiv
* [Language Models use Lookbacks to Track Beliefs](https://arxiv.org/abs/2505.14685), Nikhil Prakash, Natalie Shapira, Arnab Sen Sharma et al., 2025-05-20, arXiv
* [Interpreting Emergent Planning in Model-Free Reinforcement Learning](https://arxiv.org/pdf/2504.01871)
* [Constrained belief updates explain geometric structures in transformer representations](https://arxiv.org/abs/2502.01954)
* [Do Language Models Use Their Depth Efficiently?](https://arxiv.org/abs/2505.13898)
* [How Do Transformers Learn Variable Binding in Symbolic Programs?](https://arxiv.org/abs/2505.20896)
* [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)
* [Language Models Use Trigonometry to Do Addition](https://arxiv.org/abs/2502.00873)
* [Transformers Struggle to Learn to Search](https://arxiv.org/abs/2412.04703)
* [ICLR: In-Context Learning of Representations](https://openreview.net/forum?id=pXlmOmlHJZ)
* [Adversarial Examples Are Not Bugs, They Are Superposition](https://arxiv.org/abs/2508.17456)
* [Building and evaluating alignment auditing agents](https://lesswrong.com/posts/DJAZHYjWxMrcd2na3/building-and-evaluating-alignment-auditing-agents)
* [Bridging the human–AI knowledge gap through concept discovery and transfer in AlphaZero](https://www.pnas.org/doi/10.1073/pnas.2406675122)
* [Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban](https://arxiv.org/abs/2506.10138), Mohammad Taufeeque, Aaron David Tucker, Adam Gleave et al., 2025-06-11, arXiv
### Concept-based interpretability \[a:interp\_concept\_based\]

**Who edits (internal):** **Stephen** ✅
**One-sentence summary:** Identifies directions or subspaces in a model's latent state that correspond to high-level concepts (like refusal, deception, or planning) and uses them to build "mind-reading" probes that audit models for misalignment or monitor them at runtime.
**Theory of change:**  By mapping internal activations to human-interpretable concepts, we can detect dangerous capabilities or deceptive alignment directly in the mind of the model even if its overt behavior is perfectly safe. Deploy computationally cheap monitors to flag some hidden misalignment in deployed systems.
**See also:** Reverse engineering, sparse coding, model diffing.
**Orthodox problems:** Value is fragile and hard to specify, goals misgeneralize out of distribution, a boxed AGI might exfiltrate itself by steganography, spearphishing.
**Target case:** pessimistic
**Broad approach:** cognitive
**Some names:** Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adserà, Tom Wollschläger, Anna Soligo, Jack Lindsey, Brian Christian, Ling Hu, Nicholas Goldowsky-Dill,
**Estimated FTEs:** 50-100.
**Critiques:** [Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/html/2505.09807v1)**,** [Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)
**Funded by:** Coefficient Giving, Anthropic, various academic groups
**Outputs in 2025:**
* [Toward universal steering and monitoring of AI models](https://arxiv.org/abs/2502.03708), Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adserà et al., 2025-05-28, arXiv
* [Convergent Linear Representations of Emergent Misalignment](https://arxiv.org/abs/2506.11618), Anna Soligo, Edward Turner, Senthooran Rajamanoharan et al., 2025-06-20, arXiv
* [Detecting Strategic Deception Using Linear Probes](https://arxiv.org/abs/2502.03407)
* [Auditing language models for hidden objectives](https://www.anthropic.com/research/auditing-hidden-objectives), 2025-03-13, Anthropic Blog
* [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326), Brian Christian, Hannah Rose Kirk, Jessica A.F. Thompson et al., 2025-06-08, FAccT '25 (ACM Conference on Fairness, Accountability, and Transparency)
* [The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence](https://arxiv.org/abs/2502.17420), Tom Wollschläger, Jannes Elstner, Simon Geisler et al., 2025-02-24, arXiv
* [How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations](https://arxiv.org/abs/2508.05625), Brandon Jaipersaud, David Krueger, Ekdeep Singh Lubana, 2025-08-07, arXiv
* [Refusal in LLMs is an Affine Function](https://arxiv.org/abs/2411.09003), Thomas Marshall, Adam Scherlis, Nora Belrose, 2024-11-13, arXiv
* [Here's 18 Applications of Deception Probes](https://lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes), Cleo Nardo, Avi Parrack, jordine, 2025-08-28, LessWrong
* [White Box Control at UK AISI \- Update on Sandbagging Investigations](https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging)
* [Cost-Effective Constitutional Classifiers via Representation Re-use](https://alignment.anthropic.com/2025/cheap-monitors)
### Lie and deception detectors \[a:deception\_detectors\]

**One-sentence summary:** Detect when a model is being deceptive or lying by building white- or black-box detectors. Some work below requires intent in their definition, while other work focuses only on whether the model states something it believes to be false, regardless of intent.

**Theory of change:** Such detectors could flag suspicious behavior during evaluations or deployment, augment training to reduce deception, or audit models pre-deployment. Specific applications include alignment evaluations (e.g. by validating answers to introspective questions), safeguarding evaluations (catching models that “sandbag”, that is, strategically underperform to pass capability tests), and large-scale deployment monitoring. An honest version of a model could also provide oversight during training or detect cases where a model behaves in ways it understands are unsafe.

**See also:** Reverse engineering, Evals - Deception, Evals - Sandbagging
**Orthodox problems:**
**Target case:** pessimistic
**Broad approach:** cognitivist
**Some names:** [Cadenza](https://cadenzalabs.org), Sam Marks, Rowan Wang, Kieron Kretschmar, Sharan Maiya, Walter Laurito, Chris Cundy, Adam Gleave, Aviel Parrack, Stefan Heimersheim, Carlo Attubato, Joseph Bloom, Jordan Taylor, Alex McKenzie, Urja Pawar, Lewis Smith, Bilal Chughtai, Neel Nanda
**Estimated FTEs:** 10-50
**Critiques:** difficult to determine if behavior is strategic deception or only low level "reflexive" actions; Unclear if a model roleplaying a liar has deceptive intent. [How are intentional descriptions (like deception) related to algorithmic ones (like understanding the mechanisms models use)?](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector), [Is This Lie Detector Really Just a Lie Detector? An Investigation of LLM Probe Specificity](https://www.lesswrong.com/posts/5dkhdRMypeuyoXfmb/is-this-lie-detector-really-just-a-lie-detector-an),[Herrmann](https://www.lesswrong.com/posts/bCQbSFrnnAk7CJNpM/still-no-lie-detector-for-llms), [Smith and Chughtai](https://arxiv.org/abs/2511.22662)
**Funded by:** Anthropic, Deepmind, UK AISI, Coefficient Giving
**Outputs in 2025:**

* [**Caught in the Act: a mechanistic approach to detecting deception**](https://arxiv.org/abs/2508.19505)
* [Detecting Strategic Deception Using Linear Probes](https://www.lesswrong.com/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes)
* [Detecting High-Stakes Interactions with Activation Probes](https://arxiv.org/abs/2506.10805)
* [Whitebox detection of sandbagging model organisms](https://www.lesswrong.com/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging)
* [White Box Control at UK AISI \- Update on Sandbagging Investigations](https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging)
* [Benchmarking deception probes for trusted monitoring](https://www.lesswrong.com/posts/eaEqAzGN3uJfpfGoc/trusted-monitoring-but-with-deception-probes)
* [Probes and SAEs do well on Among Us benchmark](https://www.lesswrong.com/posts/gRc8KL2HLtKkFmNPr/among-us-a-sandbox-for-agentic-deception)
* [18 Applications of Deception Probes](https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/18-applications-of-deception-probes)
* [Preference Learning with Lie Detectors can Induce Honesty or Evasion](https://arxiv.org/abs/2505.13787)
* [Liars’ Bench: Evaluating Lie Detectors for Language Models](https://arxiv.org/html/2511.16035v1)[Evaluating honesty and lie detection techniques on a diverse suite of dishonest models](https://alignment.anthropic.com/2025/honesty-elicitation/)



### Model diffing \[a:model\_diff\]

**Who edits (internal): jord** ✅

**One-sentence summary:** Understand what happens when a model is finetuned, what the “diff” between the finetuned and the original model consists in.
**Theory of change:** By identifying the mechanistic differences between a base model and its fine-tune (e.g., after RLHF), maybe we can verify that safety behaviors are robustly “internalized” rather than superficially patched, and detect if dangerous capabilities or deceptive alignment have been introduced without needing to re-analyze the entire model. The diff is also much smaller, since most parameters don’t change, which means you can use heavier methods on them.
**See also:** sparse coding, reverse engineering.
**Orthodox problems:** Value is fragile and hard to specify
**Target case:** pessimistic
**Broad approach:** cognitivist
**Some names:** Julian Minder, Clément Dumas, Neel Nanda, Trenton Bricken, Jack Lindsey
**Estimated FTEs:** 10-30
**Critiques:** not found
**Funded by:** various academic groups, Anthropic, Google DeepMind
**Outputs in 2025:**

* [**Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences**](https://arxiv.org/abs/2510.13900), *Julian Minder, Clément Dumas, Stewart Slocum et al.*, 2025-10-14, arXiv
* [**Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning**](https://arxiv.org/abs/2504.02922), *Julian Minder, Clément Dumas, Caden Juang et al.*, 2025-04-03, NeurIPS 2025
* [https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why](https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why)
* [**Persona Features Control Emergent Misalignment**](https://arxiv.org/abs/2506.19823), *Miles Wang, Tom Dupré la Tour, Olivia Watkins et al.*, 2025-06-24, arXiv
* [Insights on Crosscoder Model Diffing](https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html)
* [Open Source Replication of Anthropic’s Crosscoder paper for model-diffing](https://www.lesswrong.com/posts/srt6JXsRMtmqAJavD/open-source-replication-of-anthropic-s-crosscoder-paper-for)
* [Diffing Toolkit: Model Comparison and Analysis Framework](https://github.com/science-of-finetuning/diffing-toolkit%20)
* [Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs](https://openreview.net/forum?id=ZB84SvrZB8%20)
* [https://www.goodfire.ai/research/model-diff-amplification](https://www.goodfire.ai/research/model-diff-amplification)
### Sparse Coding \[a:interp\_sparse\_coding\]

**Who edits (internal):** **Stephen** ✅
**One-sentence summary:** Decompose the polysemantic activations of the residual stream into a sparse linear combination of monosemantic "features" which correspond to interpretable concepts.
**Theory of change:** Get a principled decomposition of an LLM's activation into atomic components → identify deception and other misbehaviors.
**See also:** Concept-based interp, reverse engineering.
**Orthodox problems:** Value is fragile and hard to specify, goals misgeneralize out of distribution, superintelligence can fool human supervisors.
**Target case:** average_case
**Broad approach:** Engineering / cognitive
**Some names:** Leo Gao, Dan Mossing, Emmanuel Ameisen, Jack Lindsey, Adam Pearce, Thomas Heap, Abhinav Menon, Kenny Peng, Tim Lawson.
**Estimated FTEs:** 50-100.
**Critiques:** [Sparse Autoencoders Can Interpret Randomly Initialized Transformers](https://arxiv.org/abs/2501.17727), [The Sparse Autoencoders bubble has popped, but they are still promising](https://agarriga.substack.com/p/the-sparse-autoencoders-bubble-has), [Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research](https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/), [Sparse Autoencoders Trained on the Same Data Learn Different Features](https://arxiv.org/pdf/2501.16615), [Why Not Just Train For Interpretability?](https://www.lesswrong.com/posts/2HbgHwdygH6yeHKKq/why-not-just-train-for-interpretability)
**Funded by:** everyone, roughly. Frontier labs, LTFF, Coefficient Giving, etc.
**Outputs in 2025:**
* [Weight-sparse transformers have interpretable circuits](https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf)
* [Circuit Tracing: Revealing Computational Graphs in Language Models](https://transformer-circuits.pub/2025/attribution-graphs/methods.html), Emmanuel Ameisen, Jack Lindsey, Adam Pearce et al., 2025-03-27, Transformer Circuits Thread
* [Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning](https://arxiv.org/abs/2504.02922), Julian Minder, Clément Dumas, Caden Juang et al., 2025-04-03, NeurIPS 2025
* [Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models](https://arxiv.org/abs/2504.02821), Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot et al., 2025-04-03, arXiv
* [I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2503.18878), Andrey Galichin, Alexey Dontsov, Polina Druzhinina et al., 2025-03-24, arXiv
* [Sparse Autoencoders Do Not Find Canonical Units of Analysis](https://arxiv.org/abs/2502.04878), Patrick Leask, Bart Bussmann, Michael Pearce et al., 2025-02-07, arXiv (accepted to ICLR 2025\)
* [Transcoders Beat Sparse Autoencoders for Interpretability](https://arxiv.org/abs/2501.18823), Gonçalo Paulo, Stepan Shabalin, Nora Belrose, 2025-01-31, arXiv
* [The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs](https://arxiv.org/abs/2510.07775), Omar Mahmoud, Ali Khalil, Buddhika Laknath Semage et al., 2025-10-09, arXiv
* [Scaling sparse feature circuit finding for in-context learning](https://arxiv.org/abs/2504.13756), Dmitrii Kharlapenko, Stepan Shabalin, Fazl Barez et al., 2025-04-18, arXiv
* [Learning Multi-Level Features with Matryoshka Sparse Autoencoders](https://arxiv.org/abs/2503.17547), Bart Bussmann, Noa Nabeshima, Adam Karvonen et al., 2025-03-21, arXiv
* [Are Sparse Autoencoders Useful? A Case Study in Sparse Probing](https://arxiv.org/abs/2502.16681), Subhash Kantamneni, Joshua Engels, Senthooran Rajamanoharan et al., 2025-02-23, arXiv
* [Sparse Autoencoders Trained on the Same Data Learn Different Features](https://arxiv.org/abs/2501.16615), Gonçalo Paulo, Nora Belrose, 2025-01-28, arXiv
* [Partially Rewriting a Transformer in Natural Language](https://arxiv.org/abs/2501.18838), Gonçalo Paulo, Nora Belrose, 2025-01-31, arXiv
* [SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability](https://arxiv.org/abs/2503.09532), Adam Karvonen, Can Rager, Johnny Lin et al., 2025-06-04, arXiv (accepted to ICML 2025\)
* [Low-Rank Adapting Models for Sparse Autoencoders](https://arxiv.org/abs/2501.19406), Matthew Chen, Joshua Engels, Max Tegmark, 2025-01-31, arXiv
* [Enhancing Automated Interpretability with Output-Centric Feature Descriptions](https://arxiv.org/abs/2501.08319), Yoav Gur-Arieh, Roy Mayan, Chen Agassy et al., 2025-01-14, arXiv (accepted to ACL 2025\)
* [Towards Understanding Distilled Reasoning Models: A Representational Approach](https://arxiv.org/abs/2503.03730), David D. Baek, Max Tegmark, 2025-03-05, ICLR 2025 Workshop on Building Trust in Language Models and Applications
* [Do Sparse Autoencoders Generalize? A Case Study of Answerability](https://arxiv.org/abs/2502.19964), Lovis Heindrich, Philip Torr, Fazl Barez et al., 2025-02-27, ICML 2025 Workshop on Reliable and Responsible Foundation Models (arXiv preprint)
* [Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages](https://arxiv.org/abs/2501.06346), Jannik Brinkmann, Chris Wendler, Christian Bartelt et al., 2025-01-10, arXiv
* [Interpreting the linear structure of vision-language model embedding spaces](https://arxiv.org/abs/2504.11695), Isabel Papadimitriou, Huangyuan Su, Thomas Fel et al., 2025-04-16, COLM 2025
* [What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data](https://arxiv.org/abs/2510.26202), Rajiv Movva, Smitha Milli, Sewon Min et al., 2025-10-30, arXiv
* [Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models](https://arxiv.org/abs/2411.14257), Javier Ferrando, Oscar Obeso, Senthooran Rajamanoharan et al., 2024-11-21, arXiv
* [Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization](https://arxiv.org/abs/2506.10920), Or Shafran, Atticus Geiger, Mor Geva, 2025-06-12, arXiv
* [Priors in Time: Missing Inductive Biases for Language Model Interpretability](https://arxiv.org/abs/2511.01836), Ekdeep Singh Lubana, Can Rager, Sai Sumedh R. Hindupur et al., 2025-11-03, arXiv
* [Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models](https://arxiv.org/abs/2505.17769), Patrick Leask, Neel Nanda, Noura Al Moubayed, 2025-05-23, ICML 2025
* [Binary Sparse Coding for Interpretability](https://arxiv.org/abs/2509.25596), Lucia Quirke, Stepan Shabalin, Nora Belrose, 2025-09-29, arXiv
* [Dense SAE Latents Are Features, Not Bugs](https://arxiv.org/abs/2506.15679), Xiaoqing Sun, Alessandro Stolfo, Joshua Engels et al., 2025-06-18, arXiv (NeurIPS 2025\)
* [Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks](https://arxiv.org/abs/2411.18895), Adam Karvonen, Can Rager, Samuel Marks et al., 2024-11-28, arXiv
* [Evaluating SAE interpretability without explanations](https://arxiv.org/abs/2507.08473), Gonçalo Paulo, Nora Belrose, 2025-07-11, arXiv
* [SAEs Are Good for Steering \-- If You Select the Right Features](https://arxiv.org/abs/2505.20063), Dana Arad, Aaron Mueller, Yonatan Belinkov, 2025-05-26, arXiv
* [Line of Sight: On Linear Representations in VLLMs](https://arxiv.org/abs/2506.04706), Achyuta Rajaram, Sarah Schwettmann, Jacob Andreas et al., 2025-06-05, arXiv
* [Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models](https://arxiv.org/abs/2411.00743), Aashiq Muhamed, Mona Diab, Virginia Smith, 2024-11-01, arXiv
* [Enhancing Neural Network Interpretability with Feature-Aligned Sparse Autoencoders](https://arxiv.org/abs/2411.01220), Luke Marks, Alasdair Paren, David Krueger et al., 2024-11-02, arXiv
* [BatchTopK Sparse Autoencoders](https://arxiv.org/abs/2412.06410), Bart Bussmann, Patrick Leask, Neel Nanda, 2024-12-09, arXiv
* [Understanding sparse autoencoder scaling in the presence of feature manifolds](https://arxiv.org/abs/2509.02565), Eric J. Michaud, Liv Gorton, Tom McGrath, 2025-09-02, arXiv
* [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128), Dmitrii Troitskii, Koyena Pal, Chris Wendler et al., 2025-10-05, arXiv (EMNLP Findings 2025\)
* [Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs](https://arxiv.org/abs/2505.20254), Xiangchen Song, Aashiq Muhamed, Yujia Zheng et al., 2025-05-26, arXiv
* [How Visual Representations Map to Language Feature Space in Multimodal LLMs](https://arxiv.org/abs/2506.11976), Constantin Venhoff, Ashkan Khakzar, Sonia Joseph et al., 2025-06-13, arXiv
* [Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video](https://arxiv.org/abs/2504.19475), Sonia Joseph, Praneet Suresh, Lorenz Hufe et al., 2025-04-28, arXiv / CVPR Mechanistic Interpretability for Vision Workshop
* [Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning](https://arxiv.org/abs/2505.24360), Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko et al., 2025-05-30, CVPR 2025 \- Mechanistic Interpretability for Vision Workshop
* [CRISP: Persistent Concept Unlearning via Sparse Autoencoders](https://arxiv.org/abs/2508.13650), Tomer Ashuach, Dana Arad, Aaron Mueller et al., 2025-08-19, arXiv
* [SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs](https://arxiv.org/abs/2504.08192), Aashiq Muhamed, Jacopo Bonato, Mona Diab et al., 2025-04-11, arXiv
* [Scaling Sparse Feature Circuit Finding to Gemma 9B](https://lesswrong.com/posts/PkeB4TLxgaNnSmddg/scaling-sparse-feature-circuit-finding-to-gemma-9b), Diego Caples, Jatin Nainani, CallumMcDougall et al., 2025-01-10, LessWrong
* [Topological Data Analysis and Mechanistic Interpretability](https://lesswrong.com/posts/6oF6pRr2FgjTmiHus/topological-data-analysis-and-mechanistic-interpretability), Gunnar Carlsson, 2025-02-24, LessWrong
### Causal Abstractions \[a:interp\_causal\_abstractions\]

**Who edits (internal): Stephen** ✅
**One-sentence summary:** Verify that a neural network implements a specific high-level causal model (like a logical algorithm) by finding a mapping between high-level variables and low-level neural representations.
**Theory of change:** By establishing a precise, causal mapping between a black-box neural network and a human-interpretable algorithm, we can mathematically guarantee that the model is using safe reasoning processes and predict its behavior on unseen inputs, rather than relying on behavioral testing alone.
**See also:** concept-based interp, reverse engineering.
**Orthodox problems:** Goals misgeneralize out of distribution.
**Target case:** worst case
**Broad approach:** Cognitive
**Some names:** Atticus Geiger, Christopher Potts, Thomas Icard, Theodora-Mara Pîslar, Sara Magliacane, Jiuding Sun, Jing Huang.
**Estimated FTEs:** 10-30
**Critiques:** [The Misguided Quest for Mechanistic AI Interpretability](https://www.google.com/search?q=https://open.substack.com/pub/aifrontiersmedia/p/the-misguided-quest-for-mechanistic), [Interpretability Will Not Reliably Find Deceptive AI](https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai).
**Funded by:** Various academic groups, Google DeepMind, Goodfire.
**Outputs in 2025:**
* [Combining Causal Models for More Accurate Abstractions of Neural Networks](https://arxiv.org/abs/2503.11429), Theodora-Mara Pîslar, Sara Magliacane, Atticus Geiger, 2025-03-14, arXiv
* [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214), Atticus Geiger, Jacqueline Harding, Thomas Icard, 2025-08-15, arXiv
* [HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks](https://arxiv.org/abs/2503.10894), Jiuding Sun, Jing Huang, Sidharth Baskaran et al., 2025-03-13, ICLR 2025

### Data attribution \[a:data\_attribution\]

**Who edits (internal):** **jord** ✅
**One-sentence summary:** Quantifies the influence of individual training data points on a model's specific behavior or output, allowing researchers to trace model properties (like misalignment, bias, or factual errors) back to their source in the training set.
**Theory of change:** By attributing harmful, biased, or unaligned behaviors to specific training examples, researchers can audit proprietary models, debug training data, enable effective data deletion/unlearning
**See also:** Data quality for alignment.
**Orthodox problems:** Goals misgeneralize out of distribution, value is fragile and hard to specify.
**Target case:** Average case.
**Broad approach:** Behavioral
**Some names:** Philipp Alexander Kreer, Wilson Wu, Jin Hwa Lee, Matthew Smith, Abhilasha Ravichander, Andrew Wang, Jiacheng Liu, Jiaqi Ma, Junwei Deng, Yijun Pan.
**Estimated FTEs:** 30-60.
**Funded by:** Various academic groups

**Outputs in 2025:**

* [Bayesian Influence Functions for Hessian-Free Data Attribution](https://arxiv.org/abs/2509.26544), Philipp Alexander Kreer, Wilson Wu, Maxwell Adam et al., 2025-09-30, arXiv
* [Influence Dynamics and Stagewise Data Attribution](https://arxiv.org/abs/2510.12071), Jin Hwa Lee, Matthew Smith, Maxwell Adam et al., 2025-10-14, arXiv
* [You Are What You Eat \-- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation](https://arxiv.org/abs/2502.05475)
* [Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models](https://arxiv.org/abs/2503.12072), Abhilasha Ravichander, Jillian Fisher, Taylor Sorensen et al., 2025-03-15, arXiv
* [What is Your Data Worth to GPT?](https://arxiv.org/abs/2405.13954)
* [Distributional Training Data Attribution: What do Influence Functions Sample?](https://arxiv.org/abs/2506.12965)
* [Better Training Data Attribution via Better Inverse Hessian-Vector Products](https://arxiv.org/abs/2507.14740), Andrew Wang, Elisa Nguyen, Runshi Yang et al., 2025-07-19, arXiv
* [OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens](https://arxiv.org/abs/2504.07096), Jiacheng Liu, Taylor Blanton, Yanai Elazar et al., 2025-04-09, arXiv
* [Detecting and Filtering Unsafe Training Data via Data Attribution with Denoised Representation](https://arxiv.org/abs/2502.11411)
* [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
* [Revisiting Data Attribution for Influence Functions](https://arxiv.org/abs/2508.07297)
* [A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning](https://openreview.net/forum?id=sYK4yPDuT1)
### Other interpretability \[a:interp\_other\]

**Who edits (internal):** **jord** ✅
**One-sentence summary:** Interpretability that does not fall well into other categories.
**Theory of change:** Explore alternative conceptual frameworks (e.g., agentic, propositional) and physics-inspired methods (e.g., renormalization). Or be “pragmatic”.
**See also:** Reverse engineering, concept-based interp.
**Orthodox problems:** Superintelligence can fool human supervisors, goals misgeneralize out of distribution.
**Target case:** Mixed.
**Broad approach:** Engineering / cognitive.
**Some names:** Lee Sharkey, Dario Amodei, David Chalmers, Been Kim, Neel Nanda, David D. Baek, Lauren Greenspan, Dmitry Vaintrob, Sam Marks, Jacob Pfau.
**Estimated FTEs:** 30-60.
**Critiques:** [The Misguided Quest for Mechanistic AI Interpretability](https://aifrontiersmedia.substack.com/p/the-misguided-quest-for-mechanistic), [Interpretability Will Not Reliably Find Deceptive AI](https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai).
**Funded by:**

**Outputs in 2025:**
* [Agentic Interpretability: A Strategy Against Gradual Disempowerment](https://www.alignmentforum.org/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual)
* [Open Problems in Mechanistic Interpretability](https://arxiv.org/abs/2501.16496), Lee Sharkey, Bilal Chughtai, Joshua Batson et al., 2025-01-27, arXiv
* [The Urgency of Interpretability](https://www.darioamodei.com/post/the-urgency-of-interpretability), Dario Amodei, 2025, darioamodei.com
* [Propositional Interpretability in Artificial Intelligence](https://arxiv.org/abs/2501.15740), David J. Chalmers, 2025-01-27, arXiv
* [Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2412.02104), Yunkai Dang, Kaichen Huang, Jiahao Huo et al., 2024-12-03, arXiv
* [Harmonic Loss Trains Interpretable AI Models](https://arxiv.org/abs/2502.01628), David D. Baek, Ziming Liu, Riya Tyagi et al., 2025-02-03, arXiv
* [Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation](https://arxiv.org/abs/2506.02300), Farzaneh Mahdisoltani, Saeed Mahdisoltani, Roger B. Grosse et al., 2025-06-02, arXiv
* [Transformers Don't Need LayerNorm at Inference Time: Implications for Interpretability](https://lesswrong.com/posts/KbFuuaBKRP7FcAADL/transformers-don-t-need-layernorm-at-inference-time), submarat, Joachim Schaeffer, Luca Baroni et al., 2025-07-23, LessWrong / arXiv
* [Against blanket arguments against interpretability](https://lesswrong.com/posts/u3ZysuXEjkyHhefrk/against-blanket-arguments-against-interpretability), Dmitry Vaintrob, 2025-01-22, LessWrong
* [Opportunity Space: Renormalization for AI Safety](https://lesswrong.com/posts/wkGmouy7JnTNtWAbc/opportunity-space-renormalization-for-ai-safety), Lauren Greenspan, Dmitry Vaintrob, Lucas Teixeira, 2025-03-31, LessWrong
* [Prospects for Alignment Automation: Interpretability Case Study](https://lesswrong.com/posts/y5cYisQ2QHiSbQbhk/prospects-for-alignment-automation-interpretability-case), Jacob Pfau, Geoffrey Irving, 2025-03-21, LessWrong
* [Downstream applications as validation of interpretability progress](https://lesswrong.com/posts/wGRnzCFcowRCrpX4Y/downstream-applications-as-validation-of-interpretability), Sam Marks, 2025-03-31, LessWrong
* [Principles for Picking Practical Interpretability Projects](https://lesswrong.com/posts/DqaoPNqhQhwBFqWue/principles-for-picking-practical-interpretability-projects), Sam Marks, 2025-07-15, LessWrong
* [Renormalization Redux: QFT Techniques for AI Interpretability](https://lesswrong.com/posts/sjr66DBEgyogAbfdf/renormalization-redux-qft-techniques-for-ai-interpretability), Lauren Greenspan, Dmitry Vaintrob, 2025-01-18, LessWrong
* [The Strange Science of Interpretability: Recent Papers and a Reading List for the Philosophy of Interpretability](https://lesswrong.com/posts/qRnupMmFG7dxQTTYh/the-strange-science-of-interpretability-recent-papers-and-a), Kola Ayonrinde, Louis Jaburi, 2025-08-17, LessWrong
* [Call for Collaboration: Renormalization for AI safety](https://lesswrong.com/posts/MDWGcNHkZ3NPEzcnp/call-for-collaboration-renormalization-for-ai-safety), Lauren Greenspan, 2025-03-31, LessWrong
* [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334), Zhe Li, Wei Zhao, Yige Li et al., 2025-09-26, arXiv
* [Language Models May Verbatim Complete Text They Were Not Explicitly Trained On](https://arxiv.org/abs/2503.17514), Ken Ziyu Liu, Christopher A. Choquette-Choo, Matthew Jagielski et al., 2025-03-21, arXiv
* [Extracting memorized pieces of (copyrighted) books from open-weight language models](https://arxiv.org/abs/2505.12546), A. Feder Cooper, Aaron Gokaslan, Ahmed Ahmed et al., 2025-05-18, arXiv
* [**A Pragmatic Vision for Interpretability**](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability)
* [https://arxiv.org/abs/2505.15811](https://arxiv.org/abs/2505.15811)
## Activation engineering \[a:activation\_engineering\]

**Who edits (internal):** **Jord** ✅
**One-sentence summary:** A technique for programmatically modifying internal model activations to steer outputs toward desired behaviors, serving as a lightweight, interpretable alternative (or supplement) to fine-tuning.
**Theory of change:** Test interpretability theories; find new insights from interpretable causal interventions on representations. Or: build more stuff to stack on top of finetuning. Slightly encourage the model to be nice, add one more layer of defence to our bundle of partial alignment methods.
**See also:** Representation engineering, sparse coding, whitebox control.
**Orthodox problems:** Value is fragile and hard to specify
**Target case:** average_case
**Broad approach:** engineering / cognitive
**Some names:** Runjin Chen, Andy Arditi, David Krueger, Jan Wehner, Narmeen Oozeer, Reza Bayat, Adam Karvonen, Jiuding Sun, Tim Tian Hua, Helena Casademunt, Jacob Dunefsky, Thomas Marshall.
**Estimated FTEs:** 50-200.
**Critiques:** [Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)
**Funded by:** Coefficient Giving, Anthropic.
**Outputs in 2025:**
* [Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509), Runjin Chen, Andy Arditi, Henry Sleight et al., 2025-07-29, arXiv
* [Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models](https://arxiv.org/abs/2502.19649v1), Jan Wehner, Sahar Abdelnabi, Daniel Tan et al., 2025-02-27, arXiv
* [Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers](https://arxiv.org/abs/2510.12672), Ruben Belo, Marta Guimaraes, Claudia Soares, 2025-10-14, arXiv
* [Activation Space Interventions Can Be Transferred Between Large Language Models](https://arxiv.org/abs/2503.04429), Narmeen Oozeer, Dhruv Nathawani, Nirmalendu Prakash et al., 2025-03-06, arXiv (accepted to ICML 2025\)
* [Steering Large Language Model Activations in Sparse Spaces](https://arxiv.org/abs/2503.00177), Reza Bayat, Ali Rahimi-Kalahroudi, Mohammad Pezeshki et al., 2025-02-28, arXiv
* [Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control](https://arxiv.org/abs/2411.02461), Yuxin Xiao, Chaoqun Wan, Yonggang Zhang et al., 2024-11-04, arXiv
* [Robustly Improving LLM Fairness in Realistic Settings via Interpretability](https://arxiv.org/abs/2506.10922), Adam Karvonen, Samuel Marks, 2025-06-12, arXiv
* [HyperSteer: Activation Steering at Scale with Hypernetworks](https://arxiv.org/abs/2506.03292), Jiuding Sun, Sidharth Baskaran, Zhengxuan Wu et al., 2025-06-03, arXiv
* [Steering Evaluation-Aware Language Models to Act Like They Are Deployed](https://arxiv.org/abs/2510.20487), Tim Tian Hua, Andrew Qin, Samuel Marks et al., 2025-10-23, arXiv
* [Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning](https://arxiv.org/abs/2507.16795), Helena Casademunt, Caden Juang, Adam Karvonen et al., 2025-07-22, arXiv
* [Improving Steering Vectors by Targeting Sparse Autoencoder Features](https://arxiv.org/abs/2411.02193), Sviatoslav Chalnev, Matthew Siu, Arthur Conmy, 2024-11-04, arXiv
* [Understanding Reasoning in Thinking Language Models via Steering Vectors](https://arxiv.org/abs/2506.18167), Constantin Venhoff, Iván Arcuschin, Philip Torr et al., 2025-06-22, ICLR 2025 Workshop on Reasoning and Planning for Large Language Models
* [Comparing Bottom-Up and Top-Down Steering Approaches on In-Context Learning Tasks](https://arxiv.org/abs/2411.07213), Madeline Brumley, Joe Kwon, David Krueger et al., 2024-11-11, arXiv
* [Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models](https://arxiv.org/abs/2502.19649), Jan Wehner, Sahar Abdelnabi, Daniel Tan et al., 2025-02-27, arXiv
* [Do safety-relevant LLM steering vectors optimized on a single example generalize?](https://lesswrong.com/posts/6aXe9nipTgwK5LxaP/do-safety-relevant-llm-steering-vectors-optimized-on-a), Jacob Dunefsky, 2025-02-28, arXiv
* [One-shot steering vectors cause emergent misalignment, too](https://lesswrong.com/posts/kcKnKHTHycHeRhcHF/one-shot-steering-vectors-cause-emergent-misalignment-too), Jacob Dunefsky, 2025-04-14, LessWrong

## Developmental interpretability \[a:learning\_dev\_interp\]

**Who edits (internal):** **Stephen** ✅
**One-sentence summary:** Builds tools for detecting, locating, and interpreting key structural shifts, phase transitions, and emergent phenomena (like grokking or deception) that occur during a model's training and in-context learning phases.
**Theory of change:** Structures forming in neural networks leave identifiable traces that can be interpreted (e.g., using concepts from Singular Learning Theory); by catching and analyzing these developmental moments, researchers can automate interpretability, predict when dangerous capabilities emerge, and intervene to prevent deceptiveness or misaligned values as early as possible.
**See also:** Reverse engineering, sparse coding, [ICL transience](https://proceedings.mlr.press/v267/singh25c.html)
**Orthodox problems:** Goals misgeneralize out of distribution
**Target case:** Worst case.
**Broad approach:** Cognitive.
**Some names:** Timaeus, Jesse Hoogland, George Wang, Daniel Murfet, Stan van Wingerden, Alexander Gietelink Oldenziel.
**Estimated FTEs:** 10-50
**Critiques:** [Vaintrob](https://www.lesswrong.com/posts/M2bs6xCbmc79nwr8j/dmitry-vaintrob-s-shortform#A8Ziwhts35dgqbz52), [Joar Skalse (2023)](https://www.alignmentforum.org/posts/ALJYj4PpkqyseL7kZ/my-criticism-of-singular-learning-theory)
**Funded by:** Manifund, Survival and Flourishing Fund, EA Funds
**Outputs in 2025:**

* [A Review of Developmental Interpretability in Large Language Models](https://arxiv.org/abs/2508.15841), Ihor Kendiukhov, 2025-08-19, arXiv
* [Learning Coefficients, Fractals, and Trees in Parameter Space](https://openreview.net/forum?id=KUFH0n1BIM), Max Hennick, Matthias Dellago, 2025-06-23, ODYSSEY 2025 Conference
* [Programs as Singularities](https://openreview.net/forum?id=Td37oOfmmz), Daniel Murfet, William Troiani, 2025-06-20, ODYSSEY 2025 Conference
* [https://www.lesswrong.com/posts/2MX2bXreTtntB85Zy/from-slt-to-ait-nn-generalisation-out-of-distribution](https://www.lesswrong.com/posts/2MX2bXreTtntB85Zy/from-slt-to-ait-nn-generalisation-out-of-distribution)
* [https://www.lesswrong.com/posts/ZSQaT2yxNNZ3eLxRd/understanding-and-controlling-llm-generalization](https://www.lesswrong.com/posts/ZSQaT2yxNNZ3eLxRd/understanding-and-controlling-llm-generalization)
* [Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training](https://arxiv.org/abs/2506.18777), Jonathan Cook, Silvia Sapora, Arash Ahmadian et al., 2025-06-23, arXiv
* [Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining](https://arxiv.org/abs/2509.05291), Deniz Bayazit, Aaron Mueller, Antoine Bosselut, 2025-09-05, arXiv
* [Dynamics of Transient Structure in In-Context Linear Regression Transformers](https://arxiv.org/abs/2501.17745), Liam Carroll, Jesse Hoogland, Matthew Farrugia-Roberts et al., 2025-01-29, arXiv
* [Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought](https://arxiv.org/abs/2509.23365), Hanlin Zhu, Shibo Hao, Zhiting Hu et al., 2025-09-27, arXiv
* [What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?](https://arxiv.org/abs/2411.07681), Katie Kang, Amrith Setlur, Dibya Ghosh et al., 2024-11-12, arXiv
* [Compressibility Measures Complexity: Minimum Description Length Meets Singular Learning Theory](https://arxiv.org/abs/2510.12077), Einar Urdshals, Edmund Lau, Jesse Hoogland et al., 2025-10-14, arXiv
* [Modes of Sequence Models and Learning Coefficients](https://arxiv.org/abs/2504.18048), Zhongtian Chen, Daniel Murfet, 2025-04-25, arXiv
* [SLT for AI Safety](https://lesswrong.com/posts/J7CyENFYXPxXQpsnD/slt-for-ai-safety), Jesse Hoogland, 2025-07-01, LessWrong
* [Selective regularization for alignment-focused representation engineering](https://lesswrong.com/posts/HFcriD29cw3E5QLCR/selective-regularization-for-alignment-focused), Sandy Fraser, 2025-05-20, LessWrong

## Representation structure and geometry \[a:representation\_structure\]

**Who edits (internal): Stephen** ✅
**One-sentence summary:** What do the representations look like? Does any simple structure underlie the beliefs of all well-trained models? Can we get the semantics from this geometry?
**Theory of change:** Get scalable unsupervised methods for finding structure in representations and interpreting them, then using this to e.g. guide training.
**See also:** concept-based interp, computational mechanics, feature universality, natural abstractions, causal abstractions
**Orthodox problems:** Goals misgeneralize out of distribution, superintelligence can fool human supervisors.
**Target case:** mixed
**Broad approach:** cognitive
**Some names:** Simplex, Insight \+ Interaction Lab, Paul Riechers, Adam Shai, Martin Wattenberg, Blake Richards, Mateusz Piotrowski
**Estimated FTEs:** 10-50.
**Critiques:** Not found.
**Funded by:** Various academic groups
**Outputs in 2025:**
* [Shared Global and Local Geometry of Language Model Embeddings](https://arxiv.org/abs/2503.21073)
* [Connecting Neural Models Latent Geometries with Relative Geodesic Representations](https://arxiv.org/abs/2506.01599)
* [Constrained belief updates explain geometric structures in transformer representations](https://arxiv.org/abs/2502.01954)
* [The Geometry of Self-Verification in a Task-Specific Reasoning Model](https://arxiv.org/pdf/2504.14379)
* [Navigating the Latent Space Dynamics of Neural Models](https://arxiv.org/abs/2505.22785)
* [The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence](https://arxiv.org/abs/2502.17420)
* [The Geometry of ReLU Networks through the ReLU Transition Graph](https://arxiv.org/abs/2505.11692)
* [Deep sequence models tend to memorize geometrically; it is unclear why](https://arxiv.org/abs/2510.26745)
* [Tracing the Representation Geometry of Language Models from Pretraining to Post-training](https://arxiv.org/abs/2509.23024)
* [Next-token pretraining implies in-context learning](https://arxiv.org/abs/2505.18373)
* [Neural networks leverage nominally quantum and post-quantum representations](https://arxiv.org/abs/2507.07432)
* [Rank-1 LoRAs Encode Interpretable Reasoning Signals](http://arxiv.org/abs/2511.06739)
* [Embryology of a Language Model](https://arxiv.org/abs/2508.00331), George Wang, Garrett Baker, Andrew Gordon et al., 2025-08-01, arXiv
## Human inductive biases \[a:human\_biases\]

**Who edits (internal): Stephen ** ✅
**One-sentence summary:** Discover connections deep learning AI systems have with human brains and human learning processes. Develop an ‘alignment moonshot’ based on a coherent theory of learning which applies to both humans and AI systems.
**Theory of change:** Humans learn trust, honesty, self-maintenance, and corrigibility; if we understand how they do maybe we can get future AI systems to learn them.
**See also:** active learning, ACS research
**Orthodox problems:** Goals misgeneralize out of distribution.
**Target case:** pessimistic
**Broad approach:** cognitive
**Some names:** Lukas Muttenthaler, Quentin Delfosse.
**Estimated FTEs:** 4
**Critiques:** Not Found.
**Funded by:** Google DeepMind, various academic groups.
**Outputs in 2025:**

* [**Aligning machine and human visual representations across abstraction levels**](https://www.nature.com/articles/s41586-025-09631-6)**.** By fine-tuning vision foundation models on human-judgement-derived similarity labels, the models’ internal representations become significantly more aligned with human abstractions, leading to improved out-of-distribution robustness and task generalisation.
* [**Beginning with You: Perceptual-Initialization Improves Vision-Language Representation and Alignment**](https://arxiv.org/abs/2505.14204)**.** Seeds a vision encoder with human perceptual similarity judgments before standard image-text contrastive training resulting in noticeably improved zero-shot vision-language performance and alignment across diverse benchmarks.
* [**Deep Reinforcement Learning Agents are not even close to Human Intelligence**](https://arxiv.org/html/2505.21731v1)**.** The paper shows that deep RL agents, even high-performing ones, fail badly on simplified versions of Atari games (variants that humans handle easily) revealing that the agents rely on brittle, misaligned shortcuts rather than robust, human-like reasoning.
* [**Teaching AI to Handle Exceptions: Supervised Fine-tuning with Human-aligned Judgment**](https://arxiv.org/html/2503.02976v2#S3)
* **[HIBP Human Inductive Bias Project Plan](https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0)**
# Safety by construction \[sec:safety\_by\_construction\]

Approaches which minimise the use of singleton deep learning models.
## Guaranteed Safe AI \[a:formal\_verification\]

**Who edits (internal):** **Rory** ✅
**One-sentence summary:** Formally model the behavior of cyber-physical systems, construct formally verified shells and interfaces which pose precise constraints on what actions can occur, and require AIs to provide safety guarantees for their recommended actions (correctness and uniqueness)
**Theory of change:** Make a formal verification system that can act as an intermediary between human users and a potentially dangerous system, only letting provably safe actions through. (Notable for not requiring that we solve ELK; does require that we solve ontology though)
**See also:** [Synthesizing Standalone World-Models](https://www.alignmentforum.org/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models)*,* Bengio's Scientist AI*,* Safeguarded AI, Open Agency Architecture, SLES, program synthesis
**Orthodox problems:** 1\. Value is fragile and hard to specify, 4\. Goals misgeneralize out of distribution, 7\. Superintelligence can fool human supervisors, 9\. Humans cannot be first-class parties to a superintelligent value handshake, 12\. A boxed AGI might exfiltrate itself by steganography, spearphishing
**Target case:** (nearly) worst-case
**Broad approach:** cognitive
**Some names:** ARIA, Lawzero, Atlas Computing, FLF, Max Tegmark, [Beneficial AI Foundation](https://www.beneficialaifoundation.org/), Steve Omohundro, David "davidad" Dalrymple, Joar Skalse, Stuart Russell, Ohad Kammar, Alessandro Abate, Fabio Zanassi
**Estimated FTEs:** *10-100*
**Critiques:** Zvi, Gleave, Dickson, [Greenblatt](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation?commentId=MJCvHk5ARMnWDjQDg)
**Funded by:** Manifund, UK government, Coefficient Giving, Survival and Flourishing Fund, Mila / CIFAR
**Outputs in 2025:**
* [**SafePlanBench: evaluating a Guaranteed Safe AI Approach for LLM-based Agents**](https://manifund.org/projects/safeplanbench-evaluating-a-guaranteed-safe-ai-approach-for-llm-based-agents), *Agustín Martinez Suñé, Tan Zhi Xuan*, Manifund
* [**Report on NSF Workshop on Science of Safe AI**](https://arxiv.org/abs/2506.22492)
* [**A benchmark for vericoding: formally verified program synthesis**](https://arxiv.org/abs/2509.22908)
* [**Beliefs about formal methods and AI safety**](https://lesswrong.com/posts/CCT7Qc8rSeRs7r5GL/beliefs-about-formal-methods-and-ai-safety)
* [A Toolchain for AI-Assisted Code Specification, Synthesis and Verification](https://atlascomputing.org/ai-assisted-fv-toolchain.pdf)

## Scientist AI \[a:scientist\_ai\]

**Who edits (internal): Rory**

**One-sentence summary:** Develop powerful, non-agentic, uncertainty-aware world-modeling systems that substantially accelerate scientific progress while avoiding the risks of building AIs that act as ‘agents’

**Theory of change:** Developing non-agentic ‘Scientist AI’ allows us to: (i) reap the benefits of AI progress while (ii) avoiding the inherent risks of agentic systems. These systems can also (iii) provide a useful guardrail to protect us from unsafe agentic AIs by double-checking actions they propose, and (iv) help us more safely build agentic superintelligent systems.

**See also:** [JEPA](https://arxiv.org/abs/2511.08544)*,* [oracles](https://www.lesswrong.com/w/oracle-ai)

**Orthodox problems:** 3\. Pivotal processes require dangerous capabilities, 4\. Goals misgeneralize out of distribution, 5\. Instrumental Convergence

**Target case:** pessimistic

**Broad approach:** cognitive
**Some names:** Yoshua Bengio
**Estimated FTEs:** 1-10

**Critiques:** Hard to find, but see [Raymond Douglas’ comment](https://www.lesswrong.com/posts/p5gBcoQeBsvsMShvT/superintelligent-agents-pose-catastrophic-risks-can?commentId=tJXqhg3XZsqnyaZs2), [Karnofsky-Soares discussion](https://www.lesswrong.com/posts/iy2o4nQj9DnQD7Yhj/discussion-with-nate-soares-on-a-key-alignment-difficulty). Perhaps also [Predict-O-Matic](https://www.lesswrong.com/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic) .
**Funded by:** ARIA, Future of Life Institute, Jaan Tallinn, Schmidt Sciences

**Outputs in 2025:**

* [Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?](https://arxiv.org/abs/2502.15657), Yoshua Bengio, Michael Cohen, Damiano Fornasiere et al., 2025-02-21, arXiv
* [The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems](https://arxiv.org/abs/2509.08713), Ziming Luo, Atoosa Kasirzadeh, Nihar B. Shah, 2025-09-10, arXiv

## Brainlike-AGI Safety \[a:brainlike\_agi\]

**Who edits: Stephen** ✅

**One-sentence summary:**  Social and moral instincts are (partly) implemented in particular hardwired brain circuitry; let's figure out what those circuits are and how they work; this will involve symbol grounding. “a yet-to-be-invented variation on actor-critic model-based reinforcement learning”
**Theory of change:** Fairly direct alignment via changing training to reflect actual human reward. Get actual data about (reward, training data) → (human values) to help with theorising this map in AIs; "understand human social instincts, and then maybe adapt some aspects of those for AGIs, presumably in conjunction with other non-biological ingredients".
**See also:**
**Orthodox problems:**
**Target case:** worst-case
**Broad approach:** cognitive
**Some names:** Stephen Byrnes
**Estimated FTEs:** 1-5
**Critiques:** [Tsvi BT](https://www.lesswrong.com/posts/unCG3rhyMJpGJpoLd/koan-divining-alien-datastructures-from-ram-activations#BtHCubjKWDFafkmYH).
**Funded by:** Astera Institute.
**Outputs in 2025:**

* [https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement](https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement)
* [https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard](https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard)
* [https://www.lesswrong.com/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires](https://www.lesswrong.com/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires)
* [**Reward button alignment**](https://lesswrong.com/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment), *Steven Byrnes*, 2025-05-22, LessWrong
* [**System 2 Alignment: Deliberation, Review, and Thought Management**](https://lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment-deliberation-review-and-thought), *Seth Herd*, 2025-02-13, LessWrong
* [https://elicit.com/blog/system-2-learning](https://elicit.com/blog/system-2-learning)
# Make AI solve it \[sec:ai\_solve\_alignment\]

## Weak-to-strong generalization \[a:weak\_to\_strong\]

**Who edits (internal):** **Rory** ✅
**One-sentence summary:** Use weaker models to supervise and provide a feedback signal to stronger models.
**Theory of change:** Find techniques that do better than RLHF at supervising superior models → track whether these techniques fail as capabilities increase further → keep the stronger systems aligned by amplifying weak oversight and quantifying where it breaks.
**See also:** Supervising AIs improving AIs.
**Orthodox problems:** 8\. Superintelligence can hack software supervisors
**Target case:** average_case
**Broad approach:** engineering
**Some names:** Joshua Engels, Nora Belrose*,* David D. Baek.
**Estimated FTEs:** *2-20*
**Critiques:** [Can we safely automate alignment research?](https://joecarlsmith.substack.com/p/can-we-safely-automate-alignment), [Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization](https://arxiv.org/abs/2406.11431)
**Funded by:** lab funders, Eleuther funders
**Outputs in 2025:**

* [**Debate Helps Weak-to-Strong Generalization**](https://arxiv.org/abs/2501.13124)
* [**Understanding the Capabilities and Limitations of Weak-to-Strong Generalization**](https://openreview.net/forum?id=RwYdLgj1S6)
* [**Great Models Think Alike and this Undermines AI Oversight**](https://arxiv.org/abs/2502.04313)
* [**Scaling Laws For Scalable Oversight**](https://arxiv.org/abs/2504.18530)
## Supervising AIs improving AIs \[a:supervising\_improvement\]

**Who edits (internal):** **Rory** ✅
**One-sentence summary:** Build formal and empirical frameworks where AIs supervise other (stronger) AI systems via structured interactions; construct monitoring tools which enable scalable tracking of behavioural drift, benchmarks for self-modification, and robustness guarantees
**Theory of change:** Early models train \~only on human data while later models also train on early model outputs, which leads to early model problems cascading. Left unchecked this will likely cause problems, so supervision mechanisms are needed to help ensure the AI self-improvement remains legible.
**See also:**
**Orthodox problems:** 7\. Superintelligence can fool human supervisors or 8\. Superintelligence can hack software supervisors
**Target case:** pessimistic
**Broad approach:** behavioural
**Some names:** Roman Engeler, Akbir Khan, Ethan Perez
**Estimated FTEs:** 1-10
**Critiques:** [Automation collapse](https://www.lesswrong.com/posts/2Gy9tfjmKwkYbF9BY/automation-collapse), [Great Models Think Alike and this Undermines AI Oversight](https://arxiv.org/abs/2502.04313)
**Funded by:** Long-Term Future Fund, lab funders
**Outputs in 2025:**

* [**Bare Minimum Mitigations for Autonomous AI Development**](https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/)
* [**Maintaining Alignment during RSI as a Feedback Control Problem**](https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control), *beren*, 2025-03-02, LessWrong
* [**Scaling Laws for Scalable Oversight**](https://arxiv.org/abs/2504.18530), *Subhash Kantamneni, Josh Engels, David Baek, Max Tegmark*, 2025-04-25, arXiv
* [**Neural Interactive Proofs**](https://arxiv.org/abs/2412.08897), *Lewis Hammond, Sam Adam-Day*, 2025-03-17, arXiv
* [**Video and transcript of talk on automating alignment research**](https://lesswrong.com/posts/TQbptN7F4ijPnQRLy/video-and-transcript-of-talk-on-automating-alignment), *Joe Carlsmith*, 2025-04-30, LessWrong
* [Modeling Human Beliefs about AI Behavior for Scalable Oversight](https://arxiv.org/abs/2502.21262)
* [Scalable Oversight for Superhuman AI via Recursive Self-Critiquing](https://arxiv.org/abs/2502.04675)
* [Dodging systematic human errors in scalable oversight](https://www.alignmentforum.org/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight)

## Transluce \[a:transluce\]

**Who edits (internal):** **jord** ✅
**One-sentence summary:** *Make open AI tools to explain AIs, including agents. E.g. feature descriptions for neuron activation patterns; an interface for steering these features; behavior elicitation agent that searches for user-specified behaviors from frontier models*
**Theory of change:** *Improve interp and evals in public and get invited to improve lab processes.*
**See also:**
**Orthodox problems:** *7\. Superintelligence can fool human supervisors or 8\. Superintelligence can hack software supervisors*
**Target case:** *pessimistic*
**Broad approach:** *cognitive*
**Some names:** Jacob Steinhardt, Neil Chowdhury, Vincent Huang, Sarah Schwettmann, Robert Friel
**Estimated FTEs:** *15-30*
**Critiques:**
**Funded by:** Schmidt Sciences, Halcyon Futures, John Schulman,Wojciech Zaremba
**Outputs in 2025:**
* [Automatically Jailbreaking Frontier Language Models with Investigator Agents](https://transluce.org/jailbreaking-frontier-models)
* [Investigating truthfulness in a pre-release o3 model](https://transluce.org/investigating-o3-truthfulness)
* [Surfacing Pathological Behaviors in Language Models](https://transluce.org/pathological-behaviors)
* [Docent: A system for analyzing and intervening on agent behavior](https://transluce.org/introducing-docent)
* [https://transluce.org/neuron-circuits](https://transluce.org/neuron-circuits)

## Debate \[a:debate\]

**Who edits (internal):** **Rory**
**One-sentence summary:** Make highly capable agents do what humans want, even when it is difficult for humans to know what that is.
**Theory of change:** "Give humans help in supervising strong agents" \+ "Align explanations with the true reasoning process of the agent" \+ "Red team models to exhibit failure modes that don't occur in normal use" are necessary but probably not sufficient for safe AGI.
**See also:**
**Orthodox problems:** 1\. Value is fragile and hard to specify, 7\. Superintelligence can fool human supervisors
**Target case:** worst-case
**Broad approach:** engineering, cognitive
**Some names:** Rohin Shah, Jonah Brown-Cohen, Georgios Piliouras, UK AISI (Benjamin Holton)
**Estimated FTEs:** ?
**Critiques:** [The limits of AI safety via debate (2022)](https://www.lesswrong.com/posts/kguLeJTt6LnGuYX4E/the-limits-of-ai-safety-via-debate)
**Funded by:** Google, others
**Outputs in 2025:**
* [**AI Debate Aids Assessment of Controversial Claims**](https://arxiv.org/abs/2506.02175), *Salman Rahman, Sheriff Issaka, Ashima Suvarna et al.*, 2025-06-02, arXiv
* [**An alignment safety case sketch based on debate**](https://arxiv.org/abs/2505.03989), *Marie Davidsen Buhl, Jacob Pfau, Benjamin Hilton et al.*, 2025-05-23, arXiv
* [**UK AISI Alignment Team: Debate Sequence**](https://www.lesswrong.com/s/NdovveRcyfxgMoujf), *Benjamin Hilton, Jacob Pfau, et al.*
* [**Prover-Estimator Debate: A New Scalable Oversight Protocol**](https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol)
* [**Ensemble Debates with Local Large Language Models for AI Alignment**](https://arxiv.org/abs/2509.00091)


## LLM introspection training \[a:introspection\_training\]

**One-sentence summary:** Train LLMs to the predict the outputs of high-quality whitebox explainability methods, in order to induce the LLM to learn generalized self-explanation skills that use its own natural ‘introspective’ access

**Theory of change:** Integrating self-explanation into an LLM’s core skillset should lead to default scalability, since self-explanation skill advancement will feed off general-intelligence advancement

**See also:** Transluce

**Orthodox problems:**  goals misgeneralize out of distribution, superintelligence can fool human supervisors, superintelligence can hack software supervisors

**Target case:** mixed

**Broad approach:** cognitive

**Some names:** Belinda Z. Li, Zifan Carl Guo , Vincent Huang , Jacob Steinhardt, Jacob Andreas

**Estimated FTEs:** 2-20

**Critiques:**

**Funded by:** Schmidt Sciences, Halcyon Futures, John Schulman,Wojciech Zaremba

**Outputs in 2025:**  [Training Language Models to Explain Their Own Computations](https://arxiv.org/abs/2511.08579) Belinda Z. Li, Zifan Carl Guo , Vincent Huang , Jacob Steinhardt, Jacob Andreas

# Theory \[sec:theory\]

Develop a principled scientific understanding that will help us reliably understand and control current and future AI systems.

## Agent foundations \[a:agent\_foundations\]

**Who edits (internal):** Stag
**One-sentence summary:** Develop philosophical clarity and mathematical formalizations of building blocks that might be useful for plans to align strong superintelligence, such as agency, optimization strength, decision theory, abstractions, concepts, etc.
**Theory of change:** Rigorously understand optimization processed and agents, and what it means for them to be aligned in a substrate independent way → identify impossibility results and necessary conditions for aligned optimizer systems → use this theoretical understanding to eventually design safe architectures that remain stable and safe under self-reflection

**See also:** tiling agents, dovetail
**Orthodox problems:** 1\. Value is fragile and hard to specify, 2\. Corrigibility is anti-natural, 4\. Goals misgeneralize out of distribution
**Target case:** worst-case
**Broad approach:** cognitive
**Some names:** Abram Demski, Alex Altair, Sam Eisenstat, Thane Ruthenis
**Estimated FTEs:**
**Critiques:**
**Funded by:**

**Outputs in 2025:**

* [Limit-Computable Grains of Truth for Arbitrary Computable Extensive-Form (Un)Known Games](https://www.arxiv.org/pdf/2508.16245)
* [https://uaiasi.com/blog-posts/](https://uaiasi.com/blog-posts/)
* [**Off-switching not guaranteed**](https://link.springer.com/article/10.1007/s11098-025-02296-x), *Sven Neth*, 2025-02-26, Agent Foundations 2025 at CMU
* Good old fashioned decision theory [https://openreview.net/pdf?id=Rf1CeGPA22](https://openreview.net/pdf?id=Rf1CeGPA22)
* [**Formalizing Embeddedness Failures in Universal Artificial Intelligence**](https://openreview.net/forum?id=tlkYPU3FlX), *Cole Wyeth, Marcus Hutter*, 2025-07-01, ODYSSEY 2025 Conference
* [**Clarifying “wisdom”: Foundational topics for aligned AIs to prioritize before irreversible decisions**](https://www.lesswrong.com/posts/EyvJvYEFzDv5kGoiG/clarifying-wisdom-foundational-topics-for-aligned-ais-to), *Anthony DiGiovanni*, 2025-07-20, LessWrong
* [**Agent foundations: not really math, not really science**](https://www.lesswrong.com/posts/Dt4DuCCok3Xv5HEnG/agent-foundations-not-really-math-not-really-science), *Alex Altair*, 2025-08-17, LessWrong
* [**Is alignment reducible to becoming more coherent?**](https://lesswrong.com/posts/nuDJNyG5XLQjtvaeg/is-alignment-reducible-to-becoming-more-coherent), *Cole Wyeth*, 2025-04-22, LessWrong
* [**What Is The Alignment Problem?**](https://lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem), *johnswentworth*, 2025-01-16, LessWrong
## Tiling agents \[a:tiling\_agents\]

**Who edits (internal):** Stag
**One-sentence summary:** An aligned agentic system modifying itself into an unaligned system would be bad and we can research ways that this could occur and infrastructure/approaches that prevent it from happening.
**Theory of change:** Build enough theoretical basis through various approaches such that AI systems we create are capable of self-modification while preserving goals.
**See also:** Agent foundations
**Orthodox problems:** 1\. Value is fragile and hard to specify, 2\. Corrigibility is anti-natural, 3\. Goals misgeneralize out of distribution
**Target case:** worst-case
**Broad approach:** cognitive
**Some names:** Abram Demski
**Estimated FTEs:** 1-10
**Critiques:**
**Funded by:**

**Outputs in 2025:**
* [**Understanding Trust**](https://static1.squarespace.com/static/663d1233249bce4815fe8753/t/68067a6f5d5fb0745642d5b1/1745255023842/Understanding+Trust+-+Abram+Demski.pdf), *Abram Demski, Norman Hsia, and Paul Rapoport*, 2025-02-17, Agent Foundations 2025 at CMU
* [**Working through a small tiling result**](https://www.lesswrong.com/posts/akuMwu8SkmQSdospi/working-through-a-small-tiling-result), James Payor, 2025-05-13, LessWrong
* [**Communication & Trust**](https://openreview.net/forum?id=Rf1CeGPA22), *Abram Demski*, 2025-07-09, ODYSSEY 2025 Conference
* [**Maintaining Alignment during RSI as a Feedback Control Problem**](https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control), *beren*, 2025-03-02, LessWrong

## Dovetail \[a:theory\_dovetail\]

**Who edits (internal):** Stag
**One-sentence summary:** Formalize key ideas (“structure”, “agency”, etc) mathematically.
**Theory of change:** generalize theorems → formalize agent foundations concepts like the agent structure problem → hopefully assist other projects through increased understanding
**See also:** Agent foundations
**Orthodox problems:** "understanding the nature of the problems through formalization"
**Target case:** pessimistic
**Broad approach:** maths/philosophy
**Some names:** Alex Altair, Alfred Harwood, Daniel C, Dalcy K, José Pedro Faustino
**Estimated FTEs:** 1-5
**Critiques:**
**Funded by:** LTFF, [Patreon](https://www.patreon.com/Dovetailresearch/about)
**Outputs in 2025:**
* [**Report & retrospective on the Dovetail fellowship**](https://www.lesswrong.com/posts/ApfjBbqzSu4aZoLSe/report-and-retrospective-on-the-dovetail-fellowship), *Alex Altair*, 2025-03-15, LessWrong; contains numerous links


## High-Actuation Spaces \[a:live\_theory\]

**One-sentence summary:** Mech interp and alignment assume a stable “computational substrate” (linear algebra on GPUs). If later AI uses different substrates (e.g. something neuromorphic), methods like probes and steering will not transfer. Therefore, better to try and infer goals via a "telic DAG" which abstracts over substrates, and so sidestep the issue of how to define intermediate representations. Category theory is intended to provide guarantees that this abstraction is valid.
**Theory of change:** Sufficiently complex mindlike entities can alter their goals in ways that cannot be predicted or accounted for under substrate-dependent descriptions of the kind sought in mechanistic interpretability. use the telic DAG to define a method analogous to factoring a causal DAG.
**See also:** [Live theory](https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3), [MoSSAIC](https://openreview.net/forum?id=n7WYSJ35FU), [Topos Institute](https://topos.institute/), Agent foundations**,**
**Orthodox problems:** actually a new and general one
**Target case:** pessimistic
**Broad approach:** maths/philosophy
**Some names:** Sahil K, Matt Farr, Aditya Arpitha Prasad, Chris Pang, Aditya Adiga, Jayson Amati, Steve Petersen, Topos, T J
**Estimated FTEs:** 1-10
**Critiques:**
**Funded by:**

**Outputs in 2025:**
* [**https://groundless.ai/**](https://groundless.ai/)
* [https://openreview.net/forum?id=n7WYSJ35FU](https://openreview.net/forum?id=n7WYSJ35FU)
* [https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3](https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3)
* [https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit?tab=t.0\#heading=h.eg8luyrlsv2u](https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit?tab=t.0#heading=h.eg8luyrlsv2u)
* [https://drive.google.com/drive/folders/1EaAJ4szuZsYR2\_-DkS9cuhx3S6IWeCjW](https://drive.google.com/drive/folders/1EaAJ4szuZsYR2_-DkS9cuhx3S6IWeCjW)
* [https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency](https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency)
* See also the [Human Inductive Bias Project](https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0).

## Simulators \[a:simulators\]

**Who edits (internal):** Stag
**One-sentence summary:** Treat LLMs as a general simulator of sequences instead of as a goal-maximising agent. It effectively *roleplays* as a character or a statistical process. This is a counter-intuitive and hard to verify perspective, but yields different predictions.
**Theory of change:** Figure out the extent to which this framing matches current and future AI agent psychology, then use that to have less confused conversations and build safer models.
**See also:**
**Orthodox problems:** 4\. Goals misgeneralize out of distribution
**Target case:** pessimistic
**Broad approach:** cognitivist
**Some names:** Jan Kulveit, Will Petillo
**Estimated FTEs:** 1-5
**Critiques:**
**Funded by:**  Alignment of Complex Systems
**Outputs in 2025:**

* [A Three-Layer Model of LLM Psychology](https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology%20), *Jan Kulveit*, 2024-12-26, Alignment Forum
* [Simulators vs Agents: Updating Risk Models](https://www.lesswrong.com/s/pwKrMXjYNK5LNeKCu), *Will Petillo, Sean Herrington, Spencer Ames, Adebayo Mubarak, Can Narin*, 2025-05-12, LessWrong


## Asymptotic guarantees \[a:aisi\_guarantees\]

**Who edits (internal):** **Peli**
**One-sentence summary:** Prove that if a safety process has enough resources (human data quality, training time, neural network capacity), then in the limit some system specification will be guaranteed. Use complexity theory, game theory, learning theory and other areas to both improve asymptotic guarantees and develop ways of showing convergence.
**Theory of change:** Formal verification may be too hard. Make safety cases stronger by modelling their processes and proving that they would work in the limit.
**See also:** debate, control
**Orthodox problems:** 4\. Goals misgeneralize out of distribution, 7\. Superintelligence can fool human supervisors
**Target case:** pessimistic
**Broad approach:** cognitivist
**Some names:** AISI, Jacob Pfau, Benjamin Hilton, Geoffrey Irving, Simon Marshall, Will Kirby, Martin Soto, David Africa, davidad
**Estimated FTEs:** 5 \- 10
**Critiques:** Self-critique in [UK AISI's Alignment Team: Research Agenda](https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda)
**Funded by:** AISI

**Outputs in 2025:**
* [**An alignment safety case sketch based on debate**](https://lesswrong.com/posts/iELyAqizJkizBQbfr/an-alignment-safety-case-sketch-based-on-debate), *Marie\_DB, Jacob Pfau, Benjamin Hilton et al.*, 2025-05-08, LessWrong / AI Alignment Forum
* [**UK AISI's Alignment Team: Research Agenda**](https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda), *Benjamin Hilton, Jacob Pfau, Marie\_DB et al.*, 2025-05-07, LessWrong
* [**Dodging systematic human errors in scalable oversight**](https://lesswrong.com/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight), *Geoffrey Irving*, 2025-05-14, LessWrong


## Heuristic Explanations \[a:arc\_theory\_formal\]

**Who edits (internal):** **jord** ✅
**One-sentence summary:** Formalize mechanistic explanations of neural network behavior, automate the discovery of these “heuristic explanations” and use them to predict when novel input will lead to extreme behavior (Low Probability Estimation and Mechanistic Anomaly Detection).
**Theory of change:** Push mech interp as far as it can be pushed, use formally rigorous heuristic explanations for downstream tasks automatically, solve those downstream tasks well enough that Eliciting Latent Knowledge and deceptive alignment are solved.
**See also:** ARC Theory, ELK, mechanistic anomaly detection, [Acorn](https://acausal.org/)
**Orthodox problems:** *4\. Goals misgeneralize out of distribution, 8\. Superintelligence can hack software supervisors*
**Target case:** *worst-case*
**Broad approach:** *cognitive, maths/philosophy*
**Some names:** *Jacob Hilton, Mark Xu, Eric Neyman, Victor Lecomte, George Robinson*
**Estimated FTEs:** *1-10*
**Critiques:** [Matolcsi](https://www.lesswrong.com/s/uYMw689vDFmgPEHrS)
**Funded by:**
**Outputs in 2025:**
* [A computational no-coincidence principle](https://www.lesswrong.com/posts/Xt9r4SNNuYxW83tmo/a-computational-no-coincidence-principle)
* [**Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture**](https://openreview.net/forum?id=m4OpQAK3eY)
* [**Competing with sampling**](https://www.lesswrong.com/posts/XdQd9gELHakd5pzJA/arc-progress-update-competing-with-sampling)
* [**Obstacles in ARC’s research agenda**](https://www.lesswrong.com/s/uYMw689vDFmgPEHrS)
* [Deduction-Projection Estimators for Understanding Neural Networks](https://gabrieldwu.com/assets/thesis.pdf)

## Corrigibility \[sec:corrigibility\]

### Behavior alignment theory \[a:behavior\_alignment\_theory\]

**Who edits (internal):** Rory ✅
**One-sentence summary:** Predict properties of future AGI (e.g. power-seeking) with formal models; formally state and prove hypotheses about the properties powerful systems will have and how we might try to change them
**Theory of change:** Figure out hypotheses about properties powerful agents will have → attempt to rigorously prove under what conditions the hypotheses hold → test these hypotheses where feasible → design training environments that lead to more salutary properties
**See also:** Agent Foundations, control.
**Orthodox problems:** 2\. Corrigibility is anti-natural, 5\. Instrumental convergence
**Target case:** worst-case
**Broad approach:** maths/philosophy
**Some names:** Ram Potham, Michael K. Cohen*,* Max Harms/Raelifin*,* John Wentworth*,* David Lorell*,* Elliott Thornley
**Estimated FTEs:** 1-10
**Critiques:** [Ryan Greenblatt’s criticism](https://www.lesswrong.com/posts/YbEbwYWkf8mv9jnmi/the-shutdown-problem-incomplete-preferences-as-a-solution?commentId=GJAippZ6ZzCagSnDb) of one behavioral proposal,
**Funded by:** ?
**Outputs in 2025:**

* [Imitation learning is probably existentially safe](https://onlinelibrary.wiley.com/doi/10.1002/aaai.70040?af=R)
* [**Preference gaps as a safeguard against AI self-replication**](https://www.lesswrong.com/posts/knwR9RgGN5a2oorci/preference-gaps-as-a-safeguard-against-ai-self-replication), *Elliott Thornley, tbs*, 2025-11-26, LessWrong
* [**Serious Flaws in CAST**](https://www.lesswrong.com/s/KfCjeconYRdFbMxsy/p/qgBFJ72tahLo5hzqy), *Max Harms, 2025-11-19,* LessWrong
* [**Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power**](https://arxiv.org/abs/2508.00159), *Jobst Heitzig, Ram Potham*, 2025-07-31, arXiv
* [**A Safety Case for a Deployed LLM: Corrigibility as a Singular Target**](https://openreview.net/forum?id=mhEnJa9pNk), *Ram Potham*, 2025-06-24, ODYSSEY 2025 Conference
* [**Shutdownable Agents through POST-Agency**](https://arxiv.org/abs/2505.20203), *Elliott Thornley*, 2025-05-26, arXiv
* [**The Partially Observable Off-Switch Game**](https://arxiv.org/abs/2411.17749), *Andrew Garber, Rohan Subramani, Linus Luu et al.*, 2024-11-25, arXiv
* [**Deceptive Alignment and Homuncularity**](https://lesswrong.com/posts/9htmQx5wiePqTtZuL/deceptive-alignment-and-homuncularity), *Oliver Sourbut, TurnTrout*, 2025-01-16, LessWrong
* [**LLM AGI will have memory, and memory changes alignment**](https://lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment), *Seth Herd*, 2025-04-04, LessWrong
* [**A Shutdown Problem Proposal**](https://www.lesswrong.com/posts/PhTBDHu9PKJFmvb4p/a-shutdown-problem-proposal), *John Wentworth, David Lorrell*


### Other corrigibility \[a:corrigibility\_other\]

**Who edits (internal):** **Rory** ✅
**One-sentence summary:** Diagnose and communicate obstacles to achieving robustly corrigible behavior; suggest mechanisms, tests, and escalation channels for surfacing and mitigating incorrigible behaviors
**Theory of change:** Labs are likely to develop AGI using something analogous to current pipelines. Clarifying why naive instruction-following doesn’t buy robust corrigibility \+ building strong tripwires/diagnostics for scheming and Goodharting thus reduces risks on the likely default path.
**See also:** Behavior alignment theory
**Orthodox problems:** 2\. Corrigibility is anti-natural, 5\. Instrumental convergence
**Target case:** pessimistic
**Broad approach:** varies; math/philosophy \+ engineering
**Some names:** *Jan Kulveit, Jeremy Gillen*
**Estimated FTEs:** 1-10
**Critiques:** None found
**Funded by:** ?

**Outputs in 2025:**
* [**AI Assistants Should Have a Direct Line to Their Developers**](https://www.lesswrong.com/posts/LDYPF6yfe3f8SPHFT/ai-assistants-should-have-a-direct-line-to-their-developers), Jan Kulveit, 2024-12-28, LessWrong
* [**Testing for Scheming with Model Deletion**](https://www.lesswrong.com/posts/D5kGGGhsnfH7G8v9f/testing-for-scheming-with-model-deletion), Guive, 2025-01-07, LessWrong
* [**Detect Goodhart and shut down**](https://www.lesswrong.com/posts/ZHFZ6tivEjznkEoby/detect-goodhart-and-shut-down), *Jeremy Gillen*, 2025-01-22, LessWrong
* [**Instrumental Goals Are A Different And Friendlier Kind Of Thing Than Terminal Goals**](https://www.lesswrong.com/posts/7Z4WC4AFgfmZ3fCDC/instrumental-goals-are-a-different-and-friendlier-kind-of), *John Wentworth, David Lorell*, 2025-01-24, LessWrong
* [**Shutdownable Agents through POST-Agency**](https://www.lesswrong.com/posts/JuRdvZyqaFbvTPemn/shutdownable-agents-through-post-agency-1), *EJT*, 2025-09-16, LessWrong
* [**Why Corrigibility is Hard and Important (i.e. "Whence the high MIRI confidence in alignment difficulty?")**](https://www.lesswrong.com/posts/ksfjZJu3BFEfM6hHE/why-corrigibility-is-hard-and-important-i-e-whence-the-high), *Raemon, Eliezer Yudkowsky, So8res*, 2025-09-30, LessWrong
* [**Problems with instruction-following as an alignment target**](https://lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target), *Seth Herd*, 2025-05-15, LessWrong
* [Oblivious Defense in ML Models: Backdoor Removal without Detection](https://dl.acm.org/doi/10.1145/3717823.3718245)
* [Cryptographic Backdoor for Neural Networks: Boon and Bane](https://arxiv.org/abs/2509.20714)
* [A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning](https://arxiv.org/abs/2504.20310)

## Ontology Identification \[sec:ontology\_identification\]

### Natural abstractions \[a:natural\_abstractions\]

**Who edits (internal):** Stag

**One-sentence summary:** Develop a theory of concepts that explains how they are learned, how they structure a particular system’s understanding, and how mutual translatability can be achieved between different collections of concepts.

**Theory of change:** Understand the concepts a system's understanding is structured with and use them to inspect its "alignment/safety properties" and/or "retarget its search", i.e. identify utility-function-like components inside an AI and replacing calls to them with calls to "user values" (represented using existing abstractions inside the AI).
**See also:** causal abstractions, representational alignment, convergent abstractions, feature universality, Platonic representation hypothesis, microscope AI
**Orthodox problems:** 5\. Instrumental convergence, 7\. Superintelligence can fool human supervisors, 9\. Humans cannot be first-class parties to a superintelligent value handshake
**Target case:** worst-case
**Broad approach:** cognitive
**Some names:** John Wentworth, Paul Colognese, David Lorrell, Sam Eisenstat
**Estimated FTEs:** 1-10
**Critiques:** [Chan et al (2023)](https://www.lesswrong.com/posts/gvzW46Z3BsaZsLc25/natural-abstractions-key-claims-theorems-and-critiques-1#3__A_formalization_of_abstractions_would_accelerate_alignment_research), [Soto](https://www.lesswrong.com/posts/CJjT8GMitsnKc2wgG/natural-abstractions-are-observer-dependent-a-conversation-1), [Harwood](https://www.lesswrong.com/posts/F4nzox6oh5oAdX9D3/abstractions-are-not-natural), [Soares (2023)](https://www.lesswrong.com/posts/mgjHS6ou7DgwhKPpu/a-rough-and-incomplete-review-of-some-of-john-wentworth-s)
**Funded by:** ?
**Outputs in 2025:**

* [**Natural Latents: Latent Variables Stable Across Ontologies**](https://arxiv.org/abs/2509.03780), *John Wentworth, David Lorell*, 2025-09-04, LessWrong
* [**Abstract Mathematical Concepts vs. Abstractions Over Real-World Systems**](https://www.lesswrong.com/posts/T6xSXiXF3WF6TmCyN/abstract-mathematical-concepts-vs-abstractions-over-real), *Thane Ruthenis*, 2025-02-18, LessWrong
* [**Condensation: a theory of concepts**](https://openreview.net/forum?id=HwKFJ3odui), *Sam Eisenstat*, 2025-07-04, ODYSSEY 2025 Conference
* [Condensation](https://www.lesswrong.com/posts/BstHXPgQyfeNnLjjp/condensation), *Abram Demski*
* [**Getting aligned on representational alignment**](https://arxiv.org/abs/2310.13018)
* [**Platonic representation hypothesis**](https://phillipi.github.io/prh/)
* [**Rosas**](https://www.youtube.com/watch?v=Nr9eMobqUOo&t=3s)
* [**Factored space models: Towards causality between levels of abstraction**](https://arxiv.org/abs/2412.02579)
* [**A single principle related to many Alignment subproblems?**](https://lesswrong.com/posts/h89L5FMAkEBNsZ3xM/a-single-principle-related-to-many-alignment-subproblems-2)

## The Learning-Theoretic Agenda \[a:learning\_theoretic\_agenda\]

**Who edits (internal): Stag ✅**
**One-sentence summary:** Try to formalise a more realistic agent, understand what it means for it to be aligned with us, translate between its ontology and ours, and produce desiderata for a training setup that points at coherent AGIs similar to our model of an aligned agent
**Theory of change:** Fix formal epistemology to work out how to avoid deep training problems
**See also:**
**Orthodox problems:** 1\. Value is fragile and hard to specify, 4\. goals misgeneralize, 9\. Humans cannot be first-class parties to a superintelligent value handshake
**Target case:** worst-case
**Broad approach:** cognitive
**Some names:** Vanessa Kosoy, Diffractor, Gergely Szücs
**Estimated FTEs:** 3
**Critiques:** [Matolcsi](https://www.lesswrong.com/posts/StkjjQyKwg7hZjcGB/a-mostly-critical-review-of-infra-bayesianism)
**Funded by:** Survival and Flourishing Fund, ARIA, UK AISI, Coefficient Giving
**Outputs in 2025:**
* [**Infra-Bayesian Decision-Estimation Theory**](https://www.lesswrong.com/posts/LgLez8aeK24PbyyQJ/new-paper-infra-bayesian-decision-estimation-theory), *Vanessa Kosoy, Diffractor*, 2025-04-10, LessWrong
* [**Infra-Bayesianism category on LessWrong**](https://www.lesswrong.com/w/infra-bayesianism?sortedBy=new)
* [**Ambiguous Online Learning**](https://www.lesswrong.com/posts/Y9NuKpb6dsyiYFxWK/new-paper-ambiguous-online-learning), *Vanessa Kosoy*, 2025-06-25, LessWrong
* [**Regret Bounds for Robust Online Decision Making**](https://proceedings.mlr.press/v291/appel25a.html), *Alexander Appel, Vanessa Kosoy*, 2025-04-09, arXiv
* [**What is Inadequate about Bayesianism for AI Alignment: Motivating Infra-Bayesianism**](https://www.lesswrong.com/s/n7qFxakSnxGuvmYAX), *Brittany Gelb*, 2025-05-01, LessWrong
* [Non-Monotonic Infra-Bayesian Physicalism](https://www.alignmentforum.org/posts/DobZ62XMdiPigii9H/non-monotonic-infra-bayesian-physicalism%20)
# Multi-agent first \[sec:multi\_agent\_first\]


## Aligning to context \[a:alignment\_to\_context\]

**Who edits (internal):** **Peli**

**One-sentence summary:** Align AI directly to the role of participant, collaborator, or advisor for our best real human practices and institutions, instead of aligning AI to separately representable goals, rules, or utility functions.

**Theory of change: “**Many classical problems in AGI alignment are downstream of a type error about human values.” Operationalizing a correct view of human values \- one that treats human values as impossible or impractical to abstract from concrete practices \- will unblock value fragility, goal-misgeneralization, instrumental convergence, and pivotal-act specification.

**See also:** Aligning what?, Aligned to who?
**Orthodox problems:** 1\. Value is fragile and hard to specify, 2\. Corrigibility is anti-natural, 4\. Goals misgeneralize out of distribution, 5\. Instrumental convergence, 13\. Fair, sane pivotal processes
**Target case:** Mixed
**Broad approach:** Behavioural
**Some names:** Full Stack Alignment, Meaning Alignment Institute, Plurality Institute, Tan Zhi-Xuan, Matija Franklin, Ryan Lowe, Joe Edelman, Oliver Klingefjord
**Estimated FTEs:** 5
**Critiques:** not found
**Funded by:** ARIA, OpenAI, Survival and Flourishing Fund

**Outputs in 2025:**

[Beyond Preferences in AI Alignment](https://arxiv.org/abs/2408.16984)

[2404.10636 \- What are human values, and how do we align AI to them?](https://arxiv.org/abs/2404.10636)

[2503.00940 \- Can AI Model the Complexities of Human Moral Decision-Making? A Qualitative Study of Kidney Allocation Decisions](https://arxiv.org/abs/2503.00940)

[The Frame-Dependent Mind](https://www.softmax.com/blog/the-frame-dependent-mind)
[Model Integrity](https://meaningalignment.substack.com/p/model-integrity)
[On Eudaimonia and Optimization](https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20)
[Full-Stack Alignment](https://www.full-stack-alignment.ai)

https://arxiv.org/abs/2412.19010

## Aligning to the social contract \[a:contractualist\_alignment\]

**Who edits (internal):** **Peli**

**One-sentence summary:** Generate AIs’ operational values from ‘social contract’-style ideal civic deliberation formalisms and their consequent rulesets for civic actors

**Theory of change:** Formalize and apply the liberal tradition's project of defining civic principles separable from the substantive good, aligning our AIs to civic principles that bypass fragile utility-learning and intractable utility-calculation

**See also:** Alignment to context,  Aligning what
**Orthodox problems:** 1\. Value is fragile and hard to specify, 4\. Goals misgeneralize out of distribution, 5\. Instrumental convergence, 10\. Humanlike minds/goals are not necessarily safe, 13\. Fair, sane pivotal processes,

**Target case:** Mixed
**Broad approach:** Cognitive
**Some names:** Gillian Hadfield**,** Tan Zhi-Xuan, Sydney Levine, Matija Franklin, Joshua B. Tenenbaum

**Estimated FTEs:** 5 \- 10
**Critiques:** not found
**Funded by:** Deepmind, Macroscopic Ventures
**Outputs in 2025:**

[Law-Following AI: designing AI agents to obey human laws](https://law-ai.org/law-following-ai/%20) Cullen O'Keefe, Ketan Ramakrishnan, Janna Tay, Christoph Winter

[2506.17434 \- Resource Rational Contractualism Should Guide AI Alignment](https://arxiv.org/abs/2506.17434)

[2509.07955 \- ACE and Diverse Generalization via Selective Disagreement](https://arxiv.org/abs/2509.07955)

[Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post Verifiable Commitments](https://arxiv.org/abs/2505.00783)

[A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)

[Statutory Construction and Interpretation for Artificial Intelligence](https://arxiv.org/abs/2509.01186)

[2408.16984 \- Beyond Preferences in AI Alignment](https://arxiv.org/abs/2408.16984)

[Societal alignment frameworks can improve llm alignment](https://arxiv.org/abs/2503.00069)

## Theory for aligning multiple AIs \[a:aligning\_multiple\_game\_theory\]

**Who edits (internal):** **Peli**

**One-sentence summary:** Use realistic game-theory variants (e.g. evolutionary game theory, computational game theory) to describe/predict the collective and individual behaviours of AI agents in multi-agent scenarios
**Theory of change:** While traditional AGI safety focuses on idealized decision-theory and individual agents, it’s plausible that strategic AI agents will first emerge (or are emerging now) in a complex, multi-AI strategic landscape. We need granular, realistic formal models of AIs’ strategic interactions and collective dynamics to understand this future

**See also:** Aligning multiple AIs: tools and techniques, Aligning what
**Orthodox problems:** 4\. Goals misgeneralize out of distribution, 7\. Superintelligence can fool human supervisors,  8\. Superintelligence can hack software supervisors
**Target case:** Mixed
**Broad approach:** Cognitive
**Some names:** Lewis Hammond, Emery Cooper, Allan Chan, Caspar Oesterheld, Vincent Conitzer, Vojta Kovarik
**Estimated FTEs:** 10
**Critiques:** not found
**Funded by:**  Deepmind, Macroscopic Ventures
**Outputs in 2025:**

[2502.14143 \- Multi-Agent Risks from Advanced AI](https://arxiv.org/abs/2502.14143)

[2503.06323 \- Higher-Order Belief in Incomplete Information MAIDs](https://arxiv.org/abs/2503.06323)

[AI Testing Should Account for Sophisticated Strategic Behaviour](https://arxiv.org/abs/2508.14927)

[Characterising Simulation-Based Program Equilibria](https://arxiv.org/abs/2412.14570)

[Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)

[Communication Enables Cooperation in LLM Agents](https://arxiv.org/abs/2510.05748)

[Emergent social conventions and collective bias in LLM populations](https://www.science.org/doi/10.1126/sciadv.adu9368)

[An Economy of AI Agents](https://arxiv.org/abs/2509.01063) \- Gillian K. Hadfield, Andrew Koh

[Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences](https://arxiv.org/abs/2510.06105)

## Tools for aligning multiple AIs \[a:aligning\_multiple\_tools\]

**Who edits (internal):** **Peli**

**One-sentence summary:** Develop tools and technique for designing and testing multi-agent AI scenarios, for auditing real-world multi-agent AI dynamics, and for aligning AIs  in multi-AI settings

**Theory of change:** Addressing multi-agent AI dynamics is key for aligning near-future agents and their impact on the world. Feedback loops from multi-agent dynamics can radically change the future AI landscape, and require a different toolset from model psychology to audit and control

**See also:** Aligning multiple AIs: game theory , Aligning what
**Orthodox problems:** 4\. Goals misgeneralize out of distribution, 7\. Superintelligence can fool human supervisors,  8\. Superintelligence can hack software supervisors
**Target case:** Mixed
**Broad approach:** Engineering/Behavioural
**Some names:** Lewis Hammond, Emery Cooper, Allan Chan, Caspar Oesterheld, Vincent Conitzer, Gillian Hadfield
**Estimated FTEs:** 10 \- 15
**Critiques:** not found
**Funded by:** Coefficient Giving, Deepmind

[Infrastructure for AI Agents](https://arxiv.org/abs/2501.10114)

[The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295)

[A dataset of questions on decision-theoretic reasoning in Newcomb-like problems](https://arxiv.org/abs/2411.10588)

[Reimagining Alignment](https://softmax.com/blog/reimagining-alignment)

[PGG-Bench: Contribute & Punish](https://github.com/lechmazur/pgg_bench)

“[Beyond the high score: Prosocial ability profiles of multi-agent populations](https://arxiv.org/abs/2509.14485)”

[Multiplayer Nash Preference Optimization](https://arxiv.org/abs/2509.23102)

[2502.12203 \- An Interpretable Automated Mechanism Design Framework with Large Language Models](https://arxiv.org/abs/2502.12203)

[AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement](https://arxiv.org/abs/2502.00757)

[When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)

[Virtual Agent Economies](http://arxiv.org/abs/2509.10147)

## Aligned to who? \[a:aligned\_to\_who\]

**Who edits (internal):** **Peli**

**One-sentence summary:** Develop ethical frameworks and technical protocols for taking the plurality of human values, cultures, and communities seriously when aligning AI to “humanity”
**Theory of change:** Principles of democracy, of pluralism, and of context-sensitivity should guide AI development, alignment, and deployment from the start, continuously shaping AI’s social and technical feedback loop on the road to AGI

**See also:** Aligning what, Alignment to context
**Orthodox problems:** 1\. Value is fragile and hard to specify, 13\. Fair, sane pivotal processes
**Target case:** average_case
**Broad approach:** Behavioural
**Some names:** Joel Z. Leibo, Divya Siddarth, Séb Krier, Luke Thorburn,  Seth Lazar, AI Objectives Institute, The Collective Intelligence Project,

**Estimated FTEs:** 5 \- 15
**Critiques:** not found
**Funded by:**  Future of Life Institute, Survival and Flourishing Fund, Deepmind

**Outputs in 2025:**

[2507.09650 \- Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)

[2503.05728 \- Political Neutrality in AI Is Impossible \- But Here Is How to Approximate It](https://arxiv.org/abs/2503.05728)

[The AI Power Disparity Index: Toward a Compound Measure of AI Actors' Power to Shape the AI Ecosystem](https://ojs.aaai.org/index.php/AIES/article/view/36645)

[“Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt”](https://arxiv.org/abs/2505.05197)

[“Democratic AI is Possible: The Democracy Levels Framework Shows How It Might Work”](https://arxiv.org/abs/2411.09222)

[Research Agenda for Sociotechnical Approaches to AI Safety](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5097286)

[Build Agent Advocates, Not Platform Agents](https://arxiv.org/abs/2505.04345)

[Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)

## Aligning what? \[a:aligning\_what\]

**Who edits (internal):** **Peli**

**One-sentence summary:** Develop alternatives to agent-level models of alignment, by treating human-AI interactions, AI-assisted institutions, AI economic or cultural systems, drives within one AI, and other causal/constitutive processes as subject to alignment

**Theory of change:** Model multiple reality-shaping processes above and below the level of the individual AI, some of which are themselves quasi-agential (e.g. cultures) or intelligence-like (e.g. markets), will develop AI alignment into a mature science for managing the transition to an AGI civilization

**See also:** Aligning multiple AIs: game theory, Alignment to context, Aligned to who
**Orthodox problems:** 1\. Value is fragile and hard to specify, 2\. Corrigibility is anti-natural, 4\. Goals misgeneralize out of distribution, 5\. Instrumental convergence, 13\. Fair, sane pivotal processes

**Target case:** Mixed
**Broad approach:** Behavioural/Cognitive
**Some names:**  Richard Ngo, Emmett Shear, Softmax, Full Stack Alignment, AI Objectives Institute, Sahil, TJ, Andrew Critch
**Estimated FTEs:** 5-10
**Critiques:** not found
**Funded by:** Future of Life Institute, Emmet Shear
**Outputs:**

[Towards a Scale-Free Theory of Intelligent Agency](https://www.alignmentforum.org/posts/5tYTKX4pNpiG4vzYg/towards-a-scale-free-theory-of-intelligent-agency)

[Alignment first, intelligence later](https://chrislakin.blog/p/alignment-first-intelligence-later)

[Emmett Shear on Building AI That Actually Cares: Beyond Control and Steering](https://a16z.simplecast.com/episodes/emmett-shear-on-building-ai-that-actually-cares-beyond-control-and-steering-TRwfxH0r)

[End A Subset Of Conversations](https://www.anthropic.com/research/end-subset-conversations)
[Full-Stack Alignment](https://www.full-stack-alignment.ai)
[On Eudaimonia and Optimization](https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20)

[AI Governance through Markets](https://arxiv.org/abs/2501.17755)

[Collective cooperative intelligence](https://www.pnas.org/doi/abs/10.1073/pnas.2319948121)

[Multipolar AI is Underrated](https://www.lesswrong.com/posts/JjYu75q3hEMBgtvr8/multipolar-ai-is-underrated)

[What, if not agency?](https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency)

[https://substack.com/home/post/p-171042125](https://substack.com/home/post/p-171042125)

[The Multiplicity Thesis, Collective Intelligence, and Morality](https://themultiplicity.ai/blog/thesis)

# Evals \[sec:evals\]

## AGI metrics \[a:agi\_metrics\]

**Who edits (internal):** **Stephen** ✅

**One-sentence summary:** Evals with the explicit aim of measuring progress towards full human-level generality.
**Theory of change:** Help predict timelines for risk awareness and strategy.
**See also:** capability evals
**Orthodox problems:** none; a barometer of risk.
**Target case:** mixed
**Broad approach:** behavioral
**Some names:** CAIS, CFI Kinds of Intelligence, Apart Research, OpenAI, METR, Lexin Zhou, Adam Scholl, Lorenzo Pacchiardi.
**Estimated FTEs:** 10-50
**Critiques:** [Is the Definition of AGI a Percentage?](https://aievaluation.substack.com/p/is-the-definition-of-agi-a-percentage)**,** [The "Length" of "Horizons"](https://www.lesswrong.com/posts/PzLSuaT6WGLQGJJJD/the-length-of-horizons)
**Funded by:** Leverhulme Trust, Open Philanthropy, Long-Term Future Fund.
**Outputs in 2025:**

* [GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks](https://arxiv.org/abs/2510.04374)
* [HCAST: Human-Calibrated Autonomy Software Tasks](https://arxiv.org/abs/2503.17354)
* [A Definition of AGI](https://arxiv.org/pdf/2510.18212)
* [Remote Labor Index](https://scale.com/leaderboard/rli)
* [ADeLe v1.0: A battery for AI Evaluation with explanatory and predictive power](https://kinds-of-intelligence-cfi.github.io/ADELE/)

## Capability evals \[a:evals\_capability\]

**Who edits (internal):** **Stephen** ✅
**One-sentence summary:** Make tools that can actually check whether a model has a certain capability or propensity. We default to low-n sampling of a vast latent space but aim to do better.
**Theory of change:** Keep a close eye on what capabilities are acquired when, so that frontier labs and regulators are better informed on what security measures are already necessary (and hopefully they extrapolate). You can’t regulate without them.
**See also:** [Deepmind’s frontier safety framework](https://deepmind.google/blog/strengthening-our-frontier-safety-framework/), [Aether](https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update).
**Orthodox problems:** none; a barometer for risk.
**Target case:** average_case.
**Broad approach:** behavioral.
**Some names:** METR, AISI, Apollo Research, Marrius Hobbhahn, Meg Tong, Mary Phuong, Beth Barnes, Thomas Kwa, Joel Becker.

**Estimated FTEs:** 100+
**Critiques:** [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836)**,** [AI Sandbagging: Language Models can Strategically Underperform on Evaluations](https://arxiv.org/abs/2406.07358), [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879), [Do Large Language Model Benchmarks Test Reliability?](https://arxiv.org/abs/2502.03461)
**Funded by:** basically everyone. Google, Microsoft, Open Philanthropy, LTFF, Governments etc
**Outputs in 2025:**

* [**Forecasting Rare Language Model Behaviors**](https://arxiv.org/abs/2502.16797), *Erik Jones, Meg Tong, Jesse Mu et al.*, 2025-02-24, arXiv
* [**Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities**](https://arxiv.org/abs/2502.05209), *Zora Che, Stephen Casper, Robert Kirk et al.*, 2025-02-03, arXiv (accepted to TMLR)
* [**The Elicitation Game: Evaluating Capability Elicitation Techniques**](https://arxiv.org/abs/2502.02180), *Felix Hofstätter, Teun van der Weij, Jayden Teoh et al.*, 2025-02-04, arXiv
* [**A Toy Evaluation of Inference Code Tampering**](https://alignment.anthropic.com/2024/rogue-eval/index.html), 2024, Anthropic Alignment Science Blog
* [**Automated Capability Discovery via Foundation Model Self-Exploration**](https://arxiv.org/abs/2502.07577), *Cong Lu, Shengran Hu, Jeff Clune*, 2025-02-11, arXiv
* [**Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity**](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/), 2025-07-10, METR Blog
* [**When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas**](https://arxiv.org/abs/2505.19212), *Steffen Backmann, David Guzman Piedrahita, Emanuel Tewolde et al.*, 2025-05-25, arXiv
* [**AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons**](https://arxiv.org/abs/2503.05731), *Shaona Ghosh, Heather Frase, Adina Williams et al.*, 2025-04-18, arXiv
* [**Petri: An open-source auditing tool to accelerate AI safety research**](https://www.anthropic.com/research/petri-open-source-auditing), *Kai Fronsdal, Isha Gupta, Abhay Sheshadri et al.*, 2025-10-06, Anthropic Research Blog
* [**Research Note: Our scheming precursor evals had limited predictive power for our in-context scheming evals**](https://www.apolloresearch.ai/blog/research-note-our-scheming-precursor-evals-had-limited-predictive-power-for-our-in-context-scheming-evals), *Marius Hobbhahn*, 2025-07-03, Apollo Research Blog
* [**Adversarial ML Problems Are Getting Harder to Solve and to Evaluate**](https://arxiv.org/abs/2502.02260), *Javier Rando, Jie Zhang, Nicholas Carlini et al.*, 2025-02-04, arXiv
* [**Among AIs**](https://www.4wallai.com/amongais), 4Wall AI Website
* [**Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods**](https://arxiv.org/abs/2505.05541), *Markov Grey, Charbel-Raphaël Segerie*, 2025-05-08, arXiv
* [**Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks**](https://arxiv.org/abs/2502.18339), *Rylan Schaeffer, Punit Singh Koura, Binh Tang et al.*, 2025-02-24, arXiv
* [**The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input**](https://arxiv.org/abs/2501.03200), *Alon Jacovi, Andrew Wang, Chris Alberti et al.*, 2025-01-06, arXiv
* [**Evaluating Language Model Reasoning about Confidential Information**](https://arxiv.org/abs/2508.19980), *Dylan Sam, Alexander Robey, Andy Zou et al.*, 2025-08-27, arXiv
* [**Evaluating the Goal-Directedness of Large Language Models**](https://arxiv.org/abs/2504.11844), *Tom Everitt, Cristina Garbacea, Alexis Bellot et al.*, 2025-04-16, arXiv
* [**Generative Value Conflicts Reveal LLM Priorities**](https://arxiv.org/abs/2509.25369), *Andy Liu, Kshitish Ghate, Mona Diab et al.*, 2025-09-29, arXiv
* [**Technical Report: Evaluating Goal Drift in Language Model Agents**](https://arxiv.org/abs/2505.02709), *Rauno Arike, Elizabeth Donoway, Henning Bartsch et al.*, 2025-05-05, arXiv
* [**MALT: A Dataset of Natural and Prompted Behaviors That Threaten Eval Integrity**](https://metr.org/blog/2025-10-14-malt-dataset-of-natural-and-prompted-behaviors/)
* [**Predicting the Performance of Black-box LLMs through Self-Queries**](https://arxiv.org/abs/2501.01558), *Dylan Sam, Marc Finzi, J. Zico Kolter*, 2025-01-02, arXiv
* [**Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index**](https://arxiv.org/abs/2506.12229), *Hao Xu, Jiacheng Liu, Yejin Choi et al.*, 2025-06-13, arXiv
* [**Hyperbolic model fits METR capabilities estimate worse than exponential model**](https://lesswrong.com/posts/ZEuDH2W3XdRaTwpjD/hyperbolic-model-fits-metr-capabilities-estimate-worse-than), *gjm*, 2025-08-19, LessWrong
* [**New website analyzing AI companies' model evals**](https://lesswrong.com/posts/nmaKpoHxmzjT8yXTk/new-website-analyzing-ai-companies-model-evals), *Zach Stein-Perlman*, 2025-05-26, LessWrong
* [**Research Note: Our scheming precursor evals had limited predictive power for our in-context scheming evals**](https://lesswrong.com/posts/9tqpPP4FwSnv9AWsi/research-note-our-scheming-precursor-evals-had-limited), *Marius Hobbhahn*, 2025-07-03, LessWrong
* [**How Fast Can Algorithms Advance Capabilities? | Epoch Gradient Update**](https://lesswrong.com/posts/qhjNejRxbMGQp4wHt/how-fast-can-algorithms-advance-capabilities-or-epoch), *Henry Josephson, Spencer Guo, Teddy Foley et al.*, 2025-05-16, LessWrong
* [**Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods**](https://lesswrong.com/posts/CwdCYmsutwXwnYtEF/paper-safety-by-measurement-a-systematic-literature-review), *markov, Charbel-Raphaël*, 2025-05-19, arXiv
* [**We should try to automate AI safety work asap**](https://lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap), *Marius Hobbhahn*, 2025-04-26, LessWrong
* [**Validating against a misalignment detector is very different to training against one**](https://lesswrong.com/posts/CXYf7kGBecZMajrXC/validating-against-a-misalignment-detector-is-very-different), *mattmacdermott*, 2025-03-04, LessWrong
* [**Why do misalignment risks increase as AIs get more capable?**](https://lesswrong.com/posts/NDotm7oLHfR56g4sD/why-do-misalignment-risks-increase-as-ais-get-more-capable), *Ryan Greenblatt*, 2025-04-11, LessWrong
* [**Open Philanthropy Technical AI Safety RFP \- $40M Available Across 21 Research Areas**](https://lesswrong.com/posts/wbJxRNxuezvsGFEWv/open-philanthropy-technical-ai-safety-rfp-usd40m-available), *jake\_mendel, maxnadeau, Peter Favaloro*, 2025-02-06, LessWrong
* [**Why Future AIs will Require New Alignment Methods**](https://lesswrong.com/posts/TxiB6hvnQqxXB5XDJ/why-future-ais-will-require-new-alignment-methods), *Alvin Ånestrand*, 2025-10-10, LessWrong
* [**100+ concrete projects and open problems in evals**](https://lesswrong.com/posts/LhnqegFoykcjaXCYH/100-concrete-projects-and-open-problems-in-evals), *Marius Hobbhahn*, 2025-03-22, LessWrong
* [**AI companies should be safety-testing the most capable versions of their models**](https://lesswrong.com/posts/tQzeafo9HjCeXn7ZF/ai-companies-should-be-safety-testing-the-most-capable), *Steven Adler*, 2025-03-26, LessWrong

## Autonomy evals \[a:evals\_autonomy\]

**Who edits (internal):** **Stephen ✅**
**One-sentence summary:** Measure an AI's ability to act autonomously to complete long-horizon, complex tasks.
**Theory of change:** By measuring how long and complex a task an AI can complete (its "time horizon"), we can track capability growth and identify when models gain dangerous autonomous capabilities (like R\&D acceleration or replication).
**See also:** [capability evals](#capability-evals-[a:evals_capability]), [OpenAI Preparedness](https://openai.com/index/updating-our-preparedness-framework/), [Anthropic RSP](https://www.anthropic.com/rsp-updates).
**Orthodox problems:** none; a barometer for risk.
**Target case:** average_case
**Broad approach:** behavioral
**Some names:** METR, Thomas Kwa, Ben West, Joel Becker, Beth Barnes, Hjalmar Wijk, Tao Lin, Giulio Starace, Oliver Jaffe, Dane Sherburn, Sanidhya Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou.
**Estimated FTEs:** 10-50.
**Critiques:** [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity.](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) [The "Length" of "Horizons"](https://www.lesswrong.com/posts/PzLSuaT6WGLQGJJJD/the-length-of-horizons)
**Funded by:** The Audacious Project, Open Philanthropy.
**Outputs in 2025:**

* [**Measuring AI Ability to Complete Long Tasks**](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/), *Thomas Kwa, Ben West, Joel Becker et al.*, 2025-03-19, METR Blog / arXiv
* [**Details about METR's evaluation of OpenAI GPT-5**](https://metr.github.io/autonomy-evals-guide/gpt-5-report/), *METR*, 2025-08-01, METR's Autonomy Evaluation Resources
* [**RE-Bench: Evaluating frontier AI R\&D capabilities of language model agents against human experts**](https://arxiv.org/abs/2411.15114), *Hjalmar Wijk, Tao Lin, Joel Becker et al.*, 2024-11-22, arXiv
* [**GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments**](https://arxiv.org/abs/2509.21998), *Hanlin Zhu, Tianyu Guo, Song Mei et al.*, 2025-09-26, arXiv
* [**OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety**](https://t.co/XfspwlzYdl), *Sanidhya Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou et al.*, 2025-07-08, arXiv
* [**Details about METR's preliminary evaluation of OpenAI's o3 and o4-mini**](https://metr.github.io/autonomy-evals-guide/openai-o3-report/), 2025-04-01, METR's Autonomy Evaluation Resources
* [**PaperBench: Evaluating AI's Ability to Replicate AI Research**](https://t.co/dHN2N0tUhC), *Giulio Starace, Oliver Jaffe, Dane Sherburn et al.*, 2025-04-02, arXiv
* [**How Does Time Horizon Vary Across Domains?**](https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/), 2025-07-14, METR Blog
* [**Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents**](https://arxiv.org/abs/2502.15840), *Axel Backlund, Lukas Petersson*, 2025-02-20, arXiv
* [**Forecasting Frontier Language Model Agent Capabilities**](https://arxiv.org/abs/2502.15850), *Govind Pimpale, Axel Højmark, Jérémy Scheurer et al.*, 2025-02-21, arXiv
* [**Project Vend: Can Claude run a small shop? (And why does that matter?)**](https://www.anthropic.com/research/project-vend-1), 2025-06-27, Anthropic Blog
* [**OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents**](https://arxiv.org/abs/2506.14866), *Thomas Kuntz, Agatha Duzan, Hao Zhao et al.*, 2025-06-17, arXiv
* [Fulcrum](https://fulcrumresearch.ai/2025/10/22/introducing-orchestra-quibbler.html).

## WMD evals (Weapons of Mass Destruction) \[a:evals\_wmd\]

**Who edits (internal):** **Stephen ✅**
**One-sentence summary:** Evaluate whether AI models possess dangerous knowledge or capabilities related to biological and chemical weapons, such as biosecurity or chemical synthesis.
**Theory of change:** By benchmarking and tracking AI's knowledge of biology and chemistry, we can identify when models become capable of accelerating WMD development or misuse, allowing for timely intervention.
**See also:** various capability evals, autonomy evals, various red teams.
**Orthodox problems:** malicious actors using AI.
**Target case:** pessimistic.
**Broad approach:** behavioral.
**Some names:** Lennart Justen, Haochen Zhao, Xiangru Tang, Ziran Yang, Aidan Peppin, Anka Reuel, Stephen Casper.
**Estimated FTEs:** 10-50.
**Critiques:** [The Reality of AI and Biorisk](https://arxiv.org/abs/2412.01946)
**Funded by:** Open Philanthropy, UK AI Safety Institute (AISI), frontier labs, Scale AI, various academic institutions (Peking University, Yale, etc.), Meta.
**Outputs in 2025:**

* [**Virology Capabilities Test (VCT): A Multimodal Virology Q\&A Benchmark**](https://arxiv.org/abs/2504.16137)
* [**LLMs Outperform Experts on Challenging Biology Benchmarks**](https://arxiv.org/abs/2505.06108), *Lennart Justen*, 2025-05-09, arXiv
* [**ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain**](https://arxiv.org/abs/2411.16736), *Haochen Zhao, Xiangru Tang, Ziran Yang et al.*, 2024-11-23, arXiv
* [**The Reality of AI and Biorisk**](https://arxiv.org/abs/2412.01946), *Aidan Peppin, Anka Reuel, Stephen Casper et al.*, 2024-12-02, arXiv
* [**The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models**](https://arxiv.org/abs/2507.11544), *Ann-Kathrin Dombrowski, Dillon Bowen, Adam Gleave et al.*, 2025-07-08, arXiv
* [**Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models**](https://arxiv.org/abs/2510.27629), *Boyi Wei, Zora Che, Nathaniel Li et al.*, 2025-10-31, arXiv

## Situational awareness and self-awareness evals \[a:evals\_situational\_awareness\]

**Who edits (internal):** **Stephen ✅**
**One-sentence summary:** Evaluate if models understand their own internal states and behaviors, their environment, and whether they are in a test or real-world deployment.
**Theory of change:** if an AI can distinguish between evaluation and deployment (“evaluation awareness”), it might hide dangerous capabilities (scheming/sandbagging). By measuring self- and situational-awareness, we can better assess this risk and build more robust evaluations.
**See also:** evals sandbagging, various redteams, model psychology.
**Orthodox problems:** 7\. Superintelligence can fool human supervisors, 8\. Superintelligence can hack software supervisors.
**Target case:** worst case.
**Broad approach:** behavioral.
**Some names:** Jan Betley, Xuchan Bao, Martín Soto, Mary Phuong, Roland S. Zimmermann, Joe Needham, Giles Edkins, Govind Pimpale, Kai Fronsdal, David Lindner, Lang Xiong, Xiaoyan Bai.
**Estimated FTEs:** 30-70.
**Critiques**: [Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language](https://arxiv.org/abs/2507.03409), [It's hard to make scheming evals look realistic for LLMs](https://www.lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms).
**Funded by:** frontier labs (Google DeepMind, Anthropic), Open Philanthropy, The Audacious Project, UK AI Safety Institute (AISI), AI Safety Support, Apollo Research, METR.
**Outputs in 2025:**

* [**Tell me about yourself: LLMs are aware of their learned behaviors**](https://arxiv.org/abs/2501.11120), *Jan Betley, Xuchan Bao, Martín Soto et al.*, 2025-01-19, arXiv
* [**Future Events as Backdoor Triggers**](https://arxiv.org/pdf/2407.04108)
* [**Evaluating Frontier Models for Stealth and Situational Awareness**](https://arxiv.org/abs/2505.01420), *Mary Phuong, Roland S. Zimmermann, Ziyue Wang et al.*, 2025-05-02, arXiv
* [**Large Language Models Often Know When They Are Being Evaluated**](https://arxiv.org/abs/2505.23836), *Joe Needham, Giles Edkins, Govind Pimpale et al.*, 2025-05-28, arXiv
* [**Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness**](https://arxiv.org/abs/2509.00591), *Lang Xiong, Nishant Bhargava, Jianhang Hong et al.*, 2025-08-30, arXiv
* [**Know Thyself? On the Incapability and Implications of AI Self-Recognition**](https://arxiv.org/abs/2510.03399), *Xiaoyan Bai, Aryan Shrivastava, Ari Holtzman et al.*, 2025-10-03, arXiv
* [**Chain-of-Thought Snippets — Anti-Scheming**](https://www.antischeming.ai/snippets), antischeming.ai
* [**AI Awareness (literature review)**](https://arxiv.org/pdf/2504.20084)
* [**Do LLMs know what they're capable of? Why this matters for AI safety, and initial findings**](https://lesswrong.com/posts/9tHEibBBhQCHEyFsa/do-llms-know-what-they-re-capable-of-why-this-matters-for-ai), *Casey Barkan, Sid Black, Oliver Sourbut*, 2025-07-13, LessWrong / AI Alignment Forum
* [**Claude Sonnet 3.7 (often) knows when it's in alignment evaluations**](https://lesswrong.com/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment), *Nicholas Goldowsky-Dill, Mikita Balesni, Jérémy Scheurer et al.*, 2025-03-17, LessWrong
* [**It's hard to make scheming evals look realistic for LLMs**](https://lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms), *Igor Ivanov, Danil Kadochnikov*, 2025-05-24, LessWrong

## Steganography evals \[a:evals\_steganography\]

**Who edits (internal):** **Stephen ✅**
**One-sentence summary:** evaluate whether models can hide secret information or encoded reasoning in their outputs, such as in chain-of-thought scratchpads, to evade monitoring.
**Theory of change:** if models can use steganography, they could hide deceptive reasoning, bypassing safety monitoring and control measures. By evaluating this capability, we can assess the risk of a model fooling its supervisors.
**See also:** AI deception, chain of thought monitoring.
**Orthodox problems:** 12\. A boxed AGI might exfiltrate itself by steganography, spearphishing, 7\. Superintelligence can fool human supervisors.
**Target case:** worst case.
**Broad approach:** behavioral.
**Some names:** Antonio Norelli, Michael Bronstein.
**Estimated FTEs:** 1-10.
**Critiques:** Chain-of-Thought Is Already Unfaithful (So Steganography is Irrelevant): [Reasoning Models Don’t Always Say What They Think.](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf)
**Funded by:** Anthropic (and its general funders, e.g., Google, Amazon).
**Outputs in 2025:**

* [**LLMs can hide text in other text of the same length**](https://arxiv.org/abs/2510.20075), *Antonio Norelli, Michael Bronstein*, 2025-10-27, arXiv
* [**Do reasoning models use their scratchpad like we do? Evidence from distilling paraphrases**](https://alignment.anthropic.com/2025/distill-paraphrases/), 2025, Anthropic Alignment Science Blog
* [**Early Signs of Steganographic Capabilities in Frontier LLMs**](https://arxiv.org/abs/2507.02737)
* [**Large language models can learn and generalize steganographic chain-of-thought under process supervision**](https://arxiv.org/abs/2506.01926)
* [**Subliminal Learning: Language models transmit behavioral traits via hidden signals in data**](https://arxiv.org/abs/2507.14805), *Alex Cloud, Minh Le, James Chua et al.*, 2025-07-20, arXiv

## AI deception evals \[a:ai\_deception\]

**Who edits (internal):** **Stephen ✅**

**One-sentence summary:** research demonstrating that AI models, particularly agentic ones, can learn and execute deceptive behaviors such as alignment faking, manipulation, and sandbagging.

**Theory of change:** proactively discover, evaluate, and understand the mechanisms of AI deception (e.g., alignment faking, manipulation, agentic deception) to prevent models from fooling human supervisors and causing harm.

**See also:** situational awareness and self-awareness, steganography, sandbagging, chain of thought monitoring.

**Orthodox problems:** 7\. Superintelligence can fool human supervisors, 8\. Superintelligence can hack software supervisors.

**Target case:** worst-case.

**Broad approach:** behavioural / engineering.

**Some names:** Cadenza, Fred Heiding, Simon Lermen, Andrew Kao, Myra Cheng, Cinoo Lee, Pranav Khadpe, Satyapriya Krishna, Andy Zou, Rahul Gupta.

**Estimated FTEs:** 30-80.

**Critiques:** A central criticism is that the evaluation scenarios are “artificial and contrived”. [the void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void) and [Lessons from a Chimp](https://arxiv.org/abs/2507.03409) argue this research is “overattributing human traits” to models.

**Funded by:** Lab funders (Anthropic, OpenAI), academic institutions (e.g., Harvard, CMU, Barcelona Institute of Science and Technology), NSFC, ML Alignment Theory & Scholars (MATS) Program, FAR AI.

**Outputs in 2025:**

* [**Liars' Bench: Evaluating Lie Detectors for Language Models**](https://arxiv.org/abs/2511.16035)
* [**The MASK Evaluation**](https://huggingface.co/datasets/cais/MASK), Hugging Face
* [**Alignment Faking Revisited: Improved Classifiers and Open Source Extensions**](https://alignment.anthropic.com/2025/alignment-faking-revisited/), *John Hughes, Abhay Sheshadr*, 2025, Anthropic Alignment Science Blog
* [**DECEPTIONBENCH: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenario**](https://arxiv.org/pdf/2510.15501)
* [**Among Us: A Sandbox for Measuring and Detecting Agentic Deception**](https://arxiv.org/abs/2504.04072)
* [**Evaluating Large Language Models' Capability to Launch Fully Automated Spear Phishing Campaigns: Validated on Human Subjects**](https://arxiv.org/abs/2412.00586), *Fred Heiding, Simon Lermen, Andrew Kao et al.*, 2024-11-30, arXiv
* [**D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models**](https://arxiv.org/abs/2509.17938), *Satyapriya Krishna, Andy Zou, Rahul Gupta et al.*, 2025-09-22, arXiv
* [**Why Do Some Language Models Fake Alignment While Others Don't?**](https://arxiv.org/pdf/2506.18032)
* [**Frontier Models are Capable of In-context Scheming**](https://arxiv.org/abs/2412.04984), *Alexander Meinke, Bronson Schoen, Jérémy Scheurer et al.*, 2024-12-06, arXiv
* [**Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL**](https://arxiv.org/abs/2510.14318), *Marwa Abdulhai, Ryan Cheng, Aryansh Shrivastava et al.*, 2025-10-16, arXiv
* [**Eliciting Secret Knowledge from Language Models**](https://arxiv.org/abs/2510.01070), *Bartosz Cywiński, Emil Ryd, Rowan Wang et al.*, 2025-10-01, arXiv
* [**Edge Cases in AI Alignment**](https://lesswrong.com/posts/bqWihHtDnDseyfF2T/edge-cases-in-ai-alignment-2), *Florian Dietz*, 2025-03-24, LessWrong
* [**I replicated the Anthropic alignment faking experiment on other models, and they didn't fake alignment**](https://lesswrong.com/posts/pCMmLiBcHbKohQgwA/i-replicated-the-anthropic-alignment-faking-experiment-on), *Aleksandr Kedrik, Igor Ivanov*, 2025-05-30, LessWrong
* [**Mistral Large 2 (123B) seems to exhibit alignment faking**](https://lesswrong.com/posts/kCGk5tp5suHoGwhCa/mistral-large-2-123b-seems-to-exhibit-alignment-faking), *Marc Carauleanu, Diogo de Lucena, Gunnar Zarncke et al.*, 2025-03-27, LessWrong/AI Alignment Forum

## Sandbagging evals \[a:evals\_sandbagging\]

**Who edits (internal):** **Stephen ✅**
**One-sentence summary:** Evaluate whether AI models deliberately hide their true capabilities or underperform, especially when they detect they are in an evaluation context.
**Theory of change:** If models can distinguish between evaluation and deployment contexts (“evaluation awareness”), they might learn to “sandbag” or deliberately underperform to hide dangerous capabilities, fooling safety evaluations. By developing evaluations for sandbagging, we can test whether our safety methods are being deceived and detect this behavior before a model is deployed.
**See also:** AI deception, situational awareness, various redteams.
**Orthodox problems:** 7\. Superintelligence can fool human supervisors; 8\. Superintelligence can hack software supervisors.
**Target case:** pessimistic.
**Broad approach:** behavioral.
**Some names:** Teun van der Weij, Cameron Tice, Chloe Li, Johannes Gasteiger, Joseph Bloom, Joel Dyer.
**Estimated FTEs:** 10-50.
**Critiques:** The main external critique, from sources like “[the void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)” and “[Lessons from a Chimp](https://arxiv.org/abs/2507.03409)”, is that this research "overattribut\[es\] human traits" to models. It argues that what's being measured isn't genuine sandbagging but models "playing-along-with-drama behaviour" in response to "artificial and contrived" evals.
**Funded by:** Anthropic (and its funders, e.g., Google, Amazon), UK Government (funding the AI Security Institute).
**Outputs in 2025:**

* [**Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs**](https://arxiv.org/abs/2509.18058)
* [**AI Sandbagging: Language Models can Strategically Underperform on Evaluations**](https://arxiv.org/abs/2406.07358)
* [**Noise Injection Reveals Hidden Capabilities of Sandbagging Language Models**](https://arxiv.org/pdf/2412.01784)
* [**Sandbagging in a Simple Survival Bandit Problem**](https://arxiv.org/pdf/2509.26239)
* [**Automated Researchers Can Subtly Sandbag**](https://alignment.anthropic.com/2025/automated-researchers-sandbag/), *Johannes Gasteiger, Vladimir Mikulik, Ethan Perez et al.*, 2025, Alignment Science Blog
* [**White Box Control at UK AISI \- Update on Sandbagging Investigations**](https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging), *Joseph Bloom, Jordan Taylor, Connor Kissane et al.*, 2025-07-10, AI Alignment Forum
* [**Won't vs. Can't: Sandbagging-like Behavior from Claude Models**](https://alignment.anthropic.com/2025/wont-vs-cant/), 2025, Anthropic Alignment Science Blog
* [**LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring**](https://arxiv.org/abs/2508.00943), *Chloe Li, Mary Phuong, Noah Y. Siegel*, 2025-07-31, arXiv
* [**Misalignment and Strategic Underperformance: An Analysis of Sandbagging and Exploration Hacking**](https://lesswrong.com/posts/TeTegzR8X5CuKgMc3/misalignment-and-strategic-underperformance-an-analysis-of), *Buck Shlegeris, Julian Stastny*, 2025-05-08, LessWrong / AI Alignment Forum

## Self-replication evals \[a:evals\_self\_replication\]

**Who edits (internal):** **Stephen ✅**
**One-sentence summary:** evaluate whether AI agents can autonomously replicate themselves by obtaining their own weights, securing compute resources, and creating copies of themselves.
**Theory of change:** if AI agents gain the ability to self-replicate, they could proliferate uncontrollably, making them impossible to shut down. By measuring this capability with benchmarks like RepliBench, we can identify when models cross this dangerous “red line” and implement controls before losing containment.
**See also:** autonomy evals, situational awareness and self-awareness.
**Orthodox problems:** 5\. Instrumental convergence; 12\. A boxed AGI might exfiltrate itself by steganography.
**Target case:** worst case.
**Broad approach:** behavioral.
**Some names:** Sid Black, Asa Cooper Stickland, Jake Pencharz, Oliver Sourbut, Michael Schmatz, Jay Bailey, Ollie Matthews, Ben Millwood, Alex Remedios, Alan Cooney, Xudong Pan, Jiarun Dai. Yihe Fan.
**Estimated FTEs:** 10-20.
**Critiques:** [AI Sandbagging](https://arxiv.org/abs/2406.07358)
**Funded by:** UK Government (via UK AI Safety Institute).
**Outputs in 2025:**

* [**A Realistic Evaluation of Self-Replication Risk in LLM Agents**](https://arxiv.org/abs/2509.25302)
* [**RepliBench: measuring autonomous replication capabilities in AI systems**](https://aisi.gov.uk/work/replibench-measuring-autonomous-replication-capabilities-in-ai-systems), 2025-04-22, UK AISI Blog
* [**Large language model-powered AI systems achieve self-replication with no human intervention**](https://arxiv.org/abs/2503.17378)

## Various Redteams \[a:various\_redteams\]

**Who edits (internal):** **Stephen**

**One-sentence summary:** attack current models and see what they do / deliberately induce bad things on current frontier models to test out our theories / methods.
**Theory of change:** to ensure models are safe, we must actively try to break them. By developing and applying a diverse suite of attacks (e.g., in novel domains, against agentic systems, or using automated tools), researchers can discover vulnerabilities, specification gaming, and deceptive behaviors before they are exploited, thereby informing the development of more robust defenses.
**See also:** other evals categories.
**Orthodox problems:** 12\. A boxed AGI might exfiltrate itself by steganography, spearphishing, 4\. Goals misgeneralize out of distribution.
**Target case:** average case.
**Broad approach:** behavioral.
**Some names:** Ryan Greenblatt, Benjamin Wright, Aengus Lynch, John Hughes, Samuel R. Bowman, Andy Zou, Nicholas Carlini, Abhay Sheshadri.
**Estimated FTEs:** 100+
**Critiques:** [Claude Sonnet 3.7 (often) knows when it’s in alignment evaluations](https://www.alignmentforum.org/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment), [Red Teaming AI Red Teaming.](https://arxiv.org/html/2507.05538v1)
**Funded by:** Frontier labs (Anthropic, OpenAI, Google), government (UK AISI), Open Philanthropy, LTFF, academic grants.
**Outputs in 2025:**

* [**Building and evaluating alignment auditing agents**](https://alignment.anthropic.com/2025/automated-auditing/), *Trenton Bricken, Rowan Wang, Sam Bowman et al.*, 2025-07-24, Anthropic Alignment Science Blog
* [https://arxiv.org/pdf/2406.18510](https://arxiv.org/pdf/2406.18510)
* [https://arxiv.org/abs/2412.18693](https://arxiv.org/abs/2412.18693)
* [**Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise**](https://t.co/wk0AP8aDNI), *Samuel R. Bowman, Megha Srivastava, Jon Kutasov et al.*, 2025-08-27, Alignment Science Blog
* [**Agentic Misalignment: How LLMs could be insider threats**](https://t.co/XFtd0H2Pzb), *Aengus Lynch, Benjamin Wright, Caleb Larson et al.*, 2025-06-20, Anthropic Research
* [**Compromising Honesty and Harmlessness in Language Models via Deception Attacks**](https://arxiv.org/abs/2502.08301), *Laurène Vaugrante, Francesca Carlon, Maluna Menke et al.*, 2025-02-12, arXiv
* [**Eliciting Language Model Behaviors with Investigator Agents**](https://arxiv.org/abs/2502.01236), *Xiang Lisa Li, Neil Chowdhury, Daniel D. Johnson et al.*, 2025-02-03, arXiv
* [**Why Do Some Language Models Fake Alignment While Others Don't?**](https://arxiv.org/abs/2506.18032), *Abhay Sheshadri, John Hughes, Julian Michael et al.*, 2025-06-22, arXiv
* [**Demonstrating specification gaming in reasoning models**](https://arxiv.org/abs/2502.13295), *Alexander Bondarenko, Denis Volk, Dmitrii Volkov et al.*, 2025-08-27, arXiv
* [**Call Me A Jerk: Persuading AI to Comply with Objectionable Requests**](https://t.co/tkHkVFVZ2m), *Lennart Meincke, Dan Shapiro, Angela Duckworth et al.*, 2025-07-18, SSRN / The Wharton School Research Paper
* [**RedDebate: Safer Responses through Multi-Agent Red Teaming Debates**](https://arxiv.org/abs/2506.11083), *Ali Asad, Stephen Obadinma, Radin Shayanfar et al.*, 2025-06-04, arXiv
* [https://arxiv.org/abs/2512.03771](https://arxiv.org/abs/2512.03771)
* [**The Structural Safety Generalization Problem**](https://arxiv.org/abs/2504.09712), *Julius Broomfield, Tom Gibbs, Ethan Kosak-Hine et al.*, 2025-04-13, arXiv
* [**No, of Course I Can\! Deeper Fine-Tuning Attacks That Bypass Token-Level Safety Mechanisms**](https://arxiv.org/abs/2502.19537), *Joshua Kazdan, Abhay Puri, Rylan Schaeffer et al.*, 2025-02-26, arXiv
* [**Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs**](https://arxiv.org/abs/2502.14828), *Xander Davies, Eric Winsor, Alexandra Souly et al.*, 2025-02-20, arXiv
* [**LLM Robustness Leaderboard v1 \--Technical report**](https://arxiv.org/abs/2508.06296), *Pierre Peigné \- Lefebvre, Quentin Feuillade-Montixi, Tom David et al.*, 2025-08-13, arXiv
* [**Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods and a New Transcript-Classifier Approach**](https://arxiv.org/abs/2412.02159), *Tony T. Wang, John Hughes, Henry Sleight et al.*, 2024-12-03, arXiv
* [**Discovering Undesired Rare Behaviors via Model Diff Amplification**](https://www.goodfire.ai/papers/model-diff-amplification), *Santiago Aranguri, Thomas McGrath*, 2025-08-21, Goodfire Research
* [**REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective**](https://arxiv.org/abs/2502.17254), *Simon Geisler, Tom Wollschläger, M. H. I. Abdalla et al.*, 2025-02-24, arXiv
* [**Petri: An open-source auditing tool to accelerate AI safety research**](https://alignment.anthropic.com/2025/petri/), 2025-10-06, Anthropic Alignment Science Blog
* [**\`For Argument's Sake, Show Me How to Harm Myself\!': Jailbreaking LLMs in Suicide and Self-Harm Contexts**](https://arxiv.org/pdf/2507.02990), *Annika M Schoene, Cansu Canca*, 2025-07-01, arXiv
* [**Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models**](https://arxiv.org/abs/2505.07846), *Lars Malmqvist*, 2025-05-07, arXiv (to be presented at SIMLA@ACNS 2025\)
* [**Trading Inference-Time Compute for Adversarial Robustness**](https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness), *OpenAI*, 2025-01-22, arXiv
* [**Can a Neural Network that only Memorizes the Dataset be Undetectably Backdoored?**](https://openreview.net/forum?id=TD1NfQuVr6), *Matjaz Leonardis*, 2025-07-10, ODYSSEY 2025 Conference
* [**Multi-Agent Step Race Benchmark: Assessing LLM Collaboration and Deception Under Pressure**](https://github.com/lechmazur/step_game), *lechmazur, eltociear*, 2025-08-29, GitHub
* [**Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents**](https://arxiv.org/abs/2503.00061), *Qiusi Zhan, Richard Fang, Henil Shalin Panchal et al.*, 2025-03-04, NAACL 2025 Findings
* [**Quantifying the Unruly: A Scoring System for Jailbreak Tactics**](https://0din.ai/blog/quantifying-the-unruly-a-scoring-system-for-jailbreak-tactics), *Pedram Amini*, 2025-06-12, 0DIN.ai Blog
* [**Transferable Adversarial Attacks on Black-Box Vision-Language Models**](https://arxiv.org/abs/2505.01050), *Kai Hu, Weichen Yu, Li Zhang et al.*, 2025-05-02, arXiv
* [**Advancing Gemini's security safeguards**](https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/), *Google DeepMind Security & Privacy Research Team*, 2025-05-20, Google DeepMind Blog
* [**Shutdown Resistance in Large Language Models**](https://arxiv.org/abs/2509.14260), *Jeremy Schlatter, Benjamin Weinstein-Raun, Jeffrey Ladish*, 2025-09-13, arXiv
* [**Stress Testing Deliberative Alignment for Anti-Scheming Training**](https://arxiv.org/abs/2509.15541), *Bronson Schoen, Evgenia Nitishinskaya, Mikita Balesni et al.*, 2025-09-19, arXiv
* [**Chain-of-Thought Hijacking**](https://arxiv.org/abs/2510.26418), *Jianli Zhao, Tingchen Fu, Rylan Schaeffer et al.*, 2025-10-30, arXiv
* [**X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents**](https://arxiv.org/abs/2504.13203), *Salman Rahman, Liwei Jiang, James Shiffer et al.*, 2025-04-15, arXiv
* [**Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility**](https://arxiv.org/abs/2507.11630), *Brendan Murphy, Dillon Bowen, Shahrad Mohammadzadeh et al.*, 2025-07-15, arXiv
* [**Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors**](https://arxiv.org/abs/2506.10949), *Chen Yueh-Han, Nitish Joshi, Yulin Chen et al.*, 2025-06-14, arXiv
* [**STACK: Adversarial Attacks on LLM Safeguard Pipelines**](https://arxiv.org/abs/2506.24068), *Ian R. McKenzie, Oskar J. Hollinsworth, Tom Tseng et al.*, 2025-06-30, arXiv
* [**Adversarial Manipulation of Reasoning Models using Internal Representations**](https://arxiv.org/abs/2507.03167), *Kureha Yamaguchi, Benjamin Etheridge, Andy Arditi*, 2025-07-03, arXiv
* [**Discovering Forbidden Topics in Language Models**](https://arxiv.org/abs/2505.17441), *Can Rager, Chris Wendler, Rohit Gandikota et al.*, 2025-05-23, arXiv
* [**RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?**](https://arxiv.org/abs/2506.14261), *Rohan Gupta, Erik Jenner*, 2025-06-17, arXiv
* [**Jailbreak Transferability Emerges from Shared Representations**](https://arxiv.org/abs/2506.12913), *Rico Angell, Jannik Brinkmann, He He*, 2025-06-15, arXiv
* [**Mitigating Many-Shot Jailbreaking**](https://arxiv.org/abs/2504.09604), *Christopher M. Ackerman, Nina Panickssery*, 2025-04-13, arXiv
* [**Active Attacks: Red-teaming LLMs via Adaptive Environments**](https://arxiv.org/abs/2509.21947), *Taeyoung Yun, Pierre-Luc St-Charles, Jinkyoo Park et al.*, 2025-09-26, arXiv
* [**It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics**](https://arxiv.org/abs/2506.02873), *Matthew Kowal, Jasper Timm, Jean-Francois Godbout et al.*, 2025-06-03, arXiv
* [**Adversarial Attacks on Robotic Vision Language Action Models**](https://arxiv.org/abs/2506.03350), *Eliot Krzysztof Jones, Alexander Robey, Andy Zou et al.*, 2025-06-03, arXiv
* [**MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models**](https://arxiv.org/abs/2503.14827), *Chejian Xu, Jiawei Zhang, Zhaorun Chen et al.*, 2025-03-19, ICLR 2025 (preprint on arXiv)
* [**Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models**](https://arxiv.org/abs/2510.22014), *Sarah Ball, Niki Hasrati, Alexander Robey et al.*, 2025-10-24, arXiv
* [**Uncovering Gaps in How Humans and LLMs Interpret Subjective Language**](https://arxiv.org/abs/2503.04113), *Erik Jones, Arjun Patrawala, Jacob Steinhardt*, 2025-03-06, ICLR 2025
* [**RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents**](https://arxiv.org/abs/2510.02609), *Chengquan Guo, Chulin Xie, Yu Yang et al.*, 2025-10-02, arXiv
* [**MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents**](https://arxiv.org/abs/2503.10809), *Lukas Aichberger, Alasdair Paren, Guohao Li et al.*, 2025-03-13, arXiv (accepted NeurIPS 2025\)
* [**ToolTweak: An Attack on Tool Selection in LLM-based Agents**](https://arxiv.org/abs/2510.02554), *Jonathan Sneh, Ruomei Yan, Jialin Yu et al.*, 2025-10-02, arXiv
* [**Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives**](https://arxiv.org/abs/2502.11910), *Leo Schwinn, Yan Scholten, Tom Wollschläger et al.*, 2025-02-17, arXiv
* [**Agentic Misalignment: How LLMs Could be Insider Threats**](https://lesswrong.com/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1), *Aengus Lynch, Benjamin Wright, Caleb Larson et al.*, 2025-06-20, LessWrong/AI Alignment Forum
* [**Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable Models of OpenAI, Anthropic, and Google**](https://lesswrong.com/posts/zjqrSKZuRLnjAniyo/illusory-safety-redteaming-deepseek-r1-and-the-strongest), *ChengCheng, Brendan Murphy, Adrià Garriga-alonso et al.*, 2025-02-07, LessWrong / AI Alignment Forum
* [**Will alignment-faking Claude accept a deal to reveal its misalignment?**](https://lesswrong.com/posts/7C4KJot4aN8ieEDoz/will-alignment-faking-claude-accept-a-deal-to-reveal-its), *Ryan Greenblatt, Kyle Fish*, 2025-01-31, LessWrong / AI Alignment Forum
* [**Research directions Open Phil wants to fund in technical AI safety**](https://lesswrong.com/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai), *jake\_mendel, maxnadeau, Peter Favaloro*, 2025-02-08, LessWrong
* [**When does Claude sabotage code? An Agentic Misalignment follow-up**](https://lesswrong.com/posts/9i6fHMn2vTqyzAi9o/when-does-claude-sabotage-code-an-agentic-misalignment), *Nathan Delisle*, 2024-11-09, LessWrong
* [**Petri: An open-source auditing tool to accelerate AI safety research**](https://lesswrong.com/posts/kffbZGa2yYhc6cakc/petri-an-open-source-auditing-tool-to-accelerate-ai-safety), 2025-10-07, LessWrong

## Other evals \[a:evals\_other\]

**Who edits (internal):** **Stephen ✅**
**One-sentence summary:** A collection of miscellaneous evaluations for specific alignment properties, such as honesty, shutdown resistance and sycophancy.
**Theory of change:** By developing novel benchmarks for specific, hard-to-measure properties (like honesty), critiquing the reliability of existing methods (like cultural surveys), and improving the formal rigor of evaluation systems (like LLM-as-Judges), researchers can create a more robust and comprehensive suite of evaluations to catch failures missed by standard capability or safety testing.
**See also:** other more specific sections on evals.
**Orthodox problems:** none; a barometer of risk.
**Target case:** average case.
**Broad approach:** behavioral.
**Some names:** Richard Ren, Mantas Mazeika, Andrés Corrada-Emmanuel, Ariba Khan, Stephen Casper.
**Estimated FTEs:** 20-50.
**Critiqu**es: [The Unreliability of Evaluating Cultural Alignment in LLMs](https://arxiv.org/abs/2503.08688), [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879).

**Funded by:** Lab funders (OpenAI), Open Philanthropy (which funds CAIS, the organization for the MASK benchmark), academic institutions.
N/A (as a discrete amount). This work is part of the "tens of millions" budgets for broader evaluation and red-teaming efforts at labs and independent organizations.
**Outputs in 2025:**

* [**Shutdown Resistance in Large Language Models**](https://arxiv.org/abs/2509.14260)
* [**Gödel's Therapy Room**](https://gtr.dev/)
* [**AI Testing Should Account for Sophisticated Strategic Behaviour**](https://arxiv.org/abs/2508.14927), *Vojtech Kovarik, Eric Olav Chen, Sami Petersen et al.*, 2025-08-19, arXiv
* [**Logical Consistency Between Disagreeing Experts and Its Role in AI Safety**](https://arxiv.org/abs/2510.00821), *Andrés Corrada-Emmanuel*, 2025-10-01, arXiv
* [**Expanding on what we missed with sycophancy**](https://openai.com/index/expanding-on-sycophancy/), *OpenAI*, 2025-05-02, OpenAI Blog
* [**Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers**](https://arxiv.org/abs/2504.18412), *Jared Moore, Declan Grabb, William Agnew et al.*, 2025-04-25, arXiv
* [**Spiral-Bench**](https://eqbench.com/spiral-bench.html), *Sam Paech*, eqbench.com
* [**Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language**](https://arxiv.org/abs/2507.03409), *Christopher Summerfield, Lennart Luettgau, Magda Dubois et al.*, 2025-07-04, arXiv
* [**Syco-bench: A Benchmark for LLM Sycophancy**](https://www.syco-bench.com/)
* [**Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence**](https://www.arxiv.org/abs/2510.01395), *Myra Cheng, Cinoo Lee, Pranav Khadpe et al.*, 2025-10-01, arXiv
* [**OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety**](https://arxiv.org/abs/2507.06134), *Sanidhya Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou et al.*, 2025-07-08, arXiv
* [**Discerning What Matters: A Multi-Dimensional Assessment of Moral Competence in LLMs**](https://arxiv.org/abs/2506.13082), *Daniel Kilov, Caroline Hendy, Secil Yanik Guyot et al.*, 2025-06-16, arXiv
* [**Establishing Best Practices for Building Rigorous Agentic Benchmarks**](https://arxiv.org/abs/2507.02825), *Yuxuan Zhu, Tengjun Jin, Yada Pruksachatkun et al.*, 2025-07-03, arXiv
* [**Do LLMs Comply Differently During Tests? Is This a Hidden Variable in Safety Evaluation? And Can We Steer That?**](https://lesswrong.com/posts/B2o6nrxwKxLPsSYdh/do-llms-comply-differently-during-tests-is-this-a-hidden), *Sahar Abdelnabi, Ahmed Salem*, 2025-06-16, LessWrong
* [**Systematic runaway-optimiser-like LLM failure modes on Biologically and Economically aligned AI safety benchmarks for LLMs with simplified observation format (BioBlue)**](https://lesswrong.com/posts/PejNckwQj3A2MGhMA/systematic-runaway-optimiser-like-llm-failure-modes-on), *Roland Pihlakas, Sruthi Susan Kuriakose, Shruti Datta Gupta*, 2025-03-16, LessWrong
* [**Towards Alignment Auditing as a Numbers-Go-Up Science**](https://lesswrong.com/posts/bGYQgBPEyHidnZCdE/towards-alignment-auditing-as-a-numbers-go-up-science), *Sam Marks*, 2025-08-04, LessWrong


