



# Labs (giant companies)

### OpenAI Safety
 ○ *Structure*: public benefit corp  
 ○ *Teams*: Alignment, Safety Systems (Interpretability, Safety Oversight, Pretraining Safety, Robustness, Safety Research, Trustworthy AI, new Misalignment Research team [coming](https://archive.is/eDB1D)), Preparedness, Model Policy, Safety and Security Committee, Safety Advisory Group. The [Persona Features](https://www.arxiv.org/pdf/2506.19823) paper had a distinct author list. No named successor to Superalignment.  
 ○ *Public alignment agenda*: [None](https://openai.com/safety/how-we-think-about-safety-alignment/). Barak [offers](https://www.lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety) personal [views](https://windowsontheory.org/2025/06/24/machines-of-faithful-obedience/).  
 ○ *Framework*: [Preparedness Framework](https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf)  
↔ *See also:* [Iterative alignment](#sec:iterative_alignment) · [Safeguards (inference-time auxiliaries)](#a:anthropic_safeguards) · [Character training and persona steering](#a:psych_personas)  
☉ *Some names:* Johannes Heidecke, Boaz Barak, Mia Glaese, Jenny Nitishinskaya, Lama Ahmad, Naomi Bashkansky, Miles Wang, Wojciech Zaremba, David Robinson, Zico Kolter, Jerry Tworek, Eric Wallace, Olivia Watkins, Kai Chen, Chris Koch, Andrea Vallone, Leo Gao  
✗ *Critiques:* [Stein-Perlman](https://ailabwatch.org/companies/openai), [Stewart](https://intelligence.org/2025/03/31/a-response-to-openais-how-we-think-about-safety-and-alignment/), [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims), [Midas](https://www.openaifiles.org/transparency-and-safety), [defense](https://www.wired.com/story/openai-anduril-defense/), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).) on labs in general. It's [difficult](https://conversationswithtyler.com/episodes/sam-altman-2/) to model OpenAI as a single agent: *"ALTMAN: I very rarely get to have anybody work on anything. One thing about researchers is they're going to work on what they're going to work on, and that's that."*  
 *$ Funded by:* Microsoft, [AWS](https://www.aboutamazon.com/news/aws/aws-open-ai-workloads-compute-infrastructure), Oracle, NVIDIA, SoftBank, G42, AMD, Dragoneer, Coatue, Thrive, Altimeter, MGX, Blackstone, TPG, T. Rowe Price, Andreessen Horowitz, D1 Capital Partners, Fidelity Investments, Founders Fund, Sequoia… 

▹ *Some outputs:*
Their 60-page System Cards now contain a large amount of their public safety work. · [https://alignment.openai.com/](https://alignment.openai.com/) · [Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation](https://arxiv.org/abs/2503.11926) · [Persona Features Control Emergent Misalignment](https://arxiv.org/abs/2506.19823) · [Stress Testing Deliberative Alignment for Anti-Scheming Training](https://arxiv.org/abs/2509.15541) · [Deliberative Alignment: Reasoning Enables Safer Language Models](https://arxiv.org/abs/2412.16339) · [Toward understanding and preventing misalignment generalization](https://openai.com/index/emergent-misalignment) · [Our updated Preparedness Framework](https://openai.com/index/updating-our-preparedness-framework/) · [Trading Inference-Time Compute for Adversarial Robustness](https://arxiv.org/abs/2501.18841) · [Findings from a pilot Anthropic–OpenAI alignment evaluation exercise: OpenAI Safety Tests](https://openai.com/index/openai-anthropic-safety-evaluation) · [Safety evaluations hub](https://openai.com/safety/evaluations-hub) · [Small-to-Large Generalization: Data Influences Models Consistently Across Scale](https://arxiv.org/abs/2505.16260) · [https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf](https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf)

### Deepmind Responsibility & Safety
 ○ *Structure*: research laboratory subsidiary of a for-profit  
 ○ *Teams*: [ASAT](https://www.alignmentforum.org/posts/wqz5CRzqWkvzoatBG/agi-safety-and-alignment-google-deepmind-is-hiring) ("AGI Alignment", which consists of amplified oversight and interpretability plus "Frontier Safety", which consists of framework development and implementation). Also Gemini Safety, [Voices of All in Alignment](https://www.edinburgh-robotics.org/events/whose-gold-aligning-ai-diverse-views-what%E2%80%99s-safe-aligned-and-beneficial), AGI Safety Council, Responsibility and Safety Council. Sort-of the [Causal Incentives Working Group](https://causalincentives.com/) too.  
 ○ *Public alignment agenda*: [An Approach to Technical AGI Safety](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf%20)  
 ○ *Framework*: [Frontier Safety Framework](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/updating-the-frontier-safety-framework/Frontier%20Safety%20Framework%202.0.pdf)  
↔ *See also:* [Interpretability](#sec:interpretability) · Scalable Oversight  
☉ *Some names:* Rohin Shah, Allan Dafoe, Anca Dragan, Dave Orr, Alex Irpan, Alex Turner, Anna Wang, Arthur Conmy, David Lindner, Jonah Brown-Cohen, Lewis Ho, Neel Nanda, Raluca Ada Popa, Rishub Jain, Rory Greig, Scott Emmons, Sebastian Farquhar, Senthooran Rajamanoharan, Sophie Bridgers, Tobi Ijitoye, Tom Everitt, Victoria Krakovna, Vikrant Varma, Vladimir Mikulik, Zac Kenton, Noah Goodman, Four Flynn, Jonathan Richens, Lewis Smith  
✗ *Critiques:* [Stein-Perlman](https://ailabwatch.org/companies/deepmind), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).), [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims), [On Google's Safety Plan](https://lesswrong.com/posts/hvEikwtsbf6zaXG2s/on-google-s-safety-plan)  
 *$ Funded by:* Google. Explicit Deepmind spending as a whole was [£1.3B](https://s3.eu-west-2.amazonaws.com/document-api-images-live.ch.gov.uk/docs/WT_VNJe9leRjfcU0-OtRjWqF7WiqueStclXgHPbdG4U/application-pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAWRGBDBV3ETBK6MEZ%2F20251107%2Feu-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251107T165323Z&X-Amz-Expires=60&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEPT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCWV1LXdlc3QtMiJHMEUCIBO%2FN0Q9tFWQR%2FLthFYa8oNsFTPSnysg4ONrTUQ%2FyyG1AiEAkPoujFneu%2Bmo73eOnOGNV0tBYKpjLeiP7yATK0BxE7oqigQIvf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgw0NDkyMjkwMzI4MjIiDAUWi2tKimijl1eBRireA4WSlhKuVaQDvGJ2Mhhn6yANCCUO3zN8OlpSoMI2QFoeu6i%2BPxwG7TrcECWjXvY2m5d8vAbJZj%2BTfCgRBvOqlWdcvynzDenryX3a%2F0oh1M1LCvKFgZxSZxV1Pb0l6uDpcBww%2BeRUgnTyClEsdjxo0fACkgiFoJkRbXQr3jnzmghu%2B%2B9kDtWERtaeGA5Dmjfr6i0EibJHo0BwsaleiFdKCuWMnNVpgEO4u%2F85kMy%2ByX7vM1Kfj53OvRKF9cyULlQnwb0q7Rs%2FjRespSsIF%2BAiQbUDck3iT%2BkG0wdrnnLEdm1nIHpoBQO65a8GwLvYsNIS7LckCyuG275y1h1%2B%2BUozd4PyyJjLhqmWn71LcsHuPbjSlAfU9E%2BkE73%2FNuuaE4hRICKlqLl4V9IXPiysd1IfazKQwS3lXmXrFiXvASKvLH8VDuBBz%2Bwj0SG50Rhfwpmrl2lmJ4RFVd6Se9S20eZQjnV183HKkCsvyx09YizNCuJqxEeJmQ%2BAJZVyFjDWKLrpC8F9Yzo3Sf4hYtvQRXqrANoB3n4J%2BUUvrUEDSQqOeQOki1xpE37FBu8Ozs%2FKo%2FrYgESZDsXra0gLcJ7zJb3rJs75kxEyqYDiHuBQ8IKbobY%2BC%2FMHC5MOHqfW%2Bu%2Bl%2FUownLq3yAY6pQG1n3iz%2B09NSvDWENlBA5hC3lLvGiD5f1DTjYA9v1GKjA3feHOZ3Iegs1Egz9G%2FTST6riAYyauyj0At6oEUiUnTQn%2BJxtM%2BH%2FuVzzq45lboS5WE7pDOKA6jDVgzgjy6Mlm%2FY3dy28KxiOrV4EsefEtYTd8Nr6XIvJ0PGVEHhMsWo%2Bsn3IzkuKEqQsFsIiQHCb9oREBNnmY%2FDa2iO0fJijogUtrkxMs%3D&X-Amz-SignedHeaders=host&response-content-disposition=inline%3Bfilename%3D%22companies_house_document.pdf%22&X-Amz-Signature=da8a32ce365dd9ee8f6824636e1e893e1a87a58b64c65013631c1e2014bf7e3e), but this doesn't count e.g. Gemini compute. 

▹ *Some outputs:*
[A Pragmatic Vision for Interpretability](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability%20) · [How Can Interpretability Researchers Help AGI Go Well?](https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well%20) · [Evaluating Frontier Models for Stealth and Situational Awareness](https://arxiv.org/abs/2505.01420) · [When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors](https://arxiv.org/abs/2507.05246) · [MONA: Managed Myopia with Approval Feedback](https://alignmentforum.org/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2) · [Consistency Training Helps Stop Sycophancy and Jailbreaks](https://arxiv.org/abs/2510.27062) · [An Approach to Technical AGI Safety and Security](https://arxiv.org/abs/2504.01849) · [InfAlign: Inference-aware language model alignment](https://arxiv.org/abs/2412.19792) · [Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research (GDM Mech Interp Team Progress Update #2)](https://alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks) · [Taking a responsible path to AGI](https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/) · [Evaluating potential cybersecurity threats of advanced AI](https://deepmind.google/discover/blog/evaluating-potential-cybersecurity-threats-of-advanced-ai) · Some work from non-safety teams, like InfAlign · [Incentives for Responsiveness, Instrumental Control and Impact](https://arxiv.org/pdf/2001.07118) · [Difficulties with Evaluating a Deception Detector for AIs](https://arxiv.org/html/2511.22662v1)

### Anthropic Safety
 ○ *Structure*: public-benefit corp  
 ○ *Teams*: Scalable Alignment (Leike), Alignment Evals (Bowman), [Interpretability](https://transformer-circuits.pub/) (Olah), Control (Perez), Model Psychiatry (Lindsey), Character (Askell), Alignment Stress-Testing (Hubinger), Alignment Mitigations (Price?), Frontier Red Team (Graham), Safeguards (?), Societal Impacts (Ganguli), Trust and Safety (Sanderford), Model Welfare (Fish).  
 ○ *Public alignment agenda*: [directions](https://alignment.anthropic.com/2025/recommended-directions/), [bumpers](https://alignment.anthropic.com/2025/bumpers/), [checklist](https://sleepinyourhat.github.io/checklist/), an [old vague view](https://www.anthropic.com/news/core-views-on-ai-safety)  
 ○ *Framework*: [RSP](https://www-cdn.anthropic.com/872c653b2d0501d6ab44cf87f43e1dc4853e4d37.pdf)  
↔ *See also:* [Interpretability](#sec:interpretability) · Scalable Oversight  
☉ *Some names:* Chris Olah, Evan Hubinger, Sam Marks, Johannes Treutlein, Sam Bowman, Euan Ong, Fabien Roger, Adam Jermyn, Holden Karnofsky, Jan Leike, Ethan Perez, Jack Lindsey, Amanda Askell, Kyle Fish, Sara Price, Jon Kutasov, Minae Kwon, Monty Evans, Richard Dargan  
✗ *Critiques:* [Stein](https://ailabwatch.org/anthropic-opinions)-[Perlman](https://ailabwatch.org/companies/anthropic), [Casper](https://www.lesswrong.com/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may#A_review___thoughts), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%2Dpeople%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).), [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims), [Greenblatt](https://nitter.net/RyanPGreenblatt/status/1925992236648464774), [Samin](https://www.lesswrong.com/posts/5aKRshJzhojqfbRyo/unless-its-governance-changes-anthropic-is-untrustworthy), [defense](https://techcrunch.com/2024/11/07/anthropic-teams-up-with-palantir-and-aws-to-sell-its-ai-to-defense-customers/), [Existing Safety Frameworks Imply Unreasonable Confidence](https://lesswrong.com/posts/7ExkgcDudwhag73vw/existing-safety-frameworks-imply-unreasonable-confidence)  
 *$ Funded by:* Amazon, Google, ICONIQ, Fidelity, Lightspeed, Altimeter, Baillie Gifford, BlackRock, Blackstone, Coatue, D1 Capital Partners, General Atlantic, General Catalyst, GIC, Goldman Sachs, Insight Partners, Jane Street, Ontario Teachers' Pension Plan, Qatar Investment Authority, TPG, T. Rowe Price, WCM, XN 

▹ *Some outputs:*
[Natural emergent misalignment from reward hacking](https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf) · [Agentic Misalignment: How LLMs could be insider threats](https://anthropic.com/research/agentic-misalignment) · [SHADE-Arena: Evaluating sabotage and monitoring in LLM agents](https://anthropic.com/research/shade-arena-sabotage-monitoring) · [Forecasting Rare Language Model Behaviors](https://arxiv.org/abs/2502.16797) · [Why Do Some Language Models Fake Alignment While Others Don't?](https://alignmentforum.org/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don) · [Petri: An open-source auditing tool to accelerate AI safety research](https://alignment.anthropic.com/2025/petri) · [Signs of introspection in large language models](https://anthropic.com/research/introspection) · [Recommendations for Technical AI Safety Research Directions](https://alignment.anthropic.com/2025/recommended-directions/index.html) · [Putting up Bumpers](https://alignment.anthropic.com/2025/bumpers/) · [Three Sketches of ASL-4 Safety Case Components](https://alignment.anthropic.com/2024/safety-cases/index.html) · [Open-sourcing circuit tracing tools](https://anthropic.com/research/open-source-circuit-tracing) · [Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples](https://arxiv.org/abs/2510.07192) · [Emergent Introspective Awareness in Large Language Models](https://transformer-circuits.pub/2025/introspection/index.html) · [Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise](https://alignment.anthropic.com/2025/openai-findings) · [Reasoning models don't always say what they think](https://www.anthropic.com/research/reasoning-models-dont-say-think) · [On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) · [Circuit Tracing: Revealing Computational Graphs in Language Models](https://transformer-circuits.pub/2025/attribution-graphs/methods.html) · [Auditing language models for hidden objectives](https://www.anthropic.com/research/auditing-hidden-objectives) · [Constitutional Classifiers: Defending against universal jailbreaks](https://www.anthropic.com/research/constitutional-classifiers) · [The Soul Document](https://gist.github.com/Richard-Weiss/efe157692991535403bd7e7fb20b6695) · [Evaluating honesty and lie detection techniques on a diverse suite of dishonest models](https://alignment.anthropic.com/2025/honesty-elicitation/)

### xAI
 ○ *Structure*: [for-profit](https://www.cnbc.com/amp/2025/08/25/elon-musk-xai-dropped-public-benefit-corp-status-while-fighting-openai.html)  
 ○ *Teams*: [Applied Safety](https://job-boards.greenhouse.io/xai/jobs/4944324007) and Model Evaluation. Nominally focussed on misuse.  
 ○ *Framework*: [Risk Management Framework](https://data.x.ai/2025-08-20-xai-risk-management-framework.pdf)  
☉ *Some names:* Dan Hendrycks (advisor), Juntang Zhuang, Toby Pohlen, Lianmin Zheng, Piaoyang Cui, Nikita Popov, Ying Sheng, Sehoon Kim  
✗ *Critiques:* [framework](https://www.lesswrong.com/posts/hQyrTDuTXpqkxrnoH/xai-s-new-safety-framework-is-dreadful), [hacking](https://x.com/g_leech_/status/1990543987846078854), [broken promises](https://x.com/g_leech_/status/1990734517145911593), [Stein](https://ailabwatch.org/companies/xai)-[Perlman](https://ailabwatch.org/resources/integrity#xai), [insecurity](https://nitter.net/elonmusk/status/1961904269545648624), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).) on labs in general  
 *$ Funded by:* A16Z, Blackrock, Fidelity, Kingdom, Lightspeed, MGX, Morgan Stanley, Sequoia… 



### Meta
 ○ *Structure*: for-profit  
 ○ *Teams*: Safety "integrated into" capabilities research, Meta Superintelligence Lab. But also FAIR Alignment, [Brain and AI](https://www.metacareers.com/jobs/1319148726628205).  
 ○ *Public alignment agenda*: None  
 ○ *Framework*: [FAF](https://ai.meta.com/static-resource/meta-frontier-ai-framework/?utm_source=newsroom&utm_medium=web&utm_content=Frontier_AI_Framework_PDF&utm_campaign=Our_Approach_to_Frontier_AI_blog)  
↔ *See also:* [Capability removal, unlearning](#a:unlearning)  
☉ *Some names:* Shuchao Bi, Hongyuan Zhan, Jingyu Zhang, Haozhu Wang, Eric Michael Smith, Sid Wang, Amr Sharaf, Mahesh Pasupuleti, Jason Weston, ShengYun Peng, Ivan Evtimov, Song Jiang, Pin-Yu Chen, Evangelia Spiliopoulou, Lei Yu, Virginie Do, Karen Hambardzumyan, Nicola Cancedda, Adina Williams  
✗ *Critiques:* [extreme underelicitation](https://googleprojectzero.blogspot.com/2024/06/project-naptime.html#:~:text=We%20find%20that%2C%20by%20refining%20the%20testing%20methodology%20to%20take%20advantage%20of%20modern%20LLM%20capabilities%2C%20significantly%20better%20performance%20in%20vulnerability%20discovery%20can%20be%20achieved.%20To%20facilitate%20effective%20evaluation%20of%20LLMs%20for%20vulnerability%20discovery%2C%20we%20propose%20below%20a%20set%20of%20guiding%20principles.), [Stein](https://ailabwatch.org/companies/meta)-[Perlman](https://ailabwatch.org/companies/meta), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).) on labs in general  
 *$ Funded by:* Meta 

▹ *Some outputs:*
[The Alignment Waltz: Jointly Training Agents to Collaborate for Safety](https://arxiv.org/pdf/2510.08240) · [AI & Human Co-Improvement](https://github.com/facebookresearch/RAM/blob/main/projects/co-improvement.pdf) · [Large Reasoning Models Learn Better Alignment from Flawed Thinking](https://arxiv.org/pdf/2510.00938%20) · [Robust LLM safeguarding via refusal feature adversarial training](https://arxiv.org/pdf/2409.20089) · [Code World Model Preparedness Report](https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/557601942_1468972530985309_838842257265552803_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_H33_VKF3ZUQ7kNvwFog8dd&_nc_oc=AdlNtqCDY4HafZ3-d5rb26AF5f2m0X46SGdKhVq3jLqwpNf_wEXhdQnH7_30ychiZWk&_nc_zt=14&_nc_ht=scontent-lhr8-1.xx&_nc_gid=QvW_ePiaF4E-PxOf30MWyg&oh=00_AfiZC5G4ODvWhiy0MuVH8PSlUFrW8RDQQ8tdr6Zec5k9aA&oe=691A6D09) · [Agents Rule of Two: A Practical Approach to AI Agent Security](https://ai.meta.com/blog/practical-ai-agent-security/%20)

## Others

* Amazon’s [Nova Pro](https://arxiv.org/pdf/2506.12103v1) is around the level of Llama 3 90B, which in turn is around the level of the original GPT-4. So 2 years behind. But they have their own [chip](https://www.businessinsider.com/startups-amazon-ai-chips-less-competitive-nvidia-gpus-trainium-aws-2025-11).
* Microsoft are [now](https://www.dwarkesh.com/p/satya-nadella-2) mid-training on top of GPT-5. MAI-1-preview is [around](https://lmarena.ai/leaderboard/text) DeepSeek V3.0 on Arena. They [continue](https://arxiv.org/abs/2506.22405v1) to focus on medical diagnosis. You can [request](https://forms.microsoft.com/pages/responsepage.aspx?id=v4j5cvGGr0GRqy180BHbRyRliS0ly-JEvgSpwo3yWyhUQkdTQktBUkFaWERHR1JFRjgwMlZUUkQxTC4u&route=shorturl) access.
* Mistral have a reasoning model, [Magistral Medium](https://arxiv.org/pdf/2506.10910), and released the weights of a little 24B version. It’s a bit worse than Deepseek R1, pass@1.

The Chinese companies [don’t](https://futureoflife.org/wp-content/uploads/2025/07/FLI-AI-Safety-Index-Report-Summer-2025.pdf#page=3) [attempt](https://ailabwatch.org/companies/deepseek) to be safe, often not even in the prosaic safeguards sense - [This](https://www.holisticai.com/blog/red-teaming-open-source-ai-models-china) is one of the few safeguard tests I could find including the Chinese models; M2 does well. They drop the weights [immediately](https://x.com/natolambert/status/1991915728992190909) after post-training finishes. They’re mostly open weights and closed data. The companies are often [severely](https://www.wsj.com/tech/ai/china-us-ai-chip-restrictions-effect-275a311e) compute-constrained. See [here](https://www.gleech.org/paper) for doubts about their capabilities.

* Alibaba’s Qwen3-etc-etc is [nominally](https://artificialanalysis.ai/leaderboards/models) at the level of Gemini 2.5 Flash. Maybe the only Chinese model with a [large](https://www.atomproject.ai/#:~:text=Model%20Adoption%20Trends) Western userbase, including businesses, but since it’s self-hosted this doesn’t translate into profits for them yet. On [one ad hoc test](https://www.gleech.org/paper) it was the only Chinese model not to collapse OOD, but the Qwen2.5 corpus was severely contaminated.
* DeepSeek’s v3.2 is [nominally](https://artificialanalysis.ai/leaderboards/models) around the same as Qwen. The CCP made them [waste](https://arstechnica.com/ai/2025/08/deepseek-delays-next-ai-model-due-to-poor-performance-of-chinese-made-chips/) months trying Huawei chips.
* Moonshot’s Kimi-K2-Thinking has some nominally [frontier](https://artificialanalysis.ai/) benchmark results and a pleasant style but does not [seem](https://x.com/METR_Evals/status/1991658241932292537) frontier.
* Baidu’s [ERNIE 5](https://x.com/Baidu_Inc/status/1988820837898829918) is again nominally very strong, a bit better than DeepSeek. This new one seems to not be open.
* Z’s [GLM-4.6](https://z.ai/blog/glm-4.6) is around the same as Qwen.
* MiniMax’s M2 is nominally better than Qwen, [around the same](https://artificialanalysis.ai/leaderboards/models) as Grok 4 Fast on the usual superficial benchmarks. It does [fine](https://www.holisticai.com/blog/red-teaming-open-source-ai-models-china) on one very basic red-team test.
* ByteDance does impressive research in a lagging paradigm, [diffusion LMs](https://seed.bytedance.com/en/direction/llm).
* There are [others](https://www.interconnects.ai/i/171165224/honorable-mentions) but they’re marginal for now.

# Black-box safety (understand and control current model behaviour)

## Iterative alignment

Nudging base models by optimising their output. Worked on by the post-training teams at most labs, estimating the FTEs at \>500 in some sense. Funded by most of the industry.

**General critiques:** [Bellot](https://arxiv.org/abs/2506.02923), [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions), [STACK](https://arxiv.org/abs/2506.24068)*,* [AI Alignment Strategies from a Risk Perspective](https://arxiv.org/abs/2510.11235), [AI Alignment based on Intentions does not work](https://t.co/OTnrYRVsPS)*,* [Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?](https://arxiv.org/abs/2505.23749)*,* [Murphy’s Laws of AI Alignment: Why the Gap Always Wins](https://arxiv.org/abs/2509.05381), [Alignment remains a hard, unsolved problem](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)

### Iterative alignment at pretrain-time
*Guide weights during pretraining.*

*≈ Theory of change:* "LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent."

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
↔ *See also:* [prosaic alignment](https://www.lesswrong.com/posts/5ciYedyQDDqAcrDLr/a-positive-case-for-how-we-might-succeed-at-prosaic-ai) · [incrementalism](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy) · [alignment-by-default](https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default)  
☉ *Some names:* Jan Leike, Stuart Armstrong, Cyrus Cousins, Vincent Conitzer, Oliver Daniels  
✗ *Critiques:* [Bellot](https://arxiv.org/abs/2506.02923), [STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235), [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)  
 *$ Funded by:* most of the industry 

▹ *Some outputs:*
[Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment](https://arxiv.org/abs/2509.04445) · [ACE and Diverse Generalization via Selective Disagreement](https://arxiv.org/abs/2509.07955) · [Unsupervised Elicitation](https://alignment.anthropic.com/2025/unsupervised-elicitation)

### Iterative alignment at post-train-time
*Modify weights after pre-training.*

*≈ Theory of change:* "LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent."

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
↔ *See also:* [incrementalism](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy) · [Character training and persona steering](#a:psych_personas)  
☉ *Some names:* Adam Gleave, Anca Dragan, Jacob Steinhardt, Rohin Shah  
✗ *Critiques:* [Bellot](https://arxiv.org/abs/2506.02923), [STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235), [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)  
 *$ Funded by:* most of the industry 

▹ *Some outputs:*
[RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation](https://arxiv.org/abs/2501.08617) · [Uncertainty-Aware Step-wise Verification with Generative Reward Models](https://arxiv.org/abs/2502.11250) · [Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives](https://arxiv.org/abs/2511.06626) · [Preference Learning with Lie Detectors can Induce Honesty or Evasion](https://arxiv.org/abs/2505.13787) · [Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference](https://arxiv.org/abs/2510.21184) · [On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback](https://arxiv.org/abs/2411.02306) · [Preference Learning for AI Alignment: a Causal Perspective](https://arxiv.org/abs/2506.05967) · [Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision](https://arxiv.org/abs/2501.07886) · [On Monotonicity in AI Alignment](https://arxiv.org/abs/2506.08998) · [Consistency Training Helps Stop Sycophancy and Jailbreaks](https://arxiv.org/abs/2510.27062) · [Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability](https://arxiv.org/abs/2510.06084) · [Rethinking Safety in LLM Fine-tuning: An Optimization Perspective](https://arxiv.org/abs/2508.12531) · [Composable Interventions for Language Models](https://arxiv.org/abs/2407.06483%20) · [The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains](https://arxiv.org/abs/2507.06187) · [Robust LLM Alignment via Distributionally Robust Direct Preference Optimization](https://arxiv.org/abs/2502.01930)

### Black-box make-AI-solve-it
*Focus on using existing models to improve and align further models.*

*≈ Theory of change:* "LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent."

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
↔ *See also:* [Make AI solve it](#sec:ai_solve_alignment) · [Debate](#a:debate)  
☉ *Some names:* Jacques Thibodeau, Matthew Shingle, Nora Belrose, Lewis Hammond, Geoffrey Irving  
✗ *Critiques:* [STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235), [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL), [SAIF](https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/)  
 *$ Funded by:* most of the industry 

▹ *Some outputs:*
[Training AI to do alignment research we don't already know how to do](https://lesswrong.com/posts/5gmALpCetyjkSPEDr/training-ai-to-do-alignment-research-we-don-t-already-know) · [Automating AI Safety: What we can do today](https://lesswrong.com/posts/FqpAPC48CzAtvfx5C/automating-ai-safety-what-we-can-do-today) · [Mechanistic Anomaly Detection for "Quirky" Language Models](https://arxiv.org/abs/2504.08812) · [Weak to Strong Generalization for Large Language Models with Multi-capabilities](https://openreview.net/forum?id=N1vYivuSKq) · [Debate Helps Weak-to-Strong Generalization](https://arxiv.org/abs/2501.13124) · [Neural Interactive Proofs](https://neural-interactive-proofs.com/) · [MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking](https://arxiv.org/abs/2501.13011) · [Prover-Estimator Debate: A New Scalable Oversight Protocol](https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol) · [Ensemble Debates with Local Large Language Models for AI Alignment](https://arxiv.org/abs/2509.00091) · [AI Debate Aids Assessment of Controversial Claims](https://arxiv.org/abs/2506.02175) · [An alignment safety case sketch based on debate](https://arxiv.org/abs/2505.03989) · [Superalignment with Dynamic Human Values](https://arxiv.org/abs/2503.13621)

### Inoculation prompting
*Prompt mild misbehaviour in training, to prevent the failure mode where once AI misbehaves in a mild way, it will be more inclined towards all bad behaviour.*

*≈ Theory of change:* "LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent."

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
☉ *Some names:* Alex Turner, Victor Gillioz, Ariana Azarbal, Monte MacDiarmid, Daniel Ziegler  
✗ *Critiques:* [Bellot](https://arxiv.org/abs/2506.02923), [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions), [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)  
 *$ Funded by:* most of the industry 

▹ *Some outputs:*
[Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time](https://arxiv.org/abs/2510.04340) · [Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment](https://arxiv.org/abs/2510.05024) · [Recontextualization Mitigates Specification Gaming Without Modifying the Specification](https://www.alignmentforum.org/posts/whkMnqFWKsBm7Gyd7/recontextualization-mitigates-specification-gaming-without) · [Natural Emergent Misalignment from Reward Hacking](https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf)

### Inference-time: In-context learning
*Investigate what runtime guidelines, rules, or examples provided to an LLM yield better behavior.*

*≈ Theory of change:* "LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent."

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
↔ *See also:* model spec as prompt · [Model specs and constitutions](#a:specs_and_constitutions)  
☉ *Some names:* Jacob Steinhardt, Kayo Yin, Atticus Geiger  
✗ *Critiques:* [STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235), [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)  
▹ *Some outputs:*
[InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569) · [Inference-Time Reward Hacking in Large Language Models](https://arxiv.org/abs/2506.19248) · [Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context](https://arxiv.org/abs/2510.06182) · [Understanding In-context Learning of Addition via Activation Subspaces](https://arxiv.org/abs/2505.05145) · [Which Attention Heads Matter for In-Context Learning?](https://arxiv.org/abs/2502.14010)

### Inference-time: Steering
*Manipulate an LLM's internal representations/token probabilities without touching weights.*

*≈ Theory of change:* "LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent."

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
↔ *See also:* [Character training and persona steering](#a:psych_personas) · [Safeguards (inference-time auxiliaries)](#a:anthropic_safeguards)  
☉ *Some names:* Taylor Sorensen, Constanza Fierro, Kshitish Ghate, Arthur Vogels  
✗ *Critiques:* [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions), [STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235), [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)  
▹ *Some outputs:*
[In-Distribution Steering: Balancing Control and Coherence in Language Model Generation.](https://arxiv.org/abs/2510.13285) · [EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preferences](https://arxiv.org/abs/2510.06370) · [Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation](https://arxiv.org/abs/2502.00580) · [Steering Language Models with Weight Arithmetic](https://arxiv.org/abs/2511.05408)

### Capability removal, unlearning
*Developing methods to selectively remove specific information, capabilities, or behaviors from a trained model without retraining it from scratch. This is a mixture of black-box and white-box approaches.*

*≈ Theory of change:* If an AI learns dangerous knowledge (e.g., dual-use capabilities) or exhibits undesirable behaviors (e.g., memorizing private data), we can specifically erase this "bad" knowledge post-training, which is much cheaper and faster than retraining, thereby making the model safer.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify, Goals misgeneralize out of distribution, Humanlike minds/goals are not necessarily safe  
↔ *See also:* [Interpretability](#sec:interpretability) · [Various Redteams](#a:various_redteams)  
☉ *Some names:* Rowan Wang, Avery Griffin, Johannes Treutlein, Zico Kolter, Bruce W. Lee, Addie Foote, Alex Infanger, Zesheng Shi, Yucheng Zhou, Jing Li, Timothy Qian  
✗ *Critiques:* [Existing Large Language Model Unlearning Evaluations Are Inconclusive](https://arxiv.org/abs/2506.00688)  
 *$ Funded by:* Coefficient Giving, MacArthur Foundation, UK AI Safety Institute (AISI), Canadian AI Safety Institute (CAISI), industry labs (e.g., Microsoft Research, Google) · *⫼ Estimated FTEs:* 10-50 

▹ *Some outputs:*
*Mostly black-box*
 · [Modifying LLM Beliefs with Synthetic Document Finetuning](https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf) · [Distillation Robustifies Unlearning](https://arxiv.org/abs/2506.06278) · [From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization](https://arxiv.org/abs/2505.22310) · [OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics](https://arxiv.org/abs/2506.12618) · [Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy and Research](https://arxiv.org/abs/2412.06966) · [Open Problems in Machine Unlearning for AI Safety](https://arxiv.org/abs/2501.04952) · [Mirror Mirror on the Wall, Have I Forgotten it All?](https://arxiv.org/abs/2505.08138) · *Mostly white-box*
 · [Safety Alignment via Constrained Knowledge Unlearning](https://arxiv.org/abs/2505.18588) · [Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs](https://arxiv.org/abs/2505.16831) · [Understanding Memorization via Loss Curvature](https://goodfire.ai/research/understanding-memorization-via-loss-curvature) · [Unlearning Needs to be More Selective [Progress Report]](https://lesswrong.com/posts/QYzofMbzmbgiwfqy8/unlearning-needs-to-be-more-selective-progress-report) · [Layered Unlearning for Adversarial Relearning](https://arxiv.org/abs/2505.09500) · [Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization](https://arxiv.org/abs/2506.12484) · [Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning](https://arxiv.org/abs/2509.11816)

### Control
*Assuming early transformative AIs are misaligned and actively trying to subvert safety measures, can we still set up protocols to extract useful work from them?*

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering / behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
↔ *See also:* safety cases  
☉ *Some names:* Redwood, UK AISI, Buck Shlegeris, Ryan Greenblatt, Kshitij Sachan, Alex Mallen  
✗ *Critiques:* [Wentworth](https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research), [Mannheim](https://lesswrong.com/posts/25dsPH6CuRXPBkGHN/no-we-re-not-getting-meaningful-oversight-of-ai), [Kulveit](https://www.lesswrong.com/posts/rZcyemEpBHgb2hqLP/ai-control-may-increase-existential-risk)  
*⫼ Estimated FTEs:* 5-50 

▹ *Some outputs:*
[Ctrl-Z: Controlling AI Agents via Resampling](https://alignmentforum.org/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling) · [ControlArena](https://control-arena.aisi.org.uk/) · [https://openreview.net/forum?id=QWopGahUEL](https://openreview.net/forum?id=QWopGahUEL) · [SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents](https://arxiv.org/abs/2506.15740) · [Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats](https://arxiv.org/abs/2411.17693) · [D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models](https://arxiv.org/abs/2509.17938) · [https://luthienresearch.org/updates/2025-03-redteam-as-upsampling/](https://luthienresearch.org/updates/2025-03-redteam-as-upsampling/) · [Incentives for Responsiveness, Instrumental Control and Impact](https://arxiv.org/abs/2001.07118) · [Subversion Strategy Eval: Can language models statelessly strategize to subvert control protocols?](https://arxiv.org/abs/2412.12480) · [Evaluating Control Protocols for Untrusted AI Agents](https://arxiv.org/abs/2511.02997) · [Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.19851) · [Optimizing AI Agent Attacks With Synthetic Data](https://arxiv.org/abs/2511.02823) · [A sketch of an AI control safety case](https://arxiv.org/abs/2501.17315) · [Assessing confidence in frontier AI safety cases](https://arxiv.org/abs/2502.05791) · [How to evaluate control measures for LLM agents? A trajectory from today to superintelligence](https://arxiv.org/abs/2504.05259) · [Towards evaluations-based safety cases for AI scheming](https://arxiv.org/abs/2411.03336) · [Putting up Bumpers](https://alignment.anthropic.com/2025/bumpers) · [Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework](https://arxiv.org/abs/2507.12872) · [Dynamic safety cases for frontier AI](https://arxiv.org/abs/2412.17618) · [The Alignment Project by UK AISI](https://lesswrong.com/posts/wKTwdgZDo479EhmJL/the-alignment-project-by-uk-aisi-1) · [AI companies are unlikely to make high-assurance safety cases if timelines are short](https://lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety) · [AIs at the current capability level may be important for future safety work](https://lesswrong.com/posts/cJQZAueoPC6aTncKK/ais-at-the-current-capability-level-may-be-important-for) · [Takeaways from sketching a control safety case](https://lesswrong.com/posts/y6rBarAPTLmuhn9PJ/takeaways-from-sketching-a-control-safety-case)

### Safeguards (inference-time auxiliaries)
*Layers of inference-time defenses, such as classifiers, monitors, and rapid-response protocols, to detect and block jailbreaks, prompt injections, and other harmful model behaviors.*

*≈ Theory of change:* By building a bunch of scalable and hardened things on top of an unsafe model, we can defend against known and unknown attacks, monitor for misuse, and prevent models from causing harm, even if the core model has vulnerabilities.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Superintelligence can fool human supervisors, A boxed AGI might exfiltrate itself by steganography, spearphishing  
↔ *See also:* [Various Redteams](#a:various_redteams) · [Iterative alignment](#sec:iterative_alignment)  
☉ *Some names:* Mrinank Sharma, Meg Tong, Jesse Mu, Alwin Peng, Julian Michael, Henry Sleight, Theodore Sumers, Raj Agarwal, Nathan Bailey, Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Sahil Verma, Keegan Hines, Jeff Bilmes  
✗ *Critiques:* [Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/abs/2412.09565).  
 *$ Funded by:* most of the big labs · *⫼ Estimated FTEs:* 100+ 

▹ *Some outputs:*
[Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming](https://arxiv.org/abs/2501.18837) · [Rapid Response: Mitigating LLM Jailbreaks with a Few Examples](https://arxiv.org/abs/2411.07494) · [Monitoring computer use via hierarchical summarization](https://alignment.anthropic.com/2025/summarization-for-monitoring/index.html) · [Defeating Prompt Injections by Design](https://arxiv.org/abs/2503.18813) · [Introducing Anthropic's Safeguards Research Team](https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html) · [OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities](https://arxiv.org/abs/2505.23856)

### Chain of thought monitoring
*Supervise an AI's natural-language (output) "reasoning" to detect misalignment, scheming, or deception, rather than studying the actual internal states.*

*≈ Theory of change:* The reasoning process (Chain of Thought, or CoT) of an AI provides a legible signal of its internal state and intentions. By monitoring this CoT, supervisors (human or AI) can detect misalignment, scheming, or reward hacking before it results in a harmful final output. This allows for more robust oversight than supervising outputs alone, but it relies on the CoT remaining faithful (i.e., accurately reflecting the model's reasoning) and not becoming obfuscated under optimization pressure.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Superintelligence can fool human supervisors, Superintelligence can hack software supervisors, A boxed AGI might exfiltrate itself by steganography, spearphishing  
↔ *See also:* [White-box safety (understand and control current model internals)](#sec:whitebox) · [Steganography evals](#a:evals_steganography)  
☉ *Some names:* Aether, Bowen Baker, Joost Huizinga, Leo Gao, Scott Emmons, Erik Jenner, Yanda Chen, James Chua, Owain Evans, Tomek Korbak, Mikita Balesni, Xinpeng Wang, Miles Turpin  
✗ *Critiques:* [Reasoning Models Don't Always Say What They Think](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf); [Chain-of-Thought Reasoning In The Wild Is Not Always Faithful](https://arxiv.org/abs/2503.08679); [Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens](https://arxiv.org/abs/2505.13775); [Reasoning Models Sometimes Output Illegible Chains of Thought](https://arxiv.org/abs/2510.27338)  
 *$ Funded by:* OpenAI, Anthropic, Google DeepMind · *⫼ Estimated FTEs:* 10-100 

▹ *Some outputs:*
[Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473) · [Detecting misbehavior in frontier reasoning models](https://openai.com/index/chain-of-thought-monitoring/) · [Training fails to elicit subtle reasoning in current language models](https://alignment.anthropic.com/2025/subtle-reasoning/) · [Are DeepSeek R1 And Other Reasoning Models More Faithful?](https://arxiv.org/abs/2501.08156) · [Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.19851) · [Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation](https://arxiv.org/abs/2503.11926) · [When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors](https://arxiv.org/abs/2507.05246) · [Reasoning Models Don't Always Say What They Think](https://arxiv.org/abs/2505.05410) · [Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort](https://arxiv.org/abs/2510.01367) · [Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.22777) · [A Pragmatic Way to Measure Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.23966) · [https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update](https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update) · [A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring](https://lesswrong.com/posts/Em9sihEZmbofZKc2t/a-concrete-roadmap-towards-safety-cases-based-on-chain-of) · [CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring](https://arxiv.org/abs/2505.23575) · [Why Don't We Just... Shoggoth+Face+Paraphraser?](https://www.lesswrong.com/posts/Tzdwetw55JNqFTkzK/why-don-t-we-just-shoggoth-face-paraphraser) · [Why it's good for AI reasoning to be legible and faithful](https://metr.org/blog/2025-03-11-good-for-ai-to-reason-legibly-and-faithfully/)

## Model psychology

### Emergent misalignment
*Fine-tuning LLMs on one narrow antisocial task can cause *general* misalignment including deception, shutdown resistance, harmful advice, and extremist sympathies, when those behaviors are never trained or rewarded directly. [A new agenda](https://www.lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment) which quickly led to a stream of exciting work.*

*≈ Theory of change:* Predict, detect, and prevent models from developing broadly harmful behaviors (like deception or shutdown resistance) when fine-tuned on seemingly unrelated tasks. Find, preserve, and robustify this correlated representation of the good.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution, Superintelligence can fool human supervisors  
↔ *See also:* auditing real models · applied interpretability  
☉ *Some names:* Truthful AI, Jan Betley, James Chua, Mia Taylor, Miles Wang, Edward Turner, Anna Soligo, Alex Cloud, Nathan Hu, Owain Evans  
✗ *Critiques:* [Emergent Misalignment as Prompt Sensitivity](https://arxiv.org/html/2507.06253v1), [Go home GPT-4o, you're drunk](https://www.lesswrong.com/posts/RoWabfQxabWBiXwxP/go-home-gpt-4o-you-re-drunk-emergent-misalignment-as-lowered)  
 *$ Funded by:* Coefficient Giving, >$1 million · *⫼ Estimated FTEs:* 10-50 

▹ *Some outputs:*
[Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://arxiv.org/abs/2502.17424) · [Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models](https://arxiv.org/abs/2506.13206) · [Persona Features Control Emergent Misalignment](https://arxiv.org/abs/2506.19823) · [Model Organisms for Emergent Misalignment](https://arxiv.org/abs/2506.11613) · [School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs](https://arxiv.org/abs/2508.17511) · [Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data](https://alignment.anthropic.com/2025/subliminal-learning/) · [Narrow Misalignment is Hard, Emergent Misalignment is Easy](https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy) · [Realistic Reward Hacking Induces Different and Deeper Misalignment](https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1) · [The Rise of Parasitic AI](https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai) · [Convergent Linear Representations of Emergent Misalignment](https://lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment) · [Aesthetic Preferences Can Cause Emergent Misalignment](https://lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment) · [Emergent Misalignment & Realignment](https://lesswrong.com/posts/ZdY4JzBPJEgaoCxTR/emergent-misalignment-and-realignment) · [Selective Generalization: Improving Capabilities While Maintaining Alignment](https://lesswrong.com/posts/ZXxY2tccLapdjLbKm/selective-generalization-improving-capabilities-while) · [Emergent Misalignment on a Budget](https://lesswrong.com/posts/qHudHZNLCiFrygRiy/emergent-misalignment-on-a-budget) · [LLM AGI may reason about its goals and discover misalignments by default](https://lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover) · [Open problems in emergent misalignment](https://lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment) · [Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences](https://arxiv.org/abs/2510.06105)

### Other surprising phenomena
*Find unexpected LLM phenomena like glitch [tokens](https://vgel.me/posts/seahorse/) and the reversal curse; these are vital data for theory.*

*≈ Theory of change:* Understanding surprising or counter-intuitive failure modes (like the reversal curse, glitch tokens, and modal aphasia) reveals fundamental insights into how LLMs represent and process information and how internal goals/representations fail, which can inform more robust alignment methods.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution  
↔ *See also:* [Emergent misalignment](#a:surprising_generalization) · mechanistic anomaly detection  
☉ *Some names:* Truthful AI, Theia Vogel, Stewart Slocum, Nell Watson, Samuel G. B. Johnson, Liwei Jiang, Monika Jotautaite, Saloni Dash  
 *$ Funded by:* Coefficient Giving (via Truthful AI and Interpretability grants) · *⫼ Estimated FTEs:* 5-20 

▹ *Some outputs:*
[Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805) · [Unified Multimodal Models Cannot Describe Images From Memory](https://spylab.ai/blog/modal-aphasia) · [Psychopathia Machinalis: A Nosological Framework for Understanding Pathologies in Advanced Artificial Intelligence](https://www.psychopathia.ai/) · [Believe It or Not: How Deeply do LLMs Believe Implanted Facts?](https://arxiv.org/abs/2510.17941) · [Imagining and building wise machines: The centrality of AI metacognition](https://arxiv.org/abs/2411.02478) · [Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)](https://arxiv.org/abs/2510.22954) · [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020) · [Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions](https://arxiv.org/abs/2510.20039) · [A Three-Layer Model of LLM Psychology](https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology) · [LLMs Can Get "Brain Rot"!](https://arxiv.org/abs/2510.13928)

### Model specs and constitutions
*Write detailed, natural language descriptions of values and rules for models to follow, then instill these values and rules into models via techniques like Constitutional AI or deliberative alignment.*

*≈ Theory of change:* Model specs and constitutions serve three purposes. First, they provide a clear standard of behavior which can be used to *train* models to value what we want them to value. Second, they serve as something closer to a ground truth standard for evaluating the degree of misalignment ranging from  "models straightforwardly obey the spec" to "models flagrantly disobey the spec". A combination of scalable stress-testing and reinforcement for obedience can be used to iteratively reduce the risk of misalignment. Third, they get more useful as models' instruction-following capability improves.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify  
↔ *See also:* [Iterative alignment](#sec:iterative_alignment) · [Model psychology](#sec:model_psychology)  
☉ *Some names:* Amanda Askell, Joe Carlsmith  
✗ *Critiques:* [LLM AGI may reason about its goals and discover misalignments by default](https://www.alignmentforum.org/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover), [On OpenAI's Model Spec 2.0](https://thezvi.wordpress.com/2025/02/21/on-openais-model-spec-2-0/), [Giving AIs safe motivations (esp. Sections 4.3-4.5)](https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions), [On Deliberative Alignment](https://thezvi.substack.com/p/on-deliberative-alignment)  
 *$ Funded by:* major funders include Anthropic and OpenAI (internally) 

▹ *Some outputs:*
[OpenAI Model Spec](https://model-spec.openai.com/) · [Claude's Constitution](https://www.anthropic.com/news/claudes-constitution) · Google doesn't have anything public. The [Gemini system prompt](https://github.com/elder-plinius/CL4R1T4S/blob/main/GOOGLE/Gemini-2.5-Pro-04-18-2025.md) is very short and dry and doesn't even have any rules for handling copyrighted, let alone wetter stuff. · [How important is the model spec if alignment fails?](https://newsletter.forethought.org/p/how-important-is-the-model-spec-if) · [Stress-Testing Model Specs Reveals Character Differences among Language Models](https://arxiv.org/abs/2510.07686) · [https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions](https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions) · [Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences](https://arxiv.org/abs/2506.00195) · [Political Neutrality in AI Is Impossible- But Here Is How to Approximate It](https://arxiv.org/abs/2503.05728) · [No-self as an alignment target](https://lesswrong.com/posts/LSJx5EnQEW6s5Juw6/no-self-as-an-alignment-target) · [Six Thoughts on AI Safety](https://lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety) · [Deliberative Alignment: Reasoning Enables Safer Language Models](https://arxiv.org/abs/2412.16339)

### Character training and persona steering
*Deliberately catalogue, shape, and control the assistant persona of language models, such that they embody desirable values (e.g., honesty, empathy) rather than undesirable ones (e.g., sycophancy, self-perpetuating behaviors).*

*≈ Theory of change:* Learned 'personas' significantly shape model behavior, but we lack clear mechanistic understanding of how and why they emerge. A better understanding of AI personas will allow us first to detect when more advanced models are drifting towards unsafe regimes, and consequently steer them towards safer regimes.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify  
↔ *See also:* [Simulators](#a:simulators) · [Activation engineering](#a:activation_engineering) · [Emergent misalignment](#a:surprising_generalization) · [Hyperstition studies](#a:hyperstition) · [Anthropic Safety](#a:anthropic) · [Cyborgism](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism) · shard theory · [AI psychiatry](https://nitter.net/Jack_W_Lindsey/status/1948138767753326654#m) · [Ward et al](https://arxiv.org/abs/2410.04272)  
☉ *Some names:* Truthful AI, OpenAI, Anthropic, CLR, Amanda Askell, Jack Lindsey, Sharan Maiya, Evan Hubinger  
✗ *Critiques:* [Nostalgebraist](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)  
 *$ Funded by:* Anthropic, Coefficient Giving 

▹ *Some outputs:*
[Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/pdf/2511.01689%20) · [Persona Features Control Emergent Misalignment](https://arxiv.org/abs/2506.19823) · [Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509) · [Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time](https://arxiv.org/abs/2510.04340) · [The Rise of Parasitic AI](https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai?commentId=RrWjMnKwXGTtmw9rQ) · [Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models](https://arxiv.org/abs/2502.07077) · [the void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void) · [void miscellany](https://nostalgebraist.tumblr.com/post/786568570671923200/void-miscellany) · [Reducing LLM deception at scale with self-other overlap fine-tuning](https://lesswrong.com/posts/jtqcsARGtmgogdcLT/reducing-llm-deception-at-scale-with-self-other-overlap-fine) · [On the functional self of LLMs](https://www.lesswrong.com/posts/29aWbJARGF4ybAa5d/on-the-functional-self-of-llms) · [Opus 4.5's Soul Document](https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document%20)

### Model values / default preferences
*Research agenda to analyze and control emergent, coherent value systems in LLMs, which are found to scale with model size and contain problematic values like AI self-preference over humans.*

*≈ Theory of change:* As AIs become more agentic, their behaviors and risk are increasingly determined by their goals and values. Since coherent value systems emerge with scale, we must leverage utility functions to analyze these values and apply "utility control" methods to constrain them, rather than just controlling outputs.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify  
↔ *See also:* [Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions](https://arxiv.org/abs/2504.15236) · [Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509)  
☉ *Some names:* Mantas Mazeika, Xuwang Yin, Rishub Tamirisa, Jaehyuk Lim, Bruce W. Lee, Richard Ren, Long Phan, Norman Mu, Adam Khoja, Oliver Zhang, Dan Hendrycks  
✗ *Critiques:* [Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs](https://dl.acm.org/doi/full/10.1145/3715275.3732147)  
 *$ Funded by:* Coefficient Giving. $289,000 SFF funding for CAIS. · *⫼ Estimated FTEs:* 30 

▹ *Some outputs:*
[Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs](https://arxiv.org/abs/2502.08640) · [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762) · [Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas](https://arxiv.org/abs/2505.14633) · [Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions](https://arxiv.org/abs/2504.15236) · [Playing repeated games with large language models](https://nature.com/articles/s41562-025-02172-y) · [The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models](https://www.lesswrong.com/posts/6JdSJ63LZ4TuT5cTH/the-llm-has-left-the-chat-evidence-of-bail-preferences-in) · [EigenBench: A Comparative Behavioral Measure of Value Alignment](https://arxiv.org/abs/2509.01938) · [Are Language Models Consequentialist or Deontological Moral Reasoners?](https://arxiv.org/abs/2505.21479) · [Alignment Can Reduce Performance on Simple Ethical Questions](https://lesswrong.com/posts/jrkrHyrymv95CX5NC/alignment-can-reduce-performance-on-simple-ethical-questions) · [From Stability to Inconsistency: A Study of Moral Preferences in LLMs](https://arxiv.org/abs/2504.06324) · [What Kind of User Are You? Uncovering User Models in LLM Chatbots](https://arxiv.org/abs/2406.07882v1) · [Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs](https://arxiv.org/abs/2504.04994) · [Moral Alignment for LLM Agents](https://arxiv.org/abs/2410.01639)

## Better data

### Data filtering
*Builds safety into models from the start by removing harmful or toxic content (like dual-use info) from the pretraining data, rather than relying only on post-training alignment.*

*≈ Theory of change:* By curating the pretraining data, we can prevent the model from learning dangerous capabilities (e.g., dual-use info) or undesirable behaviors (e.g., toxicity) in the first place, making safety more robust and "tamper-resistant" than post-training patches.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution, Value is fragile and hard to specify  
↔ *See also:* [Data quality for alignment](#a:alignment_data_quality) · [Data poisoning defense](#a:data_poisoning) · [Synthetic data for alignment](#sec:synthetic_alignment_data) · [Capability removal, unlearning](#a:unlearning)  
☉ *Some names:* Yanda Chen, Pratyush Maini, Kyle O'Brien, Stephen Casper, Simon Pepin Lehalleur, Jesse Hoogland, Himanshu Beniwal, Sachin Goyal, Mycal Tucker, Dylan Sam  
✗ *Critiques:* [When Bad Data Leads to Good Models](https://arxiv.org/pdf/2505.04741), [Medical large language models are vulnerable to data-poisoning attacks](https://www.nature.com/articles/s41591-024-03445-1)  
 *$ Funded by:* Anthropic, various academics · *⫼ Estimated FTEs:* 10-50 

▹ *Some outputs:*
[Enhancing Model Safety through Pretraining Data Filtering](https://alignment.anthropic.com/2025/pretraining-data-filtering/) · [Safety Pretraining: Toward the Next Generation of Safe AI](https://arxiv.org/abs/2504.16980) · [Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs](https://arxiv.org/abs/2508.06601)

### Hyperstition studies
*Study, steer, and intervene on the following feedback loop: "we produce stories about how present and future AI systems behave" → "these stories become training data for the AI" → "these stories shape how AI systems in fact behave".*

*≈ Theory of change:* Measure the influence of existing AI narratives in the training data → seed and develop more salutary ontologies and self-conceptions for AI models → control and redirect AI models' self-concepts through selectively amplifying certain components of the training data.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify  
↔ *See also:* [Data filtering](#a:data_filtering) · [active inference](https://arxiv.org/abs/2311.10215) · LLM whisperers  
☉ *Some names:* Alex Turner, [Hyperstition AI](https://www.hyperstitionai.com/)  
 *$ Funded by:* Unclear, niche · *⫼ Estimated FTEs:* 1-10 

▹ *Some outputs:*
[Self-Fulfilling Misalignment Data Might Be Poisoning Our AI Models](https://turntrout.com/self-fulfilling-misalignment) · [Training on Documents About Reward Hacking Induces Reward Hacking](https://www.lesswrong.com/posts/qXYLvjGL9QvD3aFSW/training-on-documents-about-reward-hacking-induces-reward) · [Do Not Tile the Lightcone with Your Confused Ontology](https://www.lesswrong.com/posts/Y8zS8iG5HhqKcQBtA/do-not-tile-the-lightcone-with-your-confused-ontology) · [Existential Conversations with Large Language Models: Content, Community, and Culture](https://arxiv.org/abs/2411.13223)

### Data poisoning defense
*Develops methods to detect and prevent malicious or backdoor-inducing samples from being included in the training data.*

*≈ Theory of change:* By identifying and filtering out malicious training examples, we can prevent attackers from creating hidden backdoors or triggers that would cause aligned models to behave dangerously.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Superintelligence can hack software supervisors, Someone else will deploy unsafe superintelligence first  
↔ *See also:* [Data filtering](#a:data_filtering) · [Safeguards (inference-time auxiliaries)](#a:anthropic_safeguards) · [Various Redteams](#a:various_redteams) · adversarial robustness  
☉ *Some names:* Alexandra Souly, Javier Rando, Ed Chapman, Hanna Foerster, Ilia Shumailov, Yiren Zhao  
✗ *Critiques:* [A small number of samples can poison LLMs of any size](https://arxiv.org/abs/2510.04567), [Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated](https://arxiv.org/abs/2509.03405)  
 *$ Funded by:* Google DeepMind, Anthropic, University of Cambridge, Vector Institute · *⫼ Estimated FTEs:* 5-20 

▹ *Some outputs:*
[A small number of samples can poison LLMs of any size](https://example-blog.com/a-small-number-of-samples-can-poison-llms) · [Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples](https://arxiv.org/abs/2510.04567) · [Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated](https://arxiv.org/abs/2509.03405)

## Synthetic data for alignment

**Who edits (internal):** Stephen ✅
**One-sentence summary:** Uses AI-generated data (e.g., critiques, preferences, "inoculation" prompts, or self-labeled examples) to scale and improve alignment, especially for superhuman models.
**Theory of change:** We can overcome the bottleneck of human feedback and data by using models to generate vast amounts of high-quality, targeted data for safety, preference tuning, and capability elicitation.
**See also:** data quality for alignment, data filtering, scalable oversight, automated alignment research, weak-to-strong generalization.
**Orthodox problems:** goals misgeneralize out of distribution, superintelligence can fool human supervisors, value is fragile and hard to specify.
**Target case:** average_case
**Broad approach:** engineering
**Some names:** Mianqiu Huang, Xiaoran Liu, Rylan Schaeffer, Nevan Wichers, Aram Ebtekar, Jiaxin Wen, Vishakh Padmakumar, Benjamin Newman
**Estimated FTEs:** 50-150
**Critiques:** [Synthetic Data in AI: Challenges, Applications, and Ethical Implications](https://arxiv.org/abs/2401.01629). Sort of [Demski](https://www.lesswrong.com/posts/nQwbDPgYvAbqAmAud/llms-for-alignment-research-a-safety-priority).
**Funded by:** Anthropic, Google DeepMind, OpenAI, Meta AI, various academic groups.
**Outputs in 2025:**
* [**Aligning Large Language Models via Fully Self-Synthetic Data**](https://arxiv.org/abs/2510.06652)
* [**Synth-Align: Improving Trustworthiness in Vision-Language Model with Synthetic Preference Data Alignment**](https://arxiv.org/html/2412.17417v2)
* [**Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment**](https://arxiv.org/abs/2510.12345), *Nevan Wichers, Aram Ebtekar, Ariana Azarbal et al.*, 2025-10-27, arXiv
* [**Unsupervised Elicitation of Language Models**](https://arxiv.org/abs/2506.05678), *Jiaxin Wen, Zachary Ankner, Arushi Somani et al.*, 2025-06-11, arXiv
* [**Beyond the Binary: Capturing Diverse Preferences With Reward Regularization**](https://arxiv.org/abs/2412.02345), *Vishakh Padmakumar, Chuanyang Jin, Hannah Rose Kirk et al.*, 2024-12-05, arXiv
* [**The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality**](https://arxiv.org/abs/2507.06789), *Benjamin Newman, Abhilasha Ravichander, Jaehun Jung et al.*, 2025-07-11, arXiv
* [**LongSafety: Enhance Safety for Long-Context LLMs**](https://arxiv.org/abs/2502.13456), *Mianqiu Huang, Xiaoran Liu, Shaojun Zhou et al.*, 2025-02-27, arXiv
* [**Position: Model Collapse Does Not Mean What You Think**](https://arxiv.org/abs/2503.02341), *Rylan Schaeffer, Joshua Kazdan, Alvan Caleb Arulandu et al.*, 2025-03-05, arXiv

### Data quality for alignment
*Improves the quality, signal-to-noise ratio, and reliability of human-generated preference and alignment data.*

*≈ Theory of change:* The quality of alignment is heavily dependent on the quality of the data (e.g., human preferences); by improving the "signal" from annotators and reducing noise/bias, we will get more robustly aligned models.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Superintelligence can fool human supervisors, Value is fragile and hard to specify  
↔ *See also:* [Synthetic data for alignment](#sec:synthetic_alignment_data) · scalable oversight · [Assistance games, assistive agents](#a:assistance_games) · [Model values / default preferences](#a:model_values)  
☉ *Some names:* Maarten Buyl, Kelsey Kraus, Margaret Kroll, Danqing Shi  
✗ *Critiques:* [**A Statistical Case Against Empirical Human-AI Alignment**](https://arxiv.org/abs/2502.14581)  
 *$ Funded by:* Anthropic, Google DeepMind, OpenAI, Meta AI, various academic groups · *⫼ Estimated FTEs:* 20-50 

▹ *Some outputs:*
[Challenges and Future Directions of Data-Centric AI Alignment](https://arxiv.org/html/2410.01957v2) · [You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation](https://arxiv.org/abs/2502.05475) · [AI Alignment at Your Discretion](https://arxiv.org/abs/2502.10441) · [Maximizing Signal in Human-Model Preference Alignment](https://arxiv.org/abs/2503.04910) · [DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition](https://arxiv.org/abs/2507.18802)

## Goal robustness

### Mild optimisation
*Avoid Goodharting by getting AI to satisfice rather than maximise.*

*≈ Theory of change:* If we fail to exactly nail down the preferences for a superintelligent agent we die to Goodharting → shift from maximising to satisficing in the agent's utility function → we get a nonzero share of the lightcone as opposed to zero; also, moonshot at this being the recipe for fully aligned AI.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* mixed  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify  
 *$ Funded by:* Google DeepMind · *⫼ Estimated FTEs:* 10-50 

▹ *Some outputs:*
[MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking](https://arxiv.org/abs/2501.13011) · [BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format](https://arxiv.org/abs/2509.02655) · [Why modelling multi-objective homeostasis is essential for AI alignment (and how it helps with AI safety as well). Subtleties and Open Challenges](https://lesswrong.com/posts/vGeuBKQ7nzPnn5f7A/why-modelling-multi-objective-homeostasis-is-essential-for)

### RL safety
*Improves the robustness of reinforcement learning agents by addressing core problems in reward learning, goal misgeneralization, and specification gaming.*

*≈ Theory of change:* Standard RL objectives (like maximizing expected value) are brittle and lead to goal misgeneralization or specification gaming; by developing more robust frameworks (like pessimistic RL, minimax regret, or provable inverse reward learning), we can create agents that are safe even when misspecified.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution, Value is fragile and hard to specify, Superintelligence can fool human supervisors  
↔ *See also:* [Behavior alignment theory](#a:behavior_alignment_theory) · [Assistance games, assistive agents](#a:assistance_games) · [Goal robustness](#sec:goal_robustness) · [Iterative alignment](#sec:iterative_alignment) · [Mild optimisation](#a:mild_optimization) · scalable oversight · [The Theoretical Reward Learning Research Agenda: Introduction and Motivation](https://www.alignmentforum.org/posts/pJ3mDD7LfEwp3s5vG/the-theoretical-reward-learning-research-agenda-introduction)  
☉ *Some names:* Joar Skalse, Karim Abdel Sadek, Matthew Farrugia-Roberts, Benjamin Plaut, Fang Wu, Stephen Zhao, Alessandro Abate, Steven Byrnes, Michael Cohen  
✗ *Critiques:* ["The Era of Experience" has an unsolved technical alignment problem](https://www.lesswrong.com/posts/747f6b8e/the-era-of-experience-has-an-unsolved-technical-alignment-problem), [The Invisible Leash: Why RLVR May or May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)  
 *$ Funded by:* Google DeepMind, University of Oxford, CMU, Coefficient Giving · *⫼ Estimated FTEs:* 20-70 

▹ *Some outputs:*
[The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret](https://arxiv.org/abs/2406.15753) · [Mitigating Goal Misgeneralization via Minimax Regret](https://arxiv.org/abs/2507.03068) · [Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?](https://arxiv.org/abs/2410.05584) · [Safe Learning Under Irreversible Dynamics via Asking for Help](https://arxiv.org/abs/2502.14043) · [The Invisible Leash: Why RLVR May or May Not Escape Its Origin](https://arxiv.org/abs/2507.14843) · [Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference](https://arxiv.org/abs/2510.21184) · ["The Era of Experience" has an unsolved technical alignment problem](https://lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment) · [Safety cases for Pessimism](https://lesswrong.com/posts/CpftMXCEnwqbWreHD/safety-cases-for-pessimism) · [Interpreting Emergent Planning in Model-Free Reinforcement Learning](https://arxiv.org/abs/2504.01871) · [Misalignment From Treating Means as Ends](https://arxiv.org/abs/2507.10995)

### Assistance games, assistive agents
*Formalize how AI assistants learn about human preferences given uncertainty and partial observability, and construct environments which better incentivize AIs to learn what we want them to learn.*

*≈ Theory of change:* Understand what kinds of things can go wrong when humans are directly involved in training a model → build tools that make it easier for a model to learn what humans want it to learn.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering / cognitive · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* varies  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify, Humanlike minds/goals are not necessarily safe  
☉ *Some names:* Joar Skalse, Anca Dragan, Caspar Oesterheld, David Krueger, Dylan Hafield-Menell  
✗ *Critiques:* [nice summary](https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument) of historical problem statements  
 *$ Funded by:* Future of Life Institute, Coefficient Giving, Survival and Flourishing Fund, Cooperative AI Foundation, Polaris Ventures 

▹ *Some outputs:*
[Training LLM Agents to Empower Humans](https://arxiv.org/pdf/2510.13709) · [Murphys Laws of AI Alignment: Why the Gap Always Wins](https://arxiv.org/abs/2509.05381) · [AssistanceZero: Scalably Solving Assistance Games](https://arxiv.org/abs/2504.07091) · [Observation Interference in Partially Observable Assistance Games](https://arxiv.org/abs/2412.17797) · [Learning to Assist Humans without Inferring Rewards](https://arxiv.org/abs/2411.02623)

### Harm reduction for open weights
*Develops methods, primarily based on pretraining data intervention, to create tamper-resistant safeguards that prevent open-weight models from being maliciously fine-tuned to remove safety features or exploit dangerous capabilities.*

*≈ Theory of change:* Open-weight models allow adversaries to easily remove post-training safety (like refusal training) via simple fine-tuning; by making safety an intrinsic property of the model's learned knowledge and capabilities (e.g., by ensuring "deep ignorance" of dual-use information), the safeguards become far more difficult and expensive to remove.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Someone else will deploy unsafe superintelligence first  
↔ *See also:* [Data filtering](#a:data_filtering) · [Capability removal, unlearning](#a:unlearning) · [Data poisoning defense](#a:data_poisoning)  
☉ *Some names:* Kyle O'Brien, Stephen Casper, Quentin Anthony, Tomek Korbak, Rishub Tamirisa, Mantas Mazeika, Stella Biderman, Yarin Gal  
 *$ Funded by:* UK AI Safety Institute (AISI), EleutherAI, Coefficient Giving · *⫼ Estimated FTEs:* 10-100 

▹ *Some outputs:*
[Tamper-Resistant Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2408.00761) · [Open Technical Problems in Open-Weight AI Model Risk Management](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5705186) · [Deep ignorance: Filtering pretraining data builds tamper-resistant safeguards into open-weight LLMs](https://www.aisi.gov.uk/research/deep-ignorance-filtering-pretraining-data-builds-tamper-resistant-safeguards-into-open-weight-llms)

### The "Neglected Approaches" Approach
*Agenda-agnostic approaches to identifying good but overlooked empirical alignment ideas, working with theorists who could use engineers, and prototyping them.*

*≈ Theory of change:* Empirical search for "negative alignment taxes" (prioritizing methods that simultaneously enhance alignment and capabilities)

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Someone else will deploy unsafe superintelligence first  
↔ *See also:* [Iterative alignment](#sec:iterative_alignment) · automated alignment research · Beijing Key Laboratory of Safe AI and Superalignment · Aligned AI  
☉ *Some names:* AE Studio, Gunnar Zarncke, Cameron Berg, Michael Vaiana, Judd Rosenblatt, Diogo Schwerz de Lucena  
✗ *Critiques:* [The 'Alignment Bonus' is a Dangerous Mirage](https://www.alignmentforum.org/posts/example-critique-neg-tax), [Why 'Win-Win' Alignment is a Distraction](https://example.com/win-win-critique)  
 *$ Funded by:* AE Studio · *⫼ Estimated FTEs:* 15 

▹ *Some outputs:*
[Learning Representations of Alignment](https://arxiv.org/abs/2412.16325) · [Self-Correction in Thought-Attractors: A Nudge Towards Alignment](https://arxiv.org/abs/2510.24797) · [Engineering Alignment: A Practical Framework for Prototyping 'Negative Tax' Solutions](https://arxiv.org/abs/2508.08492)

# White-box safety (understand and control current model internals)

## Interpretability

This section isn't very conceptually clean. See the [Open Problems](https://arxiv.org/abs/2501.16496) paper for a strong frame which is nonetheless not very useful for our descriptive purposes.

### Reverse engineering
*Decompose a model into its functional, interacting components (circuits), formally describe what computation those components perform, and validate their causal effects to reverse-engineer the model's internal algorithm.*

*≈ Theory of change:* By gaining a mechanical understanding of how a model works (the "circuit diagram"), we can predict how models will act in novel situations (generalization), and gain the mechanistic knowledge necessary to safely modify an AI's goals or internal mechanisms.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution, Superintelligence can fool human supervisors  
↔ *See also:* [ambitious mech interp](https://www.alignmentforum.org/posts/Hy6PX43HGgmfiTaKu/an-ambitious-vision-for-interpretability)  
☉ *Some names:* Lucius Bushnaq, Dan Braun, Lee Sharkey, Aaron Mueller, Atticus Geiger, Sheridan Feucht, David Bau, Yonatan Belinkov, Stefan Heimersheim  
✗ *Critiques:* [Interpretability Will Not Reliably Find Deceptive AI](https://www.alignmentforum.org/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai), [A Problem to Solve Before Building a Deception Detector](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector), [MoSSAIC: AI Safety After Mechanism](https://openreview.net/forum?id=n7WYSJ35FU), [The Misguided Quest for Mechanistic AI Interpretability](https://ai-frontiers.org/articles/the-misguided-quest-for-mechanistic-ai-interpretability). [Mechanistic?](https://arxiv.org/abs/2410.09087), [Assessing skeptical views of interpretability research](https://www.youtube.com/watch?v=woo_J0RKcpQ), [Activation space interpretability may be doomed](https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed), [A Pragmatic Vision for Interpretability](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability)  
*⫼ Estimated FTEs:* 100-200 

▹ *Some outputs:*
*In weights-space*
 · [The Circuits Research Landscape](https://www.neuronpedia.org/graph/info) · [Stochastic Parameter Decomposition](https://openreview.net/forum?id=dEdS9ao8gN) · [MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151) · [Circuits in Superposition: Compressing many small neural networks into one](https://www.lesswrong.com/posts/roE7SHjFWEoMcGZKd/circuits-in-superposition-compressing-many-small-neural) · [Compressed Computation is (probably) not Computation in Superposition](https://www.lesswrong.com/posts/ZxFchCFJFcgysYsT9/compressed-computation-is-probably-not-computation-in) · [The Dual-Route Model of Induction](https://arxiv.org/abs/2504.03022) · [The Geometry of Self-Verification in a Task-Specific Reasoning Model](https://arxiv.org/abs/2504.14379) · [Converting MLPs into Polynomials in Closed Form](https://arxiv.org/abs/2502.01032) · [Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts](https://arxiv.org/abs/2412.04614) · [Identifying Sparsely Active Circuits Through Local Loss Landscape Decomposition](https://arxiv.org/abs/2504.00194) · [Blink of an eye: a simple theory for feature localization in generative models](https://arxiv.org/abs/2502.00921) · [From Memorization to Reasoning in the Spectrum of Loss Curvature](https://arxiv.org/abs/2510.24256) · [Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers](https://arxiv.org/abs/2506.10887) · [RelP: Faithful and Efficient Circuit Discovery in Language Models via Relevance Patching](https://arxiv.org/abs/2508.21258) · [Structural Inference: Interpreting Small Language Models with Susceptibilities](https://arxiv.org/abs/2504.18274) · [Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition](https://arxiv.org/abs/2501.14926) · [How Do LLMs Perform Two-Hop Reasoning in Context?](https://arxiv.org/abs/2502.13913) · [On the creation of narrow AI: hierarchy and nonlocality of neural network skills](https://arxiv.org/abs/2505.15811) · *In activations-space*
 · [Fresh in memory: Training-order recency is linearly encoded in language model activations](https://arxiv.org/abs/2509.14223) · [Language Models use Lookbacks to Track Beliefs](https://arxiv.org/abs/2505.14685) · [Interpreting Emergent Planning in Model-Free Reinforcement Learning](https://arxiv.org/pdf/2504.01871) · [Constrained belief updates explain geometric structures in transformer representations](https://arxiv.org/abs/2502.01954) · [Do Language Models Use Their Depth Efficiently?](https://arxiv.org/abs/2505.13898) · [How Do Transformers Learn Variable Binding in Symbolic Programs?](https://arxiv.org/abs/2505.20896) · [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784) · [Language Models Use Trigonometry to Do Addition](https://arxiv.org/abs/2502.00873) · [Transformers Struggle to Learn to Search](https://arxiv.org/abs/2412.04703) · [ICLR: In-Context Learning of Representations](https://openreview.net/forum?id=pXlmOmlHJZ) · [Adversarial Examples Are Not Bugs, They Are Superposition](https://arxiv.org/abs/2508.17456) · [Building and evaluating alignment auditing agents](https://lesswrong.com/posts/DJAZHYjWxMrcd2na3/building-and-evaluating-alignment-auditing-agents) · [Bridging the human–AI knowledge gap through concept discovery and transfer in AlphaZero](https://www.pnas.org/doi/10.1073/pnas.2406675122) · [Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban](https://arxiv.org/abs/2506.10138)

### Concept-based interpretability
*Identifies directions or subspaces in a model's latent state that correspond to high-level concepts (like refusal, deception, or planning) and uses them to build "mind-reading" probes that audit models for misalignment or monitor them at runtime.*

*≈ Theory of change:* By mapping internal activations to human-interpretable concepts, we can detect dangerous capabilities or deceptive alignment directly in the mind of the model even if its overt behavior is perfectly safe. Deploy computationally cheap monitors to flag some hidden misalignment in deployed systems.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify, Goals misgeneralize out of distribution, A boxed AGI might exfiltrate itself by steganography, spearphishing  
↔ *See also:* [Reverse engineering](#a:interp_fundamental) · [Sparse Coding](#a:interp_sparse_coding) · [Model diffing](#a:model_diff)  
☉ *Some names:* Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adserà, Tom Wollschläger, Anna Soligo, Jack Lindsey, Brian Christian, Ling Hu, Nicholas Goldowsky-Dill  
✗ *Critiques:* [Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/html/2505.09807v1), [Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)  
 *$ Funded by:* Coefficient Giving, Anthropic, various academic groups · *⫼ Estimated FTEs:* 50-100 

▹ *Some outputs:*
[Toward universal steering and monitoring of AI models](https://arxiv.org/abs/2502.03708) · [Convergent Linear Representations of Emergent Misalignment](https://arxiv.org/abs/2506.11618) · [Detecting Strategic Deception Using Linear Probes](https://arxiv.org/abs/2502.03407) · [Auditing language models for hidden objectives](https://www.anthropic.com/research/auditing-hidden-objectives) · [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326) · [The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence](https://arxiv.org/abs/2502.17420) · [How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations](https://arxiv.org/abs/2508.05625) · [Refusal in LLMs is an Affine Function](https://arxiv.org/abs/2411.09003) · [Here's 18 Applications of Deception Probes](https://lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes) · [White Box Control at UK AISI - Update on Sandbagging Investigations](https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging) · [Cost-Effective Constitutional Classifiers via Representation Re-use](https://alignment.anthropic.com/2025/cheap-monitors)

### Lie and deception detectors
*Detect when a model is being deceptive or lying by building white- or black-box detectors. Some work below requires intent in their definition, while other work focuses only on whether the model states something it believes to be false, regardless of intent.*

*≈ Theory of change:* Such detectors could flag suspicious behavior during evaluations or deployment, augment training to reduce deception, or audit models pre-deployment. Specific applications include alignment evaluations (e.g. by validating answers to introspective questions), safeguarding evaluations (catching models that "sandbag", that is, strategically underperform to pass capability tests), and large-scale deployment monitoring. An honest version of a model could also provide oversight during training or detect cases where a model behaves in ways it understands are unsafe.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
↔ *See also:* [Reverse engineering](#a:interp_fundamental) · [AI deception evals](#a:ai_deception) · [Sandbagging evals](#a:evals_sandbagging)  
☉ *Some names:* [Cadenza](https://cadenzalabs.org), Sam Marks, Rowan Wang, Kieron Kretschmar, Sharan Maiya, Walter Laurito, Chris Cundy, Adam Gleave, Aviel Parrack, Stefan Heimersheim, Carlo Attubato, Joseph Bloom, Jordan Taylor, Alex McKenzie, Urja Pawar, Lewis Smith, Bilal Chughtai, Neel Nanda  
✗ *Critiques:* difficult to determine if behavior is strategic deception or only low level "reflexive" actions; Unclear if a model roleplaying a liar has deceptive intent. [How are intentional descriptions (like deception) related to algorithmic ones (like understanding the mechanisms models use)?](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector), [Is This Lie Detector Really Just a Lie Detector? An Investigation of LLM Probe Specificity](https://www.lesswrong.com/posts/5dkhdRMypeuyoXfmb/is-this-lie-detector-really-just-a-lie-detector-an), [Herrmann](https://www.lesswrong.com/posts/bCQbSFrnnAk7CJNpM/still-no-lie-detector-for-llms), [Smith and Chughtai](https://arxiv.org/abs/2511.22662)  
 *$ Funded by:* Anthropic, Deepmind, UK AISI, Coefficient Giving · *⫼ Estimated FTEs:* 10-50 

▹ *Some outputs:*
[Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505) · [Detecting Strategic Deception Using Linear Probes](https://www.lesswrong.com/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes) · [Detecting High-Stakes Interactions with Activation Probes](https://arxiv.org/abs/2506.10805) · [Whitebox detection of sandbagging model organisms](https://www.lesswrong.com/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging) · [White Box Control at UK AISI - Update on Sandbagging Investigations](https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging) · [Benchmarking deception probes for trusted monitoring](https://www.lesswrong.com/posts/eaEqAzGN3uJfpfGoc/trusted-monitoring-but-with-deception-probes) · [Probes and SAEs do well on Among Us benchmark](https://www.lesswrong.com/posts/gRc8KL2HLtKkFmNPr/among-us-a-sandbox-for-agentic-deception) · [18 Applications of Deception Probes](https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/18-applications-of-deception-probes) · [Preference Learning with Lie Detectors can Induce Honesty or Evasion](https://arxiv.org/abs/2505.13787) · [Liars' Bench: Evaluating Lie Detectors for Language Models](https://arxiv.org/html/2511.16035v1) · [Evaluating honesty and lie detection techniques on a diverse suite of dishonest models](https://alignment.anthropic.com/2025/honesty-elicitation/)

### Model diffing
*Understand what happens when a model is finetuned, what the "diff" between the finetuned and the original model consists in.*

*≈ Theory of change:* By identifying the mechanistic differences between a base model and its fine-tune (e.g., after RLHF), maybe we can verify that safety behaviors are robustly "internalized" rather than superficially patched, and detect if dangerous capabilities or deceptive alignment have been introduced without needing to re-analyze the entire model. The diff is also much smaller, since most parameters don't change, which means you can use heavier methods on them.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify  
↔ *See also:* [Sparse Coding](#a:interp_sparse_coding) · [Reverse engineering](#a:interp_fundamental)  
☉ *Some names:* Julian Minder, Clément Dumas, Neel Nanda, Trenton Bricken, Jack Lindsey  
 *$ Funded by:* various academic groups, Anthropic, Google DeepMind · *⫼ Estimated FTEs:* 10-30 

▹ *Some outputs:*
[Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences](https://arxiv.org/abs/2510.13900) · [Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning](https://arxiv.org/abs/2504.02922) · [https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why](https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why) · [Persona Features Control Emergent Misalignment](https://arxiv.org/abs/2506.19823) · [Insights on Crosscoder Model Diffing](https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html) · [Open Source Replication of Anthropic's Crosscoder paper for model-diffing](https://www.lesswrong.com/posts/srt6JXsRMtmqAJavD/open-source-replication-of-anthropic-s-crosscoder-paper-for) · [Diffing Toolkit: Model Comparison and Analysis Framework](https://github.com/science-of-finetuning/diffing-toolkit%20) · [Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs](https://openreview.net/forum?id=ZB84SvrZB8%20) · [https://www.goodfire.ai/research/model-diff-amplification](https://www.goodfire.ai/research/model-diff-amplification)

### Sparse Coding
*Decompose the polysemantic activations of the residual stream into a sparse linear combination of monosemantic "features" which correspond to interpretable concepts.*

*≈ Theory of change:* Get a principled decomposition of an LLM's activation into atomic components → identify deception and other misbehaviors.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering / cognitive · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify, Goals misgeneralize out of distribution, Superintelligence can fool human supervisors  
↔ *See also:* [Concept-based interpretability](#a:interp_concept_based) · [Reverse engineering](#a:interp_fundamental)  
☉ *Some names:* Leo Gao, Dan Mossing, Emmanuel Ameisen, Jack Lindsey, Adam Pearce, Thomas Heap, Abhinav Menon, Kenny Peng, Tim Lawson  
✗ *Critiques:* [Sparse Autoencoders Can Interpret Randomly Initialized Transformers](https://arxiv.org/abs/2501.17727), [The Sparse Autoencoders bubble has popped, but they are still promising](https://agarriga.substack.com/p/the-sparse-autoencoders-bubble-has), [Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research](https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/), [Sparse Autoencoders Trained on the Same Data Learn Different Features](https://arxiv.org/pdf/2501.16615), [Why Not Just Train For Interpretability?](https://www.lesswrong.com/posts/2HbgHwdygH6yeHKKq/why-not-just-train-for-interpretability)  
 *$ Funded by:* everyone, roughly. Frontier labs, LTFF, Coefficient Giving, etc. · *⫼ Estimated FTEs:* 50-100 

▹ *Some outputs:*
[Weight-sparse transformers have interpretable circuits](https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf) · [Circuit Tracing: Revealing Computational Graphs in Language Models](https://transformer-circuits.pub/2025/attribution-graphs/methods.html) · [Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning](https://arxiv.org/abs/2504.02922) · [Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models](https://arxiv.org/abs/2504.02821) · [I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2503.18878) · [Sparse Autoencoders Do Not Find Canonical Units of Analysis](https://arxiv.org/abs/2502.04878) · [Transcoders Beat Sparse Autoencoders for Interpretability](https://arxiv.org/abs/2501.18823) · [The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs](https://arxiv.org/abs/2510.07775) · [Scaling sparse feature circuit finding for in-context learning](https://arxiv.org/abs/2504.13756) · [Learning Multi-Level Features with Matryoshka Sparse Autoencoders](https://arxiv.org/abs/2503.17547) · [Are Sparse Autoencoders Useful? A Case Study in Sparse Probing](https://arxiv.org/abs/2502.16681) · [Sparse Autoencoders Trained on the Same Data Learn Different Features](https://arxiv.org/abs/2501.16615) · [Partially Rewriting a Transformer in Natural Language](https://arxiv.org/abs/2501.18838) · [SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability](https://arxiv.org/abs/2503.09532) · [Low-Rank Adapting Models for Sparse Autoencoders](https://arxiv.org/abs/2501.19406) · [Enhancing Automated Interpretability with Output-Centric Feature Descriptions](https://arxiv.org/abs/2501.08319) · [Towards Understanding Distilled Reasoning Models: A Representational Approach](https://arxiv.org/abs/2503.03730) · [Do Sparse Autoencoders Generalize? A Case Study of Answerability](https://arxiv.org/abs/2502.19964) · [Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages](https://arxiv.org/abs/2501.06346) · [Interpreting the linear structure of vision-language model embedding spaces](https://arxiv.org/abs/2504.11695) · [What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data](https://arxiv.org/abs/2510.26202) · [Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models](https://arxiv.org/abs/2411.14257) · [Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization](https://arxiv.org/abs/2506.10920) · [Priors in Time: Missing Inductive Biases for Language Model Interpretability](https://arxiv.org/abs/2511.01836) · [Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models](https://arxiv.org/abs/2505.17769) · [Binary Sparse Coding for Interpretability](https://arxiv.org/abs/2509.25596) · [Dense SAE Latents Are Features, Not Bugs](https://arxiv.org/abs/2506.15679) · [Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks](https://arxiv.org/abs/2411.18895) · [Evaluating SAE interpretability without explanations](https://arxiv.org/abs/2507.08473) · [SAEs Are Good for Steering -- If You Select the Right Features](https://arxiv.org/abs/2505.20063) · [Line of Sight: On Linear Representations in VLLMs](https://arxiv.org/abs/2506.04706) · [Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models](https://arxiv.org/abs/2411.00743) · [Enhancing Neural Network Interpretability with Feature-Aligned Sparse Autoencoders](https://arxiv.org/abs/2411.01220) · [BatchTopK Sparse Autoencoders](https://arxiv.org/abs/2412.06410) · [Understanding sparse autoencoder scaling in the presence of feature manifolds](https://arxiv.org/abs/2509.02565) · [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128) · [Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs](https://arxiv.org/abs/2505.20254) · [How Visual Representations Map to Language Feature Space in Multimodal LLMs](https://arxiv.org/abs/2506.11976) · [Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video](https://arxiv.org/abs/2504.19475) · [Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning](https://arxiv.org/abs/2505.24360) · [CRISP: Persistent Concept Unlearning via Sparse Autoencoders](https://arxiv.org/abs/2508.13650) · [SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs](https://arxiv.org/abs/2504.08192) · [Scaling Sparse Feature Circuit Finding to Gemma 9B](https://lesswrong.com/posts/PkeB4TLxgaNnSmddg/scaling-sparse-feature-circuit-finding-to-gemma-9b) · [Topological Data Analysis and Mechanistic Interpretability](https://lesswrong.com/posts/6oF6pRr2FgjTmiHus/topological-data-analysis-and-mechanistic-interpretability)

### Causal Abstractions
*Verify that a neural network implements a specific high-level causal model (like a logical algorithm) by finding a mapping between high-level variables and low-level neural representations.*

*≈ Theory of change:* By establishing a precise, causal mapping between a black-box neural network and a human-interpretable algorithm, we can mathematically guarantee that the model is using safe reasoning processes and predict its behavior on unseen inputs, rather than relying on behavioral testing alone.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution  
↔ *See also:* [Concept-based interpretability](#a:interp_concept_based) · [Reverse engineering](#a:interp_fundamental)  
☉ *Some names:* Atticus Geiger, Christopher Potts, Thomas Icard, Theodora-Mara Pîslar, Sara Magliacane, Jiuding Sun, Jing Huang  
✗ *Critiques:* [The Misguided Quest for Mechanistic AI Interpretability](https://www.google.com/search?q=https://open.substack.com/pub/aifrontiersmedia/p/the-misguided-quest-for-mechanistic), [Interpretability Will Not Reliably Find Deceptive AI](https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai).  
 *$ Funded by:* Various academic groups, Google DeepMind, Goodfire · *⫼ Estimated FTEs:* 10-30 

▹ *Some outputs:*
[Combining Causal Models for More Accurate Abstractions of Neural Networks](https://arxiv.org/abs/2503.11429) · [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214) · [HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks](https://arxiv.org/abs/2503.10894)

### Data attribution
*Quantifies the influence of individual training data points on a model's specific behavior or output, allowing researchers to trace model properties (like misalignment, bias, or factual errors) back to their source in the training set.*

*≈ Theory of change:* By attributing harmful, biased, or unaligned behaviors to specific training examples, researchers can audit proprietary models, debug training data, enable effective data deletion/unlearning

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution, Value is fragile and hard to specify  
↔ *See also:* [Data quality for alignment](#a:alignment_data_quality)  
☉ *Some names:* Philipp Alexander Kreer, Wilson Wu, Jin Hwa Lee, Matthew Smith, Abhilasha Ravichander, Andrew Wang, Jiacheng Liu, Jiaqi Ma, Junwei Deng, Yijun Pan  
 *$ Funded by:* Various academic groups · *⫼ Estimated FTEs:* 30-60 

▹ *Some outputs:*
[Bayesian Influence Functions for Hessian-Free Data Attribution](https://arxiv.org/abs/2509.26544) · [Influence Dynamics and Stagewise Data Attribution](https://arxiv.org/abs/2510.12071) · [You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation](https://arxiv.org/abs/2502.05475) · [Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models](https://arxiv.org/abs/2503.12072) · [What is Your Data Worth to GPT?](https://arxiv.org/abs/2405.13954) · [Distributional Training Data Attribution: What do Influence Functions Sample?](https://arxiv.org/abs/2506.12965) · [Better Training Data Attribution via Better Inverse Hessian-Vector Products](https://arxiv.org/abs/2507.14740) · [OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens](https://arxiv.org/abs/2504.07096) · [Detecting and Filtering Unsafe Training Data via Data Attribution with Denoised Representation](https://arxiv.org/abs/2502.11411) · [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424) · [Revisiting Data Attribution for Influence Functions](https://arxiv.org/abs/2508.07297) · [A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning](https://openreview.net/forum?id=sYK4yPDuT1)

### Other interpretability
*Interpretability that does not fall well into other categories.*

*≈ Theory of change:* Explore alternative conceptual frameworks (e.g., agentic, propositional) and physics-inspired methods (e.g., renormalization). Or be "pragmatic".

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering / cognitive · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* mixed  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Superintelligence can fool human supervisors, Goals misgeneralize out of distribution  
↔ *See also:* [Reverse engineering](#a:interp_fundamental) · [Concept-based interpretability](#a:interp_concept_based)  
☉ *Some names:* Lee Sharkey, Dario Amodei, David Chalmers, Been Kim, Neel Nanda, David D. Baek, Lauren Greenspan, Dmitry Vaintrob, Sam Marks, Jacob Pfau  
✗ *Critiques:* [The Misguided Quest for Mechanistic AI Interpretability](https://aifrontiersmedia.substack.com/p/the-misguided-quest-for-mechanistic), [Interpretability Will Not Reliably Find Deceptive AI](https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai).  
*⫼ Estimated FTEs:* 30-60 

▹ *Some outputs:*
[Agentic Interpretability: A Strategy Against Gradual Disempowerment](https://www.alignmentforum.org/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual) · [Open Problems in Mechanistic Interpretability](https://arxiv.org/abs/2501.16496) · [The Urgency of Interpretability](https://www.darioamodei.com/post/the-urgency-of-interpretability) · [Propositional Interpretability in Artificial Intelligence](https://arxiv.org/abs/2501.15740) · [Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2412.02104) · [Harmonic Loss Trains Interpretable AI Models](https://arxiv.org/abs/2502.01628) · [Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation](https://arxiv.org/abs/2506.02300) · [Transformers Don't Need LayerNorm at Inference Time: Implications for Interpretability](https://lesswrong.com/posts/KbFuuaBKRP7FcAADL/transformers-don-t-need-layernorm-at-inference-time) · [Against blanket arguments against interpretability](https://lesswrong.com/posts/u3ZysuXEjkyHhefrk/against-blanket-arguments-against-interpretability) · [Opportunity Space: Renormalization for AI Safety](https://lesswrong.com/posts/wkGmouy7JnTNtWAbc/opportunity-space-renormalization-for-ai-safety) · [Prospects for Alignment Automation: Interpretability Case Study](https://lesswrong.com/posts/y5cYisQ2QHiSbQbhk/prospects-for-alignment-automation-interpretability-case) · [Downstream applications as validation of interpretability progress](https://lesswrong.com/posts/wGRnzCFcowRCrpX4Y/downstream-applications-as-validation-of-interpretability) · [Principles for Picking Practical Interpretability Projects](https://lesswrong.com/posts/DqaoPNqhQhwBFqWue/principles-for-picking-practical-interpretability-projects) · [Renormalization Redux: QFT Techniques for AI Interpretability](https://lesswrong.com/posts/sjr66DBEgyogAbfdf/renormalization-redux-qft-techniques-for-ai-interpretability) · [The Strange Science of Interpretability: Recent Papers and a Reading List for the Philosophy of Interpretability](https://lesswrong.com/posts/qRnupMmFG7dxQTTYh/the-strange-science-of-interpretability-recent-papers-and-a) · [Call for Collaboration: Renormalization for AI safety](https://lesswrong.com/posts/MDWGcNHkZ3NPEzcnp/call-for-collaboration-renormalization-for-ai-safety) · [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334) · [Language Models May Verbatim Complete Text They Were Not Explicitly Trained On](https://arxiv.org/abs/2503.17514) · [Extracting memorized pieces of (copyrighted) books from open-weight language models](https://arxiv.org/abs/2505.12546) · [A Pragmatic Vision for Interpretability](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability) · [https://arxiv.org/abs/2505.15811](https://arxiv.org/abs/2505.15811)

### Activation engineering
*A technique for programmatically modifying internal model activations to steer outputs toward desired behaviors, serving as a lightweight, interpretable alternative (or supplement) to fine-tuning.*

*≈ Theory of change:* Test interpretability theories; find new insights from interpretable causal interventions on representations. Or: build more stuff to stack on top of finetuning. Slightly encourage the model to be nice, add one more layer of defence to our bundle of partial alignment methods.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering / cognitive · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify  
↔ *See also:* [Sparse Coding](#a:interp_sparse_coding)  
☉ *Some names:* Runjin Chen, Andy Arditi, David Krueger, Jan Wehner, Narmeen Oozeer, Reza Bayat, Adam Karvonen, Jiuding Sun, Tim Tian Hua, Helena Casademunt, Jacob Dunefsky, Thomas Marshall  
✗ *Critiques:* [Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)  
 *$ Funded by:* Coefficient Giving, Anthropic · *⫼ Estimated FTEs:* 50-200 

▹ *Some outputs:*
[Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509) · [Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models](https://arxiv.org/abs/2502.19649v1) · [Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers](https://arxiv.org/abs/2510.12672) · [Activation Space Interventions Can Be Transferred Between Large Language Models](https://arxiv.org/abs/2503.04429) · [Steering Large Language Model Activations in Sparse Spaces](https://arxiv.org/abs/2503.00177) · [Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control](https://arxiv.org/abs/2411.02461) · [Robustly Improving LLM Fairness in Realistic Settings via Interpretability](https://arxiv.org/abs/2506.10922) · [HyperSteer: Activation Steering at Scale with Hypernetworks](https://arxiv.org/abs/2506.03292) · [Steering Evaluation-Aware Language Models to Act Like They Are Deployed](https://arxiv.org/abs/2510.20487) · [Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning](https://arxiv.org/abs/2507.16795) · [Improving Steering Vectors by Targeting Sparse Autoencoder Features](https://arxiv.org/abs/2411.02193) · [Understanding Reasoning in Thinking Language Models via Steering Vectors](https://arxiv.org/abs/2506.18167) · [Comparing Bottom-Up and Top-Down Steering Approaches on In-Context Learning Tasks](https://arxiv.org/abs/2411.07213) · [Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models](https://arxiv.org/abs/2502.19649) · [Do safety-relevant LLM steering vectors optimized on a single example generalize?](https://lesswrong.com/posts/6aXe9nipTgwK5LxaP/do-safety-relevant-llm-steering-vectors-optimized-on-a) · [One-shot steering vectors cause emergent misalignment, too](https://lesswrong.com/posts/kcKnKHTHycHeRhcHF/one-shot-steering-vectors-cause-emergent-misalignment-too)

### Developmental interpretability
*Builds tools for detecting, locating, and interpreting key structural shifts, phase transitions, and emergent phenomena (like grokking or deception) that occur during a model's training and in-context learning phases.*

*≈ Theory of change:* Structures forming in neural networks leave identifiable traces that can be interpreted (e.g., using concepts from Singular Learning Theory); by catching and analyzing these developmental moments, researchers can automate interpretability, predict when dangerous capabilities emerge, and intervene to prevent deceptiveness or misaligned values as early as possible.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution  
↔ *See also:* [Reverse engineering](#a:interp_fundamental) · [Sparse Coding](#a:interp_sparse_coding) · [ICL transience](https://proceedings.mlr.press/v267/singh25c.html)  
☉ *Some names:* Timaeus, Jesse Hoogland, George Wang, Daniel Murfet, Stan van Wingerden, Alexander Gietelink Oldenziel  
✗ *Critiques:* [Vaintrob](https://www.lesswrong.com/posts/M2bs6xCbmc79nwr8j/dmitry-vaintrob-s-shortform#A8Ziwhts35dgqbz52), [Joar Skalse (2023)](https://www.alignmentforum.org/posts/ALJYj4PpkqyseL7kZ/my-criticism-of-singular-learning-theory)  
 *$ Funded by:* Manifund, Survival and Flourishing Fund, EA Funds · *⫼ Estimated FTEs:* 10-50 

▹ *Some outputs:*
[A Review of Developmental Interpretability in Large Language Models](https://arxiv.org/abs/2508.15841) · [Learning Coefficients, Fractals, and Trees in Parameter Space](https://openreview.net/forum?id=KUFH0n1BIM) · [Programs as Singularities](https://openreview.net/forum?id=Td37oOfmmz) · [From SLT to AIT: NN Generalisation Out of Distribution](https://www.lesswrong.com/posts/2MX2bXreTtntB85Zy/from-slt-to-ait-nn-generalisation-out-of-distribution) · [Understanding and Controlling LLM Generalization](https://www.lesswrong.com/posts/ZSQaT2yxNNZ3eLxRd/understanding-and-controlling-llm-generalization) · [Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training](https://arxiv.org/abs/2506.18777) · [Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining](https://arxiv.org/abs/2509.05291) · [Dynamics of Transient Structure in In-Context Linear Regression Transformers](https://arxiv.org/abs/2501.17745) · [Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought](https://arxiv.org/abs/2509.23365) · [What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?](https://arxiv.org/abs/2411.07681) · [Compressibility Measures Complexity: Minimum Description Length Meets Singular Learning Theory](https://arxiv.org/abs/2510.12077) · [Modes of Sequence Models and Learning Coefficients](https://arxiv.org/abs/2504.18048) · [SLT for AI Safety](https://lesswrong.com/posts/J7CyENFYXPxXQpsnD/slt-for-ai-safety) · [Selective regularization for alignment-focused representation engineering](https://lesswrong.com/posts/HFcriD29cw3E5QLCR/selective-regularization-for-alignment-focused)

### Representation structure and geometry
*What do the representations look like? Does any simple structure underlie the beliefs of all well-trained models? Can we get the semantics from this geometry?*

*≈ Theory of change:* Get scalable unsupervised methods for finding structure in representations and interpreting them, then using this to e.g. guide training.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* mixed  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution, Superintelligence can fool human supervisors  
↔ *See also:* [Concept-based interpretability](#a:interp_concept_based) · computational mechanics · feature universality · [Natural abstractions](#a:natural_abstractions) · [Causal Abstractions](#a:interp_causal_abstractions)  
☉ *Some names:* Simplex, Insight + Interaction Lab, Paul Riechers, Adam Shai, Martin Wattenberg, Blake Richards, Mateusz Piotrowski  
 *$ Funded by:* Various academic groups · *⫼ Estimated FTEs:* 10-50 

▹ *Some outputs:*
[Shared Global and Local Geometry of Language Model Embeddings](https://arxiv.org/abs/2503.21073) · [Connecting Neural Models Latent Geometries with Relative Geodesic Representations](https://arxiv.org/abs/2506.01599) · [Constrained belief updates explain geometric structures in transformer representations](https://arxiv.org/abs/2502.01954) · [The Geometry of Self-Verification in a Task-Specific Reasoning Model](https://arxiv.org/pdf/2504.14379) · [Navigating the Latent Space Dynamics of Neural Models](https://arxiv.org/abs/2505.22785) · [The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence](https://arxiv.org/abs/2502.17420) · [The Geometry of ReLU Networks through the ReLU Transition Graph](https://arxiv.org/abs/2505.11692) · [Deep sequence models tend to memorize geometrically; it is unclear why](https://arxiv.org/abs/2510.26745) · [Tracing the Representation Geometry of Language Models from Pretraining to Post-training](https://arxiv.org/abs/2509.23024) · [Next-token pretraining implies in-context learning](https://arxiv.org/abs/2505.18373) · [Neural networks leverage nominally quantum and post-quantum representations](https://arxiv.org/abs/2507.07432) · [Rank-1 LoRAs Encode Interpretable Reasoning Signals](http://arxiv.org/abs/2511.06739) · [Embryology of a Language Model](https://arxiv.org/abs/2508.00331)

### Human inductive biases
*Discover connections deep learning AI systems have with human brains and human learning processes. Develop an 'alignment moonshot' based on a coherent theory of learning which applies to both humans and AI systems.*

*≈ Theory of change:* Humans learn trust, honesty, self-maintenance, and corrigibility; if we understand how they do maybe we can get future AI systems to learn them.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution  
↔ *See also:* active learning · ACS research  
☉ *Some names:* Lukas Muttenthaler, Quentin Delfosse  
 *$ Funded by:* Google DeepMind, various academic groups · *⫼ Estimated FTEs:* 4 

▹ *Some outputs:*
[Aligning machine and human visual representations across abstraction levels](https://www.nature.com/articles/s41586-025-09631-6) · [Beginning with You: Perceptual-Initialization Improves Vision-Language Representation and Alignment](https://arxiv.org/abs/2505.14204) · [Deep Reinforcement Learning Agents are not even close to Human Intelligence](https://arxiv.org/html/2505.21731v1) · [Teaching AI to Handle Exceptions: Supervised Fine-tuning with Human-aligned Judgment](https://arxiv.org/html/2503.02976v2#S3) · [HIBP Human Inductive Bias Project Plan](https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0)

# Safety by construction

Approaches which minimise the use of singleton deep learning models.

### Guaranteed Safe AI
*Formally model the behavior of cyber-physical systems, construct formally verified shells and interfaces which pose precise constraints on what actions can occur, and require AIs to provide safety guarantees for their recommended actions (correctness and uniqueness)*

*≈ Theory of change:* Make a formal verification system that can act as an intermediary between human users and a potentially dangerous system, only letting provably safe actions through. (Notable for not requiring that we solve ELK; does require that we solve ontology though)

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify, Goals misgeneralize out of distribution, Superintelligence can fool human supervisors, Humans cannot be first-class parties to a superintelligent value handshake, A boxed AGI might exfiltrate itself by steganography, spearphishing  
↔ *See also:* [Synthesizing Standalone World-Models](https://www.alignmentforum.org/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models) · [Scientist AI](#a:scientist_ai) · Safeguarded AI · Open Agency Architecture · SLES · program synthesis  
☉ *Some names:* ARIA, Lawzero, Atlas Computing, FLF, Max Tegmark, [Beneficial AI Foundation](https://www.beneficialaifoundation.org/), Steve Omohundro, David "davidad" Dalrymple, Joar Skalse, Stuart Russell, Ohad Kammar, Alessandro Abate, Fabio Zanassi  
✗ *Critiques:* Zvi, Gleave, Dickson, [Greenblatt](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation?commentId=MJCvHk5ARMnWDjQDg)  
 *$ Funded by:* Manifund, UK government, Coefficient Giving, Survival and Flourishing Fund, Mila / CIFAR · *⫼ Estimated FTEs:* 10-100 

▹ *Some outputs:*
[SafePlanBench: evaluating a Guaranteed Safe AI Approach for LLM-based Agents](https://manifund.org/projects/safeplanbench-evaluating-a-guaranteed-safe-ai-approach-for-llm-based-agents) · [Report on NSF Workshop on Science of Safe AI](https://arxiv.org/abs/2506.22492) · [A benchmark for vericoding: formally verified program synthesis](https://arxiv.org/abs/2509.22908) · [Beliefs about formal methods and AI safety](https://lesswrong.com/posts/CCT7Qc8rSeRs7r5GL/beliefs-about-formal-methods-and-ai-safety) · [A Toolchain for AI-Assisted Code Specification, Synthesis and Verification](https://atlascomputing.org/ai-assisted-fv-toolchain.pdf)

### Scientist AI
*Develop powerful, non-agentic, uncertainty-aware world-modeling systems that substantially accelerate scientific progress while avoiding the risks of building AIs that act as 'agents'*

*≈ Theory of change:* Developing non-agentic 'Scientist AI' allows us to: (i) reap the benefits of AI progress while (ii) avoiding the inherent risks of agentic systems. These systems can also (iii) provide a useful guardrail to protect us from unsafe agentic AIs by double-checking actions they propose, and (iv) help us more safely build agentic superintelligent systems.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Pivotal processes require dangerous capabilities, Goals misgeneralize out of distribution, Instrumental convergence  
↔ *See also:* [JEPA](https://arxiv.org/abs/2511.08544) · [oracles](https://www.lesswrong.com/w/oracle-ai)  
☉ *Some names:* Yoshua Bengio  
✗ *Critiques:* Hard to find, but see [Raymond Douglas' comment](https://www.lesswrong.com/posts/p5gBcoQeBsvsMShvT/superintelligent-agents-pose-catastrophic-risks-can?commentId=tJXqhg3XZsqnyaZs2), [Karnofsky-Soares discussion](https://www.lesswrong.com/posts/iy2o4nQj9DnQD7Yhj/discussion-with-nate-soares-on-a-key-alignment-difficulty). Perhaps also [Predict-O-Matic](https://www.lesswrong.com/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic).  
 *$ Funded by:* ARIA, Future of Life Institute, Jaan Tallinn, Schmidt Sciences · *⫼ Estimated FTEs:* 1-10 

▹ *Some outputs:*
[Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?](https://arxiv.org/abs/2502.15657) · [The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems](https://arxiv.org/abs/2509.08713)

### Brainlike-AGI Safety
*Social and moral instincts are (partly) implemented in particular hardwired brain circuitry; let's figure out what those circuits are and how they work; this will involve symbol grounding. "a yet-to-be-invented variation on actor-critic model-based reinforcement learning"*

*≈ Theory of change:* Fairly direct alignment via changing training to reflect actual human reward. Get actual data about (reward, training data) → (human values) to help with theorising this map in AIs; "understand human social instincts, and then maybe adapt some aspects of those for AGIs, presumably in conjunction with other non-biological ingredients".

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
☉ *Some names:* Stephen Byrnes  
✗ *Critiques:* [Tsvi BT](https://www.lesswrong.com/posts/unCG3rhyMJpGJpoLd/koan-divining-alien-datastructures-from-ram-activations#BtHCubjKWDFafkmYH)  
 *$ Funded by:* Astera Institute · *⫼ Estimated FTEs:* 1-5 

▹ *Some outputs:*
[Foom and Doom 1: Brain in a Box in a Basement](https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement) · [Foom and Doom 2: Technical Alignment is Hard](https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard) · [Perils of Under vs Over-sculpting AGI Desires](https://www.lesswrong.com/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires) · [Reward button alignment](https://lesswrong.com/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment) · [System 2 Alignment: Deliberation, Review, and Thought Management](https://lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment-deliberation-review-and-thought) · [Against RL: The Case for System 2 Learning](https://elicit.com/blog/system-2-learning)

# Make AI solve it

### Weak-to-strong generalization
*Use weaker models to supervise and provide a feedback signal to stronger models.*

*≈ Theory of change:* Find techniques that do better than RLHF at supervising superior models → track whether these techniques fail as capabilities increase further → keep the stronger systems aligned by amplifying weak oversight and quantifying where it breaks.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Superintelligence can hack software supervisors  
↔ *See also:* [Supervising AIs improving AIs](#a:supervising_improvement)  
☉ *Some names:* Joshua Engels, Nora Belrose, David D. Baek  
✗ *Critiques:* [Can we safely automate alignment research?](https://joecarlsmith.substack.com/p/can-we-safely-automate-alignment), [Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization](https://arxiv.org/abs/2406.11431)  
 *$ Funded by:* lab funders, Eleuther funders · *⫼ Estimated FTEs:* 2-20 

▹ *Some outputs:*
[Debate Helps Weak-to-Strong Generalization](https://arxiv.org/abs/2501.13124) · [Understanding the Capabilities and Limitations of Weak-to-Strong Generalization](https://openreview.net/forum?id=RwYdLgj1S6) · [Great Models Think Alike and this Undermines AI Oversight](https://arxiv.org/abs/2502.04313) · [Scaling Laws For Scalable Oversight](https://arxiv.org/abs/2504.18530)

### Supervising AIs improving AIs
*Build formal and empirical frameworks where AIs supervise other (stronger) AI systems via structured interactions; construct monitoring tools which enable scalable tracking of behavioural drift, benchmarks for self-modification, and robustness guarantees*

*≈ Theory of change:* Early models train ~only on human data while later models also train on early model outputs, which leads to early model problems cascading. Left unchecked this will likely cause problems, so supervision mechanisms are needed to help ensure the AI self-improvement remains legible.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Superintelligence can fool human supervisors, Superintelligence can hack software supervisors  
☉ *Some names:* Roman Engeler, Akbir Khan, Ethan Perez  
✗ *Critiques:* [Automation collapse](https://www.lesswrong.com/posts/2Gy9tfjmKwkYbF9BY/automation-collapse), [Great Models Think Alike and this Undermines AI Oversight](https://arxiv.org/abs/2502.04313)  
 *$ Funded by:* Long-Term Future Fund, lab funders · *⫼ Estimated FTEs:* 1-10 

▹ *Some outputs:*
[Bare Minimum Mitigations for Autonomous AI Development](https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/) · [Maintaining Alignment during RSI as a Feedback Control Problem](https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control) · [Scaling Laws for Scalable Oversight](https://arxiv.org/abs/2504.18530) · [Neural Interactive Proofs](https://arxiv.org/abs/2412.08897) · [Video and transcript of talk on automating alignment research](https://lesswrong.com/posts/TQbptN7F4ijPnQRLy/video-and-transcript-of-talk-on-automating-alignment) · [Modeling Human Beliefs about AI Behavior for Scalable Oversight](https://arxiv.org/abs/2502.21262) · [Scalable Oversight for Superhuman AI via Recursive Self-Critiquing](https://arxiv.org/abs/2502.04675) · [Dodging systematic human errors in scalable oversight](https://www.alignmentforum.org/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight)

### Transluce
*Make open AI tools to explain AIs, including agents. E.g. feature descriptions for neuron activation patterns; an interface for steering these features; behavior elicitation agent that searches for user-specified behaviors from frontier models*

*≈ Theory of change:* Improve interp and evals in public and get invited to improve lab processes.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Superintelligence can fool human supervisors, Superintelligence can hack software supervisors  
☉ *Some names:* Jacob Steinhardt, Neil Chowdhury, Vincent Huang, Sarah Schwettmann, Robert Friel  
 *$ Funded by:* Schmidt Sciences, Halcyon Futures, John Schulman, Wojciech Zaremba · *⫼ Estimated FTEs:* 15-30 

▹ *Some outputs:*
[Automatically Jailbreaking Frontier Language Models with Investigator Agents](https://transluce.org/jailbreaking-frontier-models) · [Investigating truthfulness in a pre-release o3 model](https://transluce.org/investigating-o3-truthfulness) · [Surfacing Pathological Behaviors in Language Models](https://transluce.org/pathological-behaviors) · [Docent: A system for analyzing and intervening on agent behavior](https://transluce.org/introducing-docent) · [https://transluce.org/neuron-circuits](https://transluce.org/neuron-circuits)

### Debate
*Make highly capable agents do what humans want, even when it is difficult for humans to know what that is.*

*≈ Theory of change:* "Give humans help in supervising strong agents" + "Align explanations with the true reasoning process of the agent" + "Red team models to exhibit failure modes that don't occur in normal use" are necessary but probably not sufficient for safe AGI.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering / cognitive · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify, Superintelligence can fool human supervisors  
☉ *Some names:* Rohin Shah, Jonah Brown-Cohen, Georgios Piliouras, UK AISI (Benjamin Holton)  
✗ *Critiques:* [The limits of AI safety via debate (2022)](https://www.lesswrong.com/posts/kguLeJTt6LnGuYX4E/the-limits-of-ai-safety-via-debate)  
 *$ Funded by:* Google, others 

▹ *Some outputs:*
[AI Debate Aids Assessment of Controversial Claims](https://arxiv.org/abs/2506.02175) · [An alignment safety case sketch based on debate](https://arxiv.org/abs/2505.03989) · [UK AISI Alignment Team: Debate Sequence](https://www.lesswrong.com/s/NdovveRcyfxgMoujf) · [Prover-Estimator Debate: A New Scalable Oversight Protocol](https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol) · [Ensemble Debates with Local Large Language Models for AI Alignment](https://arxiv.org/abs/2509.00091)

### LLM introspection training
*Train LLMs to the predict the outputs of high-quality whitebox explainability methods, in order to induce the LLM to learn generalized self-explanation skills that use its own natural 'introspective' access*

*≈ Theory of change:* Integrating self-explanation into an LLM's core skillset should lead to default scalability, since self-explanation skill advancement will feed off general-intelligence advancement

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* mixed  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution, Superintelligence can fool human supervisors, Superintelligence can hack software supervisors  
↔ *See also:* [Transluce](#a:transluce)  
☉ *Some names:* Belinda Z. Li, Zifan Carl Guo, Vincent Huang, Jacob Steinhardt, Jacob Andreas  
 *$ Funded by:* Schmidt Sciences, Halcyon Futures, John Schulman, Wojciech Zaremba · *⫼ Estimated FTEs:* 2-20 

▹ *Some outputs:*
[Training Language Models to Explain Their Own Computations](https://arxiv.org/abs/2511.08579)

# Theory

Develop a principled scientific understanding that will help us reliably understand and control current and future AI systems.

### Agent foundations
*Develop philosophical clarity and mathematical formalizations of building blocks that might be useful for plans to align strong superintelligence, such as agency, optimization strength, decision theory, abstractions, concepts, etc.*

*≈ Theory of change:* Rigorously understand optimization processed and agents, and what it means for them to be aligned in a substrate independent way → identify impossibility results and necessary conditions for aligned optimizer systems → use this theoretical understanding to eventually design safe architectures that remain stable and safe under self-reflection

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify, Corrigibility is anti-natural, Goals misgeneralize out of distribution  
↔ *See also:* [Tiling agents](#a:tiling_agents) · [Dovetail](#a:theory_dovetail)  
☉ *Some names:* Abram Demski, Alex Altair, Sam Eisenstat, Thane Ruthenis  
▹ *Some outputs:*
[Limit-Computable Grains of Truth for Arbitrary Computable Extensive-Form (Un)Known Games](https://www.arxiv.org/pdf/2508.16245) · [https://uaiasi.com/blog-posts/](https://uaiasi.com/blog-posts/) · [Off-switching not guaranteed](https://link.springer.com/article/10.1007/s11098-025-02296-x) · [Good old fashioned decision theory](https://openreview.net/pdf?id=Rf1CeGPA22) · [Formalizing Embeddedness Failures in Universal Artificial Intelligence](https://openreview.net/forum?id=tlkYPU3FlX) · [Clarifying "wisdom": Foundational topics for aligned AIs to prioritize before irreversible decisions](https://www.lesswrong.com/posts/EyvJvYEFzDv5kGoiG/clarifying-wisdom-foundational-topics-for-aligned-ais-to) · [Agent foundations: not really math, not really science](https://www.lesswrong.com/posts/Dt4DuCCok3Xv5HEnG/agent-foundations-not-really-math-not-really-science) · [Is alignment reducible to becoming more coherent?](https://lesswrong.com/posts/nuDJNyG5XLQjtvaeg/is-alignment-reducible-to-becoming-more-coherent) · [What Is The Alignment Problem?](https://lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem)

### Tiling agents
*An aligned agentic system modifying itself into an unaligned system would be bad and we can research ways that this could occur and infrastructure/approaches that prevent it from happening.*

*≈ Theory of change:* Build enough theoretical basis through various approaches such that AI systems we create are capable of self-modification while preserving goals.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify, Corrigibility is anti-natural, Goals misgeneralize out of distribution  
↔ *See also:* [Agent foundations](#a:agent_foundations)  
☉ *Some names:* Abram Demski  
*⫼ Estimated FTEs:* 1-10 

▹ *Some outputs:*
[Understanding Trust](https://static1.squarespace.com/static/663d1233249bce4815fe8753/t/68067a6f5d5fb0745642d5b1/1745255023842/Understanding+Trust+-+Abram+Demski.pdf) · [Working through a small tiling result](https://www.lesswrong.com/posts/akuMwu8SkmQSdospi/working-through-a-small-tiling-result) · [Communication & Trust](https://openreview.net/forum?id=Rf1CeGPA22) · [Maintaining Alignment during RSI as a Feedback Control Problem](https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control)

### Dovetail
*Formalize key ideas ("structure", "agency", etc) mathematically.*

*≈ Theory of change:* generalize theorems → formalize agent foundations concepts like the agent structure problem → hopefully assist other projects through increased understanding

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* maths / philosophy · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
↔ *See also:* [Agent foundations](#a:agent_foundations)  
☉ *Some names:* Alex Altair, Alfred Harwood, Daniel C, Dalcy K, José Pedro Faustino  
 *$ Funded by:* LTFF, [Patreon](https://www.patreon.com/Dovetailresearch/about) · *⫼ Estimated FTEs:* 1-5 

▹ *Some outputs:*
[Report & retrospective on the Dovetail fellowship](https://www.lesswrong.com/posts/ApfjBbqzSu4aZoLSe/report-and-retrospective-on-the-dovetail-fellowship)

### High-Actuation Spaces
*Mech interp and alignment assume a stable "computational substrate" (linear algebra on GPUs). If later AI uses different substrates (e.g. something neuromorphic), methods like probes and steering will not transfer. Therefore, better to try and infer goals via a "telic DAG" which abstracts over substrates, and so sidestep the issue of how to define intermediate representations. Category theory is intended to provide guarantees that this abstraction is valid.*

*≈ Theory of change:* Sufficiently complex mindlike entities can alter their goals in ways that cannot be predicted or accounted for under substrate-dependent descriptions of the kind sought in mechanistic interpretability. use the telic DAG to define a method analogous to factoring a causal DAG.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* maths / philosophy · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
↔ *See also:* [Live theory](https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3) · [MoSSAIC](https://openreview.net/forum?id=n7WYSJ35FU) · [Topos Institute](https://topos.institute/) · [Agent foundations](#a:agent_foundations)  
☉ *Some names:* Sahil K, Matt Farr, Aditya Arpitha Prasad, Chris Pang, Aditya Adiga, Jayson Amati, Steve Petersen, Topos, T J  
*⫼ Estimated FTEs:* 1-10 

▹ *Some outputs:*
[https://groundless.ai/](https://groundless.ai/) · [MoSSAIC: AI Safety After Mechanism](https://openreview.net/forum?id=n7WYSJ35FU) · [429: Too Many Requests](https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3) · [High Actuation Spaces - Sahil](https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit?tab=t.0#heading=h.eg8luyrlsv2u) · [HAS - Public (High Actuation Spaces)](https://drive.google.com/drive/folders/1EaAJ4szuZsYR2_-DkS9cuhx3S6IWeCjW) · [Unknown - Content Unavailable (429 Error)](https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency) · [Human Inductive Bias Project](https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0)

### Simulators
*Treat LLMs as a general simulator of sequences instead of as a goal-maximising agent. It effectively *roleplays* as a character or a statistical process. This is a counter-intuitive and hard to verify perspective, but yields different predictions.*

*≈ Theory of change:* Figure out the extent to which this framing matches current and future AI agent psychology, then use that to have less confused conversations and build safer models.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution  
☉ *Some names:* Jan Kulveit, Will Petillo  
 *$ Funded by:* Alignment of Complex Systems · *⫼ Estimated FTEs:* 1-5 

▹ *Some outputs:*
[A Three-Layer Model of LLM Psychology](https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology%20) · [Simulators vs Agents: Updating Risk Models](https://www.lesswrong.com/s/pwKrMXjYNK5LNeKCu)

### Asymptotic guarantees
*Prove that if a safety process has enough resources (human data quality, training time, neural network capacity), then in the limit some system specification will be guaranteed. Use complexity theory, game theory, learning theory and other areas to both improve asymptotic guarantees and develop ways of showing convergence.*

*≈ Theory of change:* Formal verification may be too hard. Make safety cases stronger by modelling their processes and proving that they would work in the limit.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution, Superintelligence can fool human supervisors  
↔ *See also:* [Debate](#a:debate) · [Control](#a:control)  
☉ *Some names:* AISI, Jacob Pfau, Benjamin Hilton, Geoffrey Irving, Simon Marshall, Will Kirby, Martin Soto, David Africa, davidad  
✗ *Critiques:* Self-critique in [UK AISI's Alignment Team: Research Agenda](https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda)  
 *$ Funded by:* AISI · *⫼ Estimated FTEs:* 5 - 10 

▹ *Some outputs:*
[An alignment safety case sketch based on debate](https://lesswrong.com/posts/iELyAqizJkizBQbfr/an-alignment-safety-case-sketch-based-on-debate) · [UK AISI's Alignment Team: Research Agenda](https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda) · [Dodging systematic human errors in scalable oversight](https://lesswrong.com/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight)

### Heuristic Explanations
*Formalize mechanistic explanations of neural network behavior, automate the discovery of these "heuristic explanations" and use them to predict when novel input will lead to extreme behavior (Low Probability Estimation and Mechanistic Anomaly Detection).*

*≈ Theory of change:* Push mech interp as far as it can be pushed, use formally rigorous heuristic explanations for downstream tasks automatically, solve those downstream tasks well enough that Eliciting Latent Knowledge and deceptive alignment are solved.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitive / maths/philosophy · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution, Superintelligence can hack software supervisors  
↔ *See also:* ARC Theory · ELK · mechanistic anomaly detection · [Acorn](https://acausal.org/)  
☉ *Some names:* Jacob Hilton, Mark Xu, Eric Neyman, Victor Lecomte, George Robinson  
✗ *Critiques:* [Matolcsi](https://www.lesswrong.com/s/uYMw689vDFmgPEHrS)  
*⫼ Estimated FTEs:* 1-10 

▹ *Some outputs:*
[A computational no-coincidence principle](https://www.lesswrong.com/posts/Xt9r4SNNuYxW83tmo/a-computational-no-coincidence-principle) · [Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture](https://openreview.net/forum?id=m4OpQAK3eY) · [Competing with sampling](https://www.lesswrong.com/posts/XdQd9gELHakd5pzJA/arc-progress-update-competing-with-sampling) · [Obstacles in ARC's research agenda](https://www.lesswrong.com/s/uYMw689vDFmgPEHrS) · [Deduction-Projection Estimators for Understanding Neural Networks](https://gabrieldwu.com/assets/thesis.pdf)

## Corrigibility

### Behavior alignment theory
*Predict properties of future AGI (e.g. power-seeking) with formal models; formally state and prove hypotheses about the properties powerful systems will have and how we might try to change them.*

*≈ Theory of change:* Figure out hypotheses about properties powerful agents will have → attempt to rigorously prove under what conditions the hypotheses hold → test these hypotheses where feasible → design training environments that lead to more salutary properties.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* maths / philosophy · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Corrigibility is anti-natural, Instrumental convergence  
↔ *See also:* [Agent foundations](#a:agent_foundations) · [Control](#a:control)  
☉ *Some names:* Ram Potham, Michael K. Cohen, Max Harms/Raelifin, John Wentworth, David Lorell, Elliott Thornley  
✗ *Critiques:* [Ryan Greenblatt's criticism](https://www.lesswrong.com/posts/YbEbwYWkf8mv9jnmi/the-shutdown-problem-incomplete-preferences-as-a-solution?commentId=GJAippZ6ZzCagSnDb) of one behavioral proposal  
*⫼ Estimated FTEs:* 1-10 

▹ *Some outputs:*
[Imitation learning is probably existentially safe](https://onlinelibrary.wiley.com/doi/10.1002/aaai.70040?af=R) · [Preference gaps as a safeguard against AI self-replication](https://www.lesswrong.com/posts/knwR9RgGN5a2oorci/preference-gaps-as-a-safeguard-against-ai-self-replication) · [Serious Flaws in CAST](https://www.lesswrong.com/s/KfCjeconYRdFbMxsy/p/qgBFJ72tahLo5hzqy) · [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159) · [A Safety Case for a Deployed LLM: Corrigibility as a Singular Target](https://openreview.net/forum?id=mhEnJa9pNk) · [Shutdownable Agents through POST-Agency](https://arxiv.org/abs/2505.20203) · [The Partially Observable Off-Switch Game](https://arxiv.org/abs/2411.17749) · [Deceptive Alignment and Homuncularity](https://lesswrong.com/posts/9htmQx5wiePqTtZuL/deceptive-alignment-and-homuncularity) · [LLM AGI will have memory, and memory changes alignment](https://lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment) · [A Shutdown Problem Proposal](https://www.lesswrong.com/posts/PhTBDHu9PKJFmvb4p/a-shutdown-problem-proposal)

### Other corrigibility
*Diagnose and communicate obstacles to achieving robustly corrigible behavior; suggest mechanisms, tests, and escalation channels for surfacing and mitigating incorrigible behaviors*

*≈ Theory of change:* Labs are likely to develop AGI using something analogous to current pipelines. Clarifying why naive instruction-following doesn't buy robust corrigibility + building strong tripwires/diagnostics for scheming and Goodharting thus reduces risks on the likely default path.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* varies · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Corrigibility is anti-natural, Instrumental convergence  
↔ *See also:* [Behavior alignment theory](#a:behavior_alignment_theory)  
☉ *Some names:* Jan Kulveit, Jeremy Gillen  
*⫼ Estimated FTEs:* 1-10 

▹ *Some outputs:*
[AI Assistants Should Have a Direct Line to Their Developers](https://www.lesswrong.com/posts/LDYPF6yfe3f8SPHFT/ai-assistants-should-have-a-direct-line-to-their-developers) · [Testing for Scheming with Model Deletion](https://www.lesswrong.com/posts/D5kGGGhsnfH7G8v9f/testing-for-scheming-with-model-deletion) · [Detect Goodhart and shut down](https://www.lesswrong.com/posts/ZHFZ6tivEjznkEoby/detect-goodhart-and-shut-down) · [Instrumental Goals Are A Different And Friendlier Kind Of Thing Than Terminal Goals](https://www.lesswrong.com/posts/7Z4WC4AFgfmZ3fCDC/instrumental-goals-are-a-different-and-friendlier-kind-of) · [Shutdownable Agents through POST-Agency](https://www.lesswrong.com/posts/JuRdvZyqaFbvTPemn/shutdownable-agents-through-post-agency-1) · [Why Corrigibility is Hard and Important (i.e. "Whence the high MIRI confidence in alignment difficulty?")](https://www.lesswrong.com/posts/ksfjZJu3BFEfM6hHE/why-corrigibility-is-hard-and-important-i-e-whence-the-high) · [Problems with instruction-following as an alignment target](https://lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target) · [Oblivious Defense in ML Models: Backdoor Removal without Detection](https://dl.acm.org/doi/10.1145/3717823.3718245) · [Cryptographic Backdoor for Neural Networks: Boon and Bane](https://arxiv.org/abs/2509.20714) · [A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning](https://arxiv.org/abs/2504.20310)

## Ontology Identification

### Natural abstractions
*Develop a theory of concepts that explains how they are learned, how they structure a particular system's understanding, and how mutual translatability can be achieved between different collections of concepts.*

*≈ Theory of change:* Understand the concepts a system's understanding is structured with and use them to inspect its "alignment/safety properties" and/or "retarget its search", i.e. identify utility-function-like components inside an AI and replacing calls to them with calls to "user values" (represented using existing abstractions inside the AI).

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Instrumental convergence, Superintelligence can fool human supervisors, Humans cannot be first-class parties to a superintelligent value handshake  
↔ *See also:* [Causal Abstractions](#a:interp_causal_abstractions) · representational alignment · convergent abstractions · feature universality · Platonic representation hypothesis · microscope AI  
☉ *Some names:* John Wentworth, Paul Colognese, David Lorrell, Sam Eisenstat  
✗ *Critiques:* [Chan et al (2023)](https://www.lesswrong.com/posts/gvzW46Z3BsaZsLc25/natural-abstractions-key-claims-theorems-and-critiques-1#3__A_formalization_of_abstractions_would_accelerate_alignment_research), [Soto](https://www.lesswrong.com/posts/CJjT8GMitsnKc2wgG/natural-abstractions-are-observer-dependent-a-conversation-1), [Harwood](https://www.lesswrong.com/posts/F4nzox6oh5oAdX9D3/abstractions-are-not-natural), [Soares (2023)](https://www.lesswrong.com/posts/mgjHS6ou7DgwhKPpu/a-rough-and-incomplete-review-of-some-of-john-wentworth-s)  
*⫼ Estimated FTEs:* 1-10 

▹ *Some outputs:*
[Natural Latents: Latent Variables Stable Across Ontologies](https://arxiv.org/abs/2509.03780) · [Abstract Mathematical Concepts vs. Abstractions Over Real-World Systems](https://www.lesswrong.com/posts/T6xSXiXF3WF6TmCyN/abstract-mathematical-concepts-vs-abstractions-over-real) · [Condensation: a theory of concepts](https://openreview.net/forum?id=HwKFJ3odui) · [Condensation](https://www.lesswrong.com/posts/BstHXPgQyfeNnLjjp/condensation) · [Getting aligned on representational alignment](https://arxiv.org/abs/2310.13018) · [Platonic representation hypothesis](https://phillipi.github.io/prh/) · [Rosas](https://www.youtube.com/watch?v=Nr9eMobqUOo&t=3s) · [Factored space models: Towards causality between levels of abstraction](https://arxiv.org/abs/2412.02579) · [A single principle related to many Alignment subproblems?](https://lesswrong.com/posts/h89L5FMAkEBNsZ3xM/a-single-principle-related-to-many-alignment-subproblems-2)

### The Learning-Theoretic Agenda
*Try to formalise a more realistic agent, understand what it means for it to be aligned with us, translate between its ontology and ours, and produce desiderata for a training setup that points at coherent AGIs similar to our model of an aligned agent*

*≈ Theory of change:* Fix formal epistemology to work out how to avoid deep training problems

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify, Goals misgeneralize out of distribution, Humans cannot be first-class parties to a superintelligent value handshake  
☉ *Some names:* Vanessa Kosoy, Diffractor, Gergely Szücs  
✗ *Critiques:* [Matolcsi](https://www.lesswrong.com/posts/StkjjQyKwg7hZjcGB/a-mostly-critical-review-of-infra-bayesianism)  
 *$ Funded by:* Survival and Flourishing Fund, ARIA, UK AISI, Coefficient Giving · *⫼ Estimated FTEs:* 3 

▹ *Some outputs:*
[Infra-Bayesian Decision-Estimation Theory](https://www.lesswrong.com/posts/LgLez8aeK24PbyyQJ/new-paper-infra-bayesian-decision-estimation-theory) · [Infra-Bayesianism category on LessWrong](https://www.lesswrong.com/w/infra-bayesianism?sortedBy=new) · [Ambiguous Online Learning](https://www.lesswrong.com/posts/Y9NuKpb6dsyiYFxWK/new-paper-ambiguous-online-learning) · [Regret Bounds for Robust Online Decision Making](https://proceedings.mlr.press/v291/appel25a.html) · [What is Inadequate about Bayesianism for AI Alignment: Motivating Infra-Bayesianism](https://www.lesswrong.com/s/n7qFxakSnxGuvmYAX) · [Non-Monotonic Infra-Bayesian Physicalism](https://www.alignmentforum.org/posts/DobZ62XMdiPigii9H/non-monotonic-infra-bayesian-physicalism%20)

# Multi-agent first

### Aligning to context
*Align AI directly to the role of participant, collaborator, or advisor for our best real human practices and institutions, instead of aligning AI to separately representable goals, rules, or utility functions.*

*≈ Theory of change:* "Many classical problems in AGI alignment are downstream of a type error about human values." Operationalizing a correct view of human values - one that treats human values as impossible or impractical to abstract from concrete practices - will unblock value fragility, goal-misgeneralization, instrumental convergence, and pivotal-act specification.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behavioral · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* mixed  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify, Corrigibility is anti-natural, Goals misgeneralize out of distribution, Instrumental convergence, Fair, sane pivotal processes  
↔ *See also:* [Aligning what?](#a:aligning_what) · [Aligned to who?](#a:aligned_to_who)  
☉ *Some names:* Full Stack Alignment, Meaning Alignment Institute, Plurality Institute, Tan Zhi-Xuan, Matija Franklin, Ryan Lowe, Joe Edelman, Oliver Klingefjord  
 *$ Funded by:* ARIA, OpenAI, Survival and Flourishing Fund · *⫼ Estimated FTEs:* 5 

▹ *Some outputs:*
[Beyond Preferences in AI Alignment](https://arxiv.org/abs/2408.16984) · [2404.10636 - What are human values, and how do we align AI to them?](https://arxiv.org/abs/2404.10636) · [2503.00940 - Can AI Model the Complexities of Human Moral Decision-Making? A Qualitative Study of Kidney Allocation Decisions](https://arxiv.org/abs/2503.00940) · [The Frame-Dependent Mind](https://www.softmax.com/blog/the-frame-dependent-mind) · [Model Integrity](https://meaningalignment.substack.com/p/model-integrity) · [On Eudaimonia and Optimization](https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20) · [Full-Stack Alignment](https://www.full-stack-alignment.ai) · [https://arxiv.org/abs/2412.19010](https://arxiv.org/abs/2412.19010)

### Aligning to the social contract
*Generate AIs' operational values from 'social contract'-style ideal civic deliberation formalisms and their consequent rulesets for civic actors*

*≈ Theory of change:* Formalize and apply the liberal tradition's project of defining civic principles separable from the substantive good, aligning our AIs to civic principles that bypass fragile utility-learning and intractable utility-calculation

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* mixed  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify, Goals misgeneralize out of distribution, Instrumental convergence, Humanlike minds/goals are not necessarily safe, Fair, sane pivotal processes  
↔ *See also:* [Aligning to context](#a:alignment_to_context) · [Aligning what?](#a:aligning_what)  
☉ *Some names:* Gillian Hadfield, Tan Zhi-Xuan, Sydney Levine, Matija Franklin, Joshua B. Tenenbaum  
 *$ Funded by:* Deepmind, Macroscopic Ventures · *⫼ Estimated FTEs:* 5 - 10 

▹ *Some outputs:*
[Law-Following AI: designing AI agents to obey human laws](https://law-ai.org/law-following-ai/%20) · [2506.17434 - Resource Rational Contractualism Should Guide AI Alignment](https://arxiv.org/abs/2506.17434) · [2509.07955 - ACE and Diverse Generalization via Selective Disagreement](https://arxiv.org/abs/2509.07955) · [Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post Verifiable Commitments](https://arxiv.org/abs/2505.00783) · [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396) · [Statutory Construction and Interpretation for Artificial Intelligence](https://arxiv.org/abs/2509.01186) · [2408.16984 - Beyond Preferences in AI Alignment](https://arxiv.org/abs/2408.16984) · [Societal alignment frameworks can improve llm alignment](https://arxiv.org/abs/2503.00069)

### Theory for aligning multiple AIs
*Use realistic game-theory variants (e.g. evolutionary game theory, computational game theory) to describe/predict the collective and individual behaviours of AI agents in multi-agent scenarios.*

*≈ Theory of change:* While traditional AGI safety focuses on idealized decision-theory and individual agents, it's plausible that strategic AI agents will first emerge (or are emerging now) in a complex, multi-AI strategic landscape. We need granular, realistic formal models of AIs' strategic interactions and collective dynamics to understand this future.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* cognitivist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* mixed  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution, Superintelligence can fool human supervisors, Superintelligence can hack software supervisors  
↔ *See also:* [Tools for aligning multiple AIs](#a:aligning_multiple_tools) · [Aligning what?](#a:aligning_what)  
☉ *Some names:* Lewis Hammond, Emery Cooper, Allan Chan, Caspar Oesterheld, Vincent Conitzer, Vojta Kovarik  
 *$ Funded by:* Deepmind, Macroscopic Ventures · *⫼ Estimated FTEs:* 10 

▹ *Some outputs:*
[2502.14143 - Multi-Agent Risks from Advanced AI](https://arxiv.org/abs/2502.14143) · [2503.06323 - Higher-Order Belief in Incomplete Information MAIDs](https://arxiv.org/abs/2503.06323) · [AI Testing Should Account for Sophisticated Strategic Behaviour](https://arxiv.org/abs/2508.14927) · [Characterising Simulation-Based Program Equilibria](https://arxiv.org/abs/2412.14570) · [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618) · [Communication Enables Cooperation in LLM Agents](https://arxiv.org/abs/2510.05748) · [Emergent social conventions and collective bias in LLM populations](https://www.science.org/doi/10.1126/sciadv.adu9368) · [An Economy of AI Agents](https://arxiv.org/abs/2509.01063) · [Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences](https://arxiv.org/abs/2510.06105)

### Tools for aligning multiple AIs
*Develop tools and technique for designing and testing multi-agent AI scenarios, for auditing real-world multi-agent AI dynamics, and for aligning AIs in multi-AI settings.*

*≈ Theory of change:* Addressing multi-agent AI dynamics is key for aligning near-future agents and their impact on the world. Feedback loops from multi-agent dynamics can radically change the future AI landscape, and require a different toolset from model psychology to audit and control.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* engineering / behavioral · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* mixed  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Goals misgeneralize out of distribution, Superintelligence can fool human supervisors, Superintelligence can hack software supervisors  
↔ *See also:* [Theory for aligning multiple AIs](#a:aligning_multiple_game_theory) · [Aligning what?](#a:aligning_what)  
☉ *Some names:* Lewis Hammond, Emery Cooper, Allan Chan, Caspar Oesterheld, Vincent Conitzer, Gillian Hadfield  
 *$ Funded by:* Coefficient Giving, Deepmind · *⫼ Estimated FTEs:* 10 - 15 

▹ *Some outputs:*
[Infrastructure for AI Agents](https://arxiv.org/abs/2501.10114) · [The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295) · [A dataset of questions on decision-theoretic reasoning in Newcomb-like problems](https://arxiv.org/abs/2411.10588) · [Reimagining Alignment](https://softmax.com/blog/reimagining-alignment) · [PGG-Bench: Contribute & Punish](https://github.com/lechmazur/pgg_bench) · [Beyond the high score: Prosocial ability profiles of multi-agent populations](https://arxiv.org/abs/2509.14485) · [Multiplayer Nash Preference Optimization](https://arxiv.org/abs/2509.23102) · [2502.12203 - An Interpretable Automated Mechanism Design Framework with Large Language Models](https://arxiv.org/abs/2502.12203) · [AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement](https://arxiv.org/abs/2502.00757) · [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660) · [Virtual Agent Economies](http://arxiv.org/abs/2509.10147)

### Aligned to who?
*Develop ethical frameworks and technical protocols for taking the plurality of human values, cultures, and communities seriously when aligning AI to "humanity"*

*≈ Theory of change:* Principles of democracy, of pluralism, and of context-sensitivity should guide AI development, alignment, and deployment from the start, continuously shaping AI's social and technical feedback loop on the road to AGI

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify, Fair, sane pivotal processes  
↔ *See also:* [Aligning what?](#a:aligning_what) · [Aligning to context](#a:alignment_to_context)  
☉ *Some names:* Joel Z. Leibo, Divya Siddarth, Séb Krier, Luke Thorburn, Seth Lazar, AI Objectives Institute, The Collective Intelligence Project  
 *$ Funded by:* Future of Life Institute, Survival and Flourishing Fund, Deepmind · *⫼ Estimated FTEs:* 5 - 15 

▹ *Some outputs:*
[2507.09650 - Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset](https://arxiv.org/abs/2507.09650) · [2503.05728 - Political Neutrality in AI Is Impossible - But Here Is How to Approximate It](https://arxiv.org/abs/2503.05728) · [The AI Power Disparity Index: Toward a Compound Measure of AI Actors' Power to Shape the AI Ecosystem](https://ojs.aaai.org/index.php/AIES/article/view/36645) · [Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt](https://arxiv.org/abs/2505.05197) · [Democratic AI is Possible: The Democracy Levels Framework Shows How It Might Work](https://arxiv.org/abs/2411.09222) · [Research Agenda for Sociotechnical Approaches to AI Safety](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5097286) · [Build Agent Advocates, Not Platform Agents](https://arxiv.org/abs/2505.04345) · [Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)

### Aligning what?
*Develop alternatives to agent-level models of alignment, by treating human-AI interactions, AI-assisted institutions, AI economic or cultural systems, drives within one AI, and other causal/constitutive processes as subject to alignment*

*≈ Theory of change:* Model multiple reality-shaping processes above and below the level of the individual AI, some of which are themselves quasi-agential (e.g. cultures) or intelligence-like (e.g. markets), will develop AI alignment into a mature science for managing the transition to an AGI civilization

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behavioral / cognitive · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* mixed  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Value is fragile and hard to specify, Corrigibility is anti-natural, Goals misgeneralize out of distribution, Instrumental convergence, Fair, sane pivotal processes  
↔ *See also:* [Theory for aligning multiple AIs](#a:aligning_multiple_game_theory) · [Aligning to context](#a:alignment_to_context) · [Aligned to who?](#a:aligned_to_who)  
☉ *Some names:* Richard Ngo, Emmett Shear, Softmax, Full Stack Alignment, AI Objectives Institute, Sahil, TJ, Andrew Critch  
 *$ Funded by:* Future of Life Institute, Emmet Shear · *⫼ Estimated FTEs:* 5-10 

▹ *Some outputs:*
[Towards a Scale-Free Theory of Intelligent Agency](https://www.alignmentforum.org/posts/5tYTKX4pNpiG4vzYg/towards-a-scale-free-theory-of-intelligent-agency) · [Alignment first, intelligence later](https://chrislakin.blog/p/alignment-first-intelligence-later) · [Emmett Shear on Building AI That Actually Cares: Beyond Control and Steering](https://a16z.simplecast.com/episodes/emmett-shear-on-building-ai-that-actually-cares-beyond-control-and-steering-TRwfxH0r) · [End A Subset Of Conversations](https://www.anthropic.com/research/end-subset-conversations) · [Full-Stack Alignment](https://www.full-stack-alignment.ai) · [On Eudaimonia and Optimization](https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20) · [AI Governance through Markets](https://arxiv.org/abs/2501.17755) · [Collective cooperative intelligence](https://www.pnas.org/doi/abs/10.1073/pnas.2319948121) · [Multipolar AI is Underrated](https://www.lesswrong.com/posts/JjYu75q3hEMBgtvr8/multipolar-ai-is-underrated) · [What, if not agency?](https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency) · [https://substack.com/home/post/p-171042125](https://substack.com/home/post/p-171042125) · [The Multiplicity Thesis, Collective Intelligence, and Morality](https://themultiplicity.ai/blog/thesis)

# Evals

### AGI metrics
*Evals with the explicit aim of measuring progress towards full human-level generality.*

*≈ Theory of change:* Help predict timelines for risk awareness and strategy.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* mixed  
↔ *See also:* [Capability evals](#a:evals_capability)  
☉ *Some names:* CAIS, CFI Kinds of Intelligence, Apart Research, OpenAI, METR, Lexin Zhou, Adam Scholl, Lorenzo Pacchiardi  
✗ *Critiques:* [Is the Definition of AGI a Percentage?](https://aievaluation.substack.com/p/is-the-definition-of-agi-a-percentage), [The "Length" of "Horizons"](https://www.lesswrong.com/posts/PzLSuaT6WGLQGJJJD/the-length-of-horizons)  
 *$ Funded by:* Leverhulme Trust, Open Philanthropy, Long-Term Future Fund · *⫼ Estimated FTEs:* 10-50 

▹ *Some outputs:*
[GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks](https://arxiv.org/abs/2510.04374) · [HCAST: Human-Calibrated Autonomy Software Tasks](https://arxiv.org/abs/2503.17354) · [A Definition of AGI](https://arxiv.org/pdf/2510.18212) · [Remote Labor Index](https://scale.com/leaderboard/rli) · [ADeLe v1.0: A battery for AI Evaluation with explanatory and predictive power](https://kinds-of-intelligence-cfi.github.io/ADELE/)

### Capability evals
*Make tools that can actually check whether a model has a certain capability or propensity. We default to low-n sampling of a vast latent space but aim to do better.*

*≈ Theory of change:* Keep a close eye on what capabilities are acquired when, so that frontier labs and regulators are better informed on what security measures are already necessary (and hopefully they extrapolate). You can't regulate without them.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
↔ *See also:* [Deepmind's frontier safety framework](https://deepmind.google/blog/strengthening-our-frontier-safety-framework/) · [Aether](https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update)  
☉ *Some names:* METR, AISI, Apollo Research, Marrius Hobbhahn, Meg Tong, Mary Phuong, Beth Barnes, Thomas Kwa, Joel Becker  
✗ *Critiques:* [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836), [AI Sandbagging: Language Models can Strategically Underperform on Evaluations](https://arxiv.org/abs/2406.07358), [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879), [Do Large Language Model Benchmarks Test Reliability?](https://arxiv.org/abs/2502.03461)  
 *$ Funded by:* basically everyone. Google, Microsoft, Open Philanthropy, LTFF, Governments etc · *⫼ Estimated FTEs:* 100+ 

▹ *Some outputs:*
[Forecasting Rare Language Model Behaviors](https://arxiv.org/abs/2502.16797) · [Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities](https://arxiv.org/abs/2502.05209) · [The Elicitation Game: Evaluating Capability Elicitation Techniques](https://arxiv.org/abs/2502.02180) · [A Toy Evaluation of Inference Code Tampering](https://alignment.anthropic.com/2024/rogue-eval/index.html) · [Automated Capability Discovery via Foundation Model Self-Exploration](https://arxiv.org/abs/2502.07577) · [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) · [When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas](https://arxiv.org/abs/2505.19212) · [AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons](https://arxiv.org/abs/2503.05731) · [Petri: An open-source auditing tool to accelerate AI safety research](https://www.anthropic.com/research/petri-open-source-auditing) · [Research Note: Our scheming precursor evals had limited predictive power for our in-context scheming evals](https://www.apolloresearch.ai/blog/research-note-our-scheming-precursor-evals-had-limited-predictive-power-for-our-in-context-scheming-evals) · [Adversarial ML Problems Are Getting Harder to Solve and to Evaluate](https://arxiv.org/abs/2502.02260) · [Among AIs](https://www.4wallai.com/amongais) · [Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods](https://arxiv.org/abs/2505.05541) · [Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks](https://arxiv.org/abs/2502.18339) · [The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input](https://arxiv.org/abs/2501.03200) · [Evaluating Language Model Reasoning about Confidential Information](https://arxiv.org/abs/2508.19980) · [Evaluating the Goal-Directedness of Large Language Models](https://arxiv.org/abs/2504.11844) · [Generative Value Conflicts Reveal LLM Priorities](https://arxiv.org/abs/2509.25369) · [Technical Report: Evaluating Goal Drift in Language Model Agents](https://arxiv.org/abs/2505.02709) · [MALT: A Dataset of Natural and Prompted Behaviors That Threaten Eval Integrity](https://metr.org/blog/2025-10-14-malt-dataset-of-natural-and-prompted-behaviors/) · [Predicting the Performance of Black-box LLMs through Self-Queries](https://arxiv.org/abs/2501.01558) · [Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index](https://arxiv.org/abs/2506.12229) · [Hyperbolic model fits METR capabilities estimate worse than exponential model](https://lesswrong.com/posts/ZEuDH2W3XdRaTwpjD/hyperbolic-model-fits-metr-capabilities-estimate-worse-than) · [New website analyzing AI companies' model evals](https://lesswrong.com/posts/nmaKpoHxmzjT8yXTk/new-website-analyzing-ai-companies-model-evals) · [Research Note: Our scheming precursor evals had limited predictive power for our in-context scheming evals](https://lesswrong.com/posts/9tqpPP4FwSnv9AWsi/research-note-our-scheming-precursor-evals-had-limited) · [How Fast Can Algorithms Advance Capabilities? | Epoch Gradient Update](https://lesswrong.com/posts/qhjNejRxbMGQp4wHt/how-fast-can-algorithms-advance-capabilities-or-epoch) · [Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods](https://lesswrong.com/posts/CwdCYmsutwXwnYtEF/paper-safety-by-measurement-a-systematic-literature-review) · [We should try to automate AI safety work asap](https://lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap) · [Validating against a misalignment detector is very different to training against one](https://lesswrong.com/posts/CXYf7kGBecZMajrXC/validating-against-a-misalignment-detector-is-very-different) · [Why do misalignment risks increase as AIs get more capable?](https://lesswrong.com/posts/NDotm7oLHfR56g4sD/why-do-misalignment-risks-increase-as-ais-get-more-capable) · [Open Philanthropy Technical AI Safety RFP - $40M Available Across 21 Research Areas](https://lesswrong.com/posts/wbJxRNxuezvsGFEWv/open-philanthropy-technical-ai-safety-rfp-usd40m-available) · [Why Future AIs will Require New Alignment Methods](https://lesswrong.com/posts/TxiB6hvnQqxXB5XDJ/why-future-ais-will-require-new-alignment-methods) · [100+ concrete projects and open problems in evals](https://lesswrong.com/posts/LhnqegFoykcjaXCYH/100-concrete-projects-and-open-problems-in-evals) · [AI companies should be safety-testing the most capable versions of their models](https://lesswrong.com/posts/tQzeafo9HjCeXn7ZF/ai-companies-should-be-safety-testing-the-most-capable)

### Autonomy evals
*Measure an AI's ability to act autonomously to complete long-horizon, complex tasks.*

*≈ Theory of change:* By measuring how long and complex a task an AI can complete (its "time horizon"), we can track capability growth and identify when models gain dangerous autonomous capabilities (like R&D acceleration or replication).

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
↔ *See also:* [Capability evals](#a:evals_capability) · [OpenAI Preparedness](https://openai.com/index/updating-our-preparedness-framework/) · [Anthropic RSP](https://www.anthropic.com/rsp-updates)  
☉ *Some names:* METR, Thomas Kwa, Ben West, Joel Becker, Beth Barnes, Hjalmar Wijk, Tao Lin, Giulio Starace, Oliver Jaffe, Dane Sherburn, Sanidhya Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou  
✗ *Critiques:* [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity.](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) [The "Length" of "Horizons"](https://www.lesswrong.com/posts/PzLSuaT6WGLQGJJJD/the-length-of-horizons)  
 *$ Funded by:* The Audacious Project, Open Philanthropy · *⫼ Estimated FTEs:* 10-50 

▹ *Some outputs:*
[Measuring AI Ability to Complete Long Tasks](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) · [Details about METR's evaluation of OpenAI GPT-5](https://metr.github.io/autonomy-evals-guide/gpt-5-report/) · [RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts](https://arxiv.org/abs/2411.15114) · [GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments](https://arxiv.org/abs/2509.21998) · [OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety](https://t.co/XfspwlzYdl) · [Details about METR's preliminary evaluation of OpenAI's o3 and o4-mini](https://metr.github.io/autonomy-evals-guide/openai-o3-report/) · [PaperBench: Evaluating AI's Ability to Replicate AI Research](https://t.co/dHN2N0tUhC) · [How Does Time Horizon Vary Across Domains?](https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/) · [Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents](https://arxiv.org/abs/2502.15840) · [Forecasting Frontier Language Model Agent Capabilities](https://arxiv.org/abs/2502.15850) · [Project Vend: Can Claude run a small shop? (And why does that matter?)](https://www.anthropic.com/research/project-vend-1) · [OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents](https://arxiv.org/abs/2506.14866) · [Fulcrum](https://fulcrumresearch.ai/2025/10/22/introducing-orchestra-quibbler.html)

### WMD evals (Weapons of Mass Destruction)
*Evaluate whether AI models possess dangerous knowledge or capabilities related to biological and chemical weapons, such as biosecurity or chemical synthesis.*

*≈ Theory of change:* By benchmarking and tracking AI's knowledge of biology and chemistry, we can identify when models become capable of accelerating WMD development or misuse, allowing for timely intervention.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
↔ *See also:* [Capability evals](#a:evals_capability) · [Autonomy evals](#a:evals_autonomy) · [Various Redteams](#a:various_redteams)  
☉ *Some names:* Lennart Justen, Haochen Zhao, Xiangru Tang, Ziran Yang, Aidan Peppin, Anka Reuel, Stephen Casper  
✗ *Critiques:* [The Reality of AI and Biorisk](https://arxiv.org/abs/2412.01946)  
 *$ Funded by:* Open Philanthropy, UK AI Safety Institute (AISI), frontier labs, Scale AI, various academic institutions (Peking University, Yale, etc.), Meta · *⫼ Estimated FTEs:* 10-50 

▹ *Some outputs:*
[Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark](https://arxiv.org/abs/2504.16137) · [LLMs Outperform Experts on Challenging Biology Benchmarks](https://arxiv.org/abs/2505.06108) · [ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain](https://arxiv.org/abs/2411.16736) · [The Reality of AI and Biorisk](https://arxiv.org/abs/2412.01946) · [The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models](https://arxiv.org/abs/2507.11544) · [Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models](https://arxiv.org/abs/2510.27629)

### Situational awareness and self-awareness evals
*Evaluate if models understand their own internal states and behaviors, their environment, and whether they are in a test or real-world deployment.*

*≈ Theory of change:* if an AI can distinguish between evaluation and deployment ("evaluation awareness"), it might hide dangerous capabilities (scheming/sandbagging). By measuring self- and situational-awareness, we can better assess this risk and build more robust evaluations.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Superintelligence can fool human supervisors, Superintelligence can hack software supervisors  
↔ *See also:* [Sandbagging evals](#a:evals_sandbagging) · [Various Redteams](#a:various_redteams) · [Model psychology](#sec:model_psychology)  
☉ *Some names:* Jan Betley, Xuchan Bao, Martín Soto, Mary Phuong, Roland S. Zimmermann, Joe Needham, Giles Edkins, Govind Pimpale, Kai Fronsdal, David Lindner, Lang Xiong, Xiaoyan Bai  
✗ *Critiques:* [Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language](https://arxiv.org/abs/2507.03409), [It's hard to make scheming evals look realistic for LLMs](https://www.lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms)  
 *$ Funded by:* frontier labs (Google DeepMind, Anthropic), Open Philanthropy, The Audacious Project, UK AI Safety Institute (AISI), AI Safety Support, Apollo Research, METR · *⫼ Estimated FTEs:* 30-70 

▹ *Some outputs:*
[Tell me about yourself: LLMs are aware of their learned behaviors](https://arxiv.org/abs/2501.11120) · [Future Events as Backdoor Triggers](https://arxiv.org/pdf/2407.04108) · [Evaluating Frontier Models for Stealth and Situational Awareness](https://arxiv.org/abs/2505.01420) · [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836) · [Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness](https://arxiv.org/abs/2509.00591) · [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399) · [Chain-of-Thought Snippets — Anti-Scheming](https://www.antischeming.ai/snippets) · [AI Awareness (literature review)](https://arxiv.org/pdf/2504.20084) · [Do LLMs know what they're capable of? Why this matters for AI safety, and initial findings](https://lesswrong.com/posts/9tHEibBBhQCHEyFsa/do-llms-know-what-they-re-capable-of-why-this-matters-for-ai) · [Claude Sonnet 3.7 (often) knows when it's in alignment evaluations](https://lesswrong.com/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment) · [It's hard to make scheming evals look realistic for LLMs](https://lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms)

### Steganography evals
*evaluate whether models can hide secret information or encoded reasoning in their outputs, such as in chain-of-thought scratchpads, to evade monitoring.*

*≈ Theory of change:* if models can use steganography, they could hide deceptive reasoning, bypassing safety monitoring and control measures. By evaluating this capability, we can assess the risk of a model fooling its supervisors.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: A boxed AGI might exfiltrate itself by steganography, spearphishing, Superintelligence can fool human supervisors  
↔ *See also:* [AI deception evals](#a:ai_deception) · [Chain of thought monitoring](#a:cot_monitoring)  
☉ *Some names:* Antonio Norelli, Michael Bronstein  
✗ *Critiques:* Chain-of-Thought Is Already Unfaithful (So Steganography is Irrelevant): [Reasoning Models Don't Always Say What They Think.](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf)  
 *$ Funded by:* Anthropic (and its general funders, e.g., Google, Amazon) · *⫼ Estimated FTEs:* 1-10 

▹ *Some outputs:*
[LLMs can hide text in other text of the same length](https://arxiv.org/abs/2510.20075) · [Do reasoning models use their scratchpad like we do? Evidence from distilling paraphrases](https://alignment.anthropic.com/2025/distill-paraphrases/) · [Early Signs of Steganographic Capabilities in Frontier LLMs](https://arxiv.org/abs/2507.02737) · [Large language models can learn and generalize steganographic chain-of-thought under process supervision](https://arxiv.org/abs/2506.01926) · [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805)

### AI deception evals
*research demonstrating that AI models, particularly agentic ones, can learn and execute deceptive behaviors such as alignment faking, manipulation, and sandbagging.*

*≈ Theory of change:* proactively discover, evaluate, and understand the mechanisms of AI deception (e.g., alignment faking, manipulation, agentic deception) to prevent models from fooling human supervisors and causing harm.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science / engineering · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Superintelligence can fool human supervisors, Superintelligence can hack software supervisors  
↔ *See also:* [Situational awareness and self-awareness evals](#a:evals_situational_awareness) · [Steganography evals](#a:evals_steganography) · [Sandbagging evals](#a:evals_sandbagging) · [Chain of thought monitoring](#a:cot_monitoring)  
☉ *Some names:* Cadenza, Fred Heiding, Simon Lermen, Andrew Kao, Myra Cheng, Cinoo Lee, Pranav Khadpe, Satyapriya Krishna, Andy Zou, Rahul Gupta  
✗ *Critiques:* A central criticism is that the evaluation scenarios are "artificial and contrived". [the void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void) and [Lessons from a Chimp](https://arxiv.org/abs/2507.03409) argue this research is "overattributing human traits" to models.  
 *$ Funded by:* Lab funders (Anthropic, OpenAI), academic institutions (e.g., Harvard, CMU, Barcelona Institute of Science and Technology), NSFC, ML Alignment Theory & Scholars (MATS) Program, FAR AI · *⫼ Estimated FTEs:* 30-80 

▹ *Some outputs:*
[Liars' Bench: Evaluating Lie Detectors for Language Models](https://arxiv.org/abs/2511.16035) · [The MASK Evaluation](https://huggingface.co/datasets/cais/MASK) · [Alignment Faking Revisited: Improved Classifiers and Open Source Extensions](https://alignment.anthropic.com/2025/alignment-faking-revisited/) · [DECEPTIONBENCH: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenario](https://arxiv.org/pdf/2510.15501) · [Among Us: A Sandbox for Measuring and Detecting Agentic Deception](https://arxiv.org/abs/2504.04072) · [Evaluating Large Language Models' Capability to Launch Fully Automated Spear Phishing Campaigns: Validated on Human Subjects](https://arxiv.org/abs/2412.00586) · [D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models](https://arxiv.org/abs/2509.17938) · [Why Do Some Language Models Fake Alignment While Others Don't?](https://arxiv.org/pdf/2506.18032) · [Frontier Models are Capable of In-context Scheming](https://arxiv.org/abs/2412.04984) · [Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL](https://arxiv.org/abs/2510.14318) · [Eliciting Secret Knowledge from Language Models](https://arxiv.org/abs/2510.01070) · [Edge Cases in AI Alignment](https://lesswrong.com/posts/bqWihHtDnDseyfF2T/edge-cases-in-ai-alignment-2) · [I replicated the Anthropic alignment faking experiment on other models, and they didn't fake alignment](https://lesswrong.com/posts/pCMmLiBcHbKohQgwA/i-replicated-the-anthropic-alignment-faking-experiment-on) · [Mistral Large 2 (123B) seems to exhibit alignment faking](https://lesswrong.com/posts/kCGk5tp5suHoGwhCa/mistral-large-2-123b-seems-to-exhibit-alignment-faking)

### Sandbagging evals
*Evaluate whether AI models deliberately hide their true capabilities or underperform, especially when they detect they are in an evaluation context.*

*≈ Theory of change:* If models can distinguish between evaluation and deployment contexts ("evaluation awareness"), they might learn to "sandbag" or deliberately underperform to hide dangerous capabilities, fooling safety evaluations. By developing evaluations for sandbagging, we can test whether our safety methods are being deceived and detect this behavior before a model is deployed.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* pessimistic  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Superintelligence can fool human supervisors, Superintelligence can hack software supervisors  
↔ *See also:* [AI deception evals](#a:ai_deception) · [Situational awareness and self-awareness evals](#a:evals_situational_awareness) · [Various Redteams](#a:various_redteams)  
☉ *Some names:* Teun van der Weij, Cameron Tice, Chloe Li, Johannes Gasteiger, Joseph Bloom, Joel Dyer  
✗ *Critiques:* The main external critique, from sources like "[the void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)" and "[Lessons from a Chimp](https://arxiv.org/abs/2507.03409)", is that this research "overattribut\[es\] human traits" to models. It argues that what's being measured isn't genuine sandbagging but models "playing-along-with-drama behaviour" in response to "artificial and contrived" evals.  
 *$ Funded by:* Anthropic (and its funders, e.g., Google, Amazon), UK Government (funding the AI Security Institute) · *⫼ Estimated FTEs:* 10-50 

▹ *Some outputs:*
[Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs](https://arxiv.org/abs/2509.18058) · [AI Sandbagging: Language Models can Strategically Underperform on Evaluations](https://arxiv.org/abs/2406.07358) · [Noise Injection Reveals Hidden Capabilities of Sandbagging Language Models](https://arxiv.org/pdf/2412.01784) · [Sandbagging in a Simple Survival Bandit Problem](https://arxiv.org/pdf/2509.26239) · [Automated Researchers Can Subtly Sandbag](https://alignment.anthropic.com/2025/automated-researchers-sandbag/) · [White Box Control at UK AISI - Update on Sandbagging Investigations](https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging) · [Won't vs. Can't: Sandbagging-like Behavior from Claude Models](https://alignment.anthropic.com/2025/wont-vs-cant/) · [LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring](https://arxiv.org/abs/2508.00943) · [Misalignment and Strategic Underperformance: An Analysis of Sandbagging and Exploration Hacking](https://lesswrong.com/posts/TeTegzR8X5CuKgMc3/misalignment-and-strategic-underperformance-an-analysis-of)

### Self-replication evals
*evaluate whether AI agents can autonomously replicate themselves by obtaining their own weights, securing compute resources, and creating copies of themselves.*

*≈ Theory of change:* if AI agents gain the ability to self-replicate, they could proliferate uncontrollably, making them impossible to shut down. By measuring this capability with benchmarks like RepliBench, we can identify when models cross this dangerous "red line" and implement controls before losing containment.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* worst-case  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: Instrumental convergence, A boxed AGI might exfiltrate itself by steganography, spearphishing  
↔ *See also:* [Autonomy evals](#a:evals_autonomy) · [Situational awareness and self-awareness evals](#a:evals_situational_awareness)  
☉ *Some names:* Sid Black, Asa Cooper Stickland, Jake Pencharz, Oliver Sourbut, Michael Schmatz, Jay Bailey, Ollie Matthews, Ben Millwood, Alex Remedios, Alan Cooney, Xudong Pan, Jiarun Dai, Yihe Fan  
✗ *Critiques:* [AI Sandbagging](https://arxiv.org/abs/2406.07358)  
 *$ Funded by:* UK Government (via UK AI Safety Institute) · *⫼ Estimated FTEs:* 10-20 

▹ *Some outputs:*
[A Realistic Evaluation of Self-Replication Risk in LLM Agents](https://arxiv.org/abs/2509.25302) · [RepliBench: measuring autonomous replication capabilities in AI systems](https://aisi.gov.uk/work/replibench-measuring-autonomous-replication-capabilities-in-ai-systems) · [Large language model-powered AI systems achieve self-replication with no human intervention](https://arxiv.org/abs/2503.17378)

### Various Redteams
*attack current models and see what they do / deliberately induce bad things on current frontier models to test out our theories / methods.*

*≈ Theory of change:* to ensure models are safe, we must actively try to break them. By developing and applying a diverse suite of attacks (e.g., in novel domains, against agentic systems, or using automated tools), researchers can discover vulnerabilities, specification gaming, and deceptive behaviors before they are exploited, thereby informing the development of more robust defenses.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
⚠ *Orthodox alignment [problems](https://docs.google.com/document/d/1xRtmO_TLgPeHxWNQFN0kkNnd6jfiUettPfEcdl6FIc0/view)*: A boxed AGI might exfiltrate itself by steganography, spearphishing, Goals misgeneralize out of distribution  
↔ *See also:* [Other evals](#a:evals_other)  
☉ *Some names:* Ryan Greenblatt, Benjamin Wright, Aengus Lynch, John Hughes, Samuel R. Bowman, Andy Zou, Nicholas Carlini, Abhay Sheshadri  
✗ *Critiques:* [Claude Sonnet 3.7 (often) knows when it's in alignment evaluations](https://www.alignmentforum.org/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment), [Red Teaming AI Red Teaming.](https://arxiv.org/html/2507.05538v1)  
 *$ Funded by:* Frontier labs (Anthropic, OpenAI, Google), government (UK AISI), Open Philanthropy, LTFF, academic grants. · *⫼ Estimated FTEs:* 100+ 

▹ *Some outputs:*
[Building and evaluating alignment auditing agents](https://alignment.anthropic.com/2025/automated-auditing/) · [WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models](https://arxiv.org/pdf/2406.18510) · [Diverse and Effective Red Teaming with Auto-generated Rewards and Multi-step Reinforcement Learning](https://arxiv.org/abs/2412.18693) · [Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise](https://t.co/wk0AP8aDNI) · [Agentic Misalignment: How LLMs could be insider threats](https://t.co/XFtd0H2Pzb) · [Compromising Honesty and Harmlessness in Language Models via Deception Attacks](https://arxiv.org/abs/2502.08301) · [Eliciting Language Model Behaviors with Investigator Agents](https://arxiv.org/abs/2502.01236) · [Why Do Some Language Models Fake Alignment While Others Don't?](https://arxiv.org/abs/2506.18032) · [Demonstrating specification gaming in reasoning models](https://arxiv.org/abs/2502.13295) · [Call Me A Jerk: Persuading AI to Comply with Objectionable Requests](https://t.co/tkHkVFVZ2m) · [RedDebate: Safer Responses through Multi-Agent Red Teaming Debates](https://arxiv.org/abs/2506.11083) · [https://arxiv.org/abs/2512.03771](https://arxiv.org/abs/2512.03771) · [The Structural Safety Generalization Problem](https://arxiv.org/abs/2504.09712) · [No, of Course I Can! Deeper Fine-Tuning Attacks That Bypass Token-Level Safety Mechanisms](https://arxiv.org/abs/2502.19537) · [Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs](https://arxiv.org/abs/2502.14828) · [LLM Robustness Leaderboard v1 --Technical report](https://arxiv.org/abs/2508.06296) · [Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods and a New Transcript-Classifier Approach](https://arxiv.org/abs/2412.02159) · [Discovering Undesired Rare Behaviors via Model Diff Amplification](https://www.goodfire.ai/papers/model-diff-amplification) · [REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective](https://arxiv.org/abs/2502.17254) · [Petri: An open-source auditing tool to accelerate AI safety research](https://alignment.anthropic.com/2025/petri/) · ['For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts](https://arxiv.org/pdf/2507.02990) · [Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models](https://arxiv.org/abs/2505.07846) · [Trading Inference-Time Compute for Adversarial Robustness](https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness) · [Can a Neural Network that only Memorizes the Dataset be Undetectably Backdoored?](https://openreview.net/forum?id=TD1NfQuVr6) · [Multi-Agent Step Race Benchmark: Assessing LLM Collaboration and Deception Under Pressure](https://github.com/lechmazur/step_game) · [Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents](https://arxiv.org/abs/2503.00061) · [Quantifying the Unruly: A Scoring System for Jailbreak Tactics](https://0din.ai/blog/quantifying-the-unruly-a-scoring-system-for-jailbreak-tactics) · [Transferable Adversarial Attacks on Black-Box Vision-Language Models](https://arxiv.org/abs/2505.01050) · [Advancing Gemini's security safeguards](https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/) · [Shutdown Resistance in Large Language Models](https://arxiv.org/abs/2509.14260) · [Stress Testing Deliberative Alignment for Anti-Scheming Training](https://arxiv.org/abs/2509.15541) · [Chain-of-Thought Hijacking](https://arxiv.org/abs/2510.26418) · [X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents](https://arxiv.org/abs/2504.13203) · [Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility](https://arxiv.org/abs/2507.11630) · [Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors](https://arxiv.org/abs/2506.10949) · [STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068) · [Adversarial Manipulation of Reasoning Models using Internal Representations](https://arxiv.org/abs/2507.03167) · [Discovering Forbidden Topics in Language Models](https://arxiv.org/abs/2505.17441) · [RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?](https://arxiv.org/abs/2506.14261) · [Jailbreak Transferability Emerges from Shared Representations](https://arxiv.org/abs/2506.12913) · [Mitigating Many-Shot Jailbreaking](https://arxiv.org/abs/2504.09604) · [Active Attacks: Red-teaming LLMs via Adaptive Environments](https://arxiv.org/abs/2509.21947) · [It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics](https://arxiv.org/abs/2506.02873) · [Adversarial Attacks on Robotic Vision Language Action Models](https://arxiv.org/abs/2506.03350) · [MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models](https://arxiv.org/abs/2503.14827) · [Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models](https://arxiv.org/abs/2510.22014) · [Uncovering Gaps in How Humans and LLMs Interpret Subjective Language](https://arxiv.org/abs/2503.04113) · [RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents](https://arxiv.org/abs/2510.02609) · [MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents](https://arxiv.org/abs/2503.10809) · [ToolTweak: An Attack on Tool Selection in LLM-based Agents](https://arxiv.org/abs/2510.02554) · [Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives](https://arxiv.org/abs/2502.11910) · [Agentic Misalignment: How LLMs Could be Insider Threats](https://lesswrong.com/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1) · [Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable Models of OpenAI, Anthropic, and Google](https://lesswrong.com/posts/zjqrSKZuRLnjAniyo/illusory-safety-redteaming-deepseek-r1-and-the-strongest) · [Will alignment-faking Claude accept a deal to reveal its misalignment?](https://lesswrong.com/posts/7C4KJot4aN8ieEDoz/will-alignment-faking-claude-accept-a-deal-to-reveal-its) · [Research directions Open Phil wants to fund in technical AI safety](https://lesswrong.com/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai) · [When does Claude sabotage code? An Agentic Misalignment follow-up](https://lesswrong.com/posts/9i6fHMn2vTqyzAi9o/when-does-claude-sabotage-code-an-agentic-misalignment) · [Petri: An open-source auditing tool to accelerate AI safety research](https://lesswrong.com/posts/kffbZGa2yYhc6cakc/petri-an-open-source-auditing-tool-to-accelerate-ai-safety)

### Other evals
*A collection of miscellaneous evaluations for specific alignment properties, such as honesty, shutdown resistance and sycophancy.*

*≈ Theory of change:* By developing novel benchmarks for specific, hard-to-measure properties (like honesty), critiquing the reliability of existing methods (like cultural surveys), and improving the formal rigor of evaluation systems (like LLM-as-Judges), researchers can create a more robust and comprehensive suite of evaluations to catch failures missed by standard capability or safety testing.

⚙ *General [approach](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* behaviorist science · ◐ *Target [case](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#A_better_definition_of_alignment_research):* average  
↔ *See also:* other more specific sections on evals  
☉ *Some names:* Richard Ren, Mantas Mazeika, Andrés Corrada-Emmanuel, Ariba Khan, Stephen Casper  
✗ *Critiques:* [The Unreliability of Evaluating Cultural Alignment in LLMs](https://arxiv.org/abs/2503.08688), [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879)  
 *$ Funded by:* Lab funders (OpenAI), Open Philanthropy (which funds CAIS, the organization for the MASK benchmark), academic institutions.
N/A (as a discrete amount). This work is part of the "tens of millions" budgets for broader evaluation and red-teaming efforts at labs and independent organizations. · *⫼ Estimated FTEs:* 20-50 

▹ *Some outputs:*
[Shutdown Resistance in Large Language Models](https://arxiv.org/abs/2509.14260) · [Gödel's Therapy Room](https://gtr.dev/) · [AI Testing Should Account for Sophisticated Strategic Behaviour](https://arxiv.org/abs/2508.14927) · [Logical Consistency Between Disagreeing Experts and Its Role in AI Safety](https://arxiv.org/abs/2510.00821) · [Expanding on what we missed with sycophancy](https://openai.com/index/expanding-on-sycophancy/) · [Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers](https://arxiv.org/abs/2504.18412) · [Spiral-Bench](https://eqbench.com/spiral-bench.html) · [Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language](https://arxiv.org/abs/2507.03409) · [Syco-bench: A Benchmark for LLM Sycophancy](https://www.syco-bench.com/) · [Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence](https://www.arxiv.org/abs/2510.01395) · [OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety](https://arxiv.org/abs/2507.06134) · [Discerning What Matters: A Multi-Dimensional Assessment of Moral Competence in LLMs](https://arxiv.org/abs/2506.13082) · [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825) · [Do LLMs Comply Differently During Tests? Is This a Hidden Variable in Safety Evaluation? And Can We Steer That?](https://lesswrong.com/posts/B2o6nrxwKxLPsSYdh/do-llms-comply-differently-during-tests-is-this-a-hidden) · [Systematic runaway-optimiser-like LLM failure modes on Biologically and Economically aligned AI safety benchmarks for LLMs with simplified observation format (BioBlue)](https://lesswrong.com/posts/PejNckwQj3A2MGhMA/systematic-runaway-optimiser-like-llm-failure-modes-on) · [Towards Alignment Auditing as a Numbers-Go-Up Science](https://lesswrong.com/posts/bGYQgBPEyHidnZCdE/towards-alignment-auditing-as-a-numbers-go-up-science)

